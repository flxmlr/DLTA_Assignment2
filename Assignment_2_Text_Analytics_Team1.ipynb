{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team information\n",
    "\n",
    "|Team-number :| 1|\n",
    "|:----:|:----:|\n",
    "\n",
    "\n",
    "|Name|    E-Mail        |matriculation-nr.|\n",
    "|:----:|:----:|:----:|\n",
    "|Tamara Scherer| schere21@ads.uni-passau.de|104218|\n",
    "|Felix Müller| muell518@ads.uni-passau.de|104227|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After labeling the data we now import it from the label-studio plattform. Therefore we used the provided workaround and downloaded the pickle file. In the next step we extract the necessary texts with the corresponding labels out of the whole downloaded data. A dataframe is created with the columns \"Text\" and \"Label\" in which the extracted data is saved. To be able to work more efficient with the data, we assign an unique ID to each label in the dataframe with the function \"categorise()\". This is especially helpful when we create the neural network in the end, as we can only use integer values as input. As stated in the task formulation we have to focus on one ID Stage. So, we implemented a filter to get only the entries with the desired ID Stage which can be specified before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary packages:\n",
    "> pickle: This package is used for serializing and de-serializing python object structures which means that any kind of python object is converted into byte streams or vice versa.\n",
    "\n",
    "> pandas: This package is usually used for data analysis and manipulation. It allows us to create and work with a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from label_studio_sdk import Client\n",
    "#from label_studio_sdk import project\n",
    "#from label_studio_sdk import project\n",
    "#import pandas as pd\n",
    "#LABEL_STUDIO_URL = 'http://132.231.59.226:8080' #this address needs to be the same as the address the label-studio is hosted on.\n",
    "#API_KEY = '1655a8922f821195356a17a3224c0532b091c61d' #please add your personal API_Key here to get your API_Key follow the Pictures below\n",
    "\n",
    "#ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "#ls.check_connection()\n",
    "#pro = project.Project.get_from_id(ls,\"1\")\n",
    "#tasks = project.Project.get_labeled_tasks(pro)\n",
    "#tasks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download labelled tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary package\n",
    "import pickle\n",
    "\n",
    "# import labeled data from downloaded pickle file\n",
    "with open(\"Files/tasks3\", 'rb') as f:\n",
    "    tasks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract text with label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[QID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_2_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, I think that as it relates to the one, I t...</td>\n",
       "      <td>[AID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>One of the areas obviously which has been a st...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[AID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_1_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11292</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_2_positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_3_no_blame]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11294 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my ...   \n",
       "1       Good afternoon and thanks a lot for taking my...   \n",
       "2      Good afternoon and thanks a lot for taking my ...   \n",
       "3      Good afternoon and thanks a lot for taking my ...   \n",
       "4      No, I think that as it relates to the one, I t...   \n",
       "...                                                  ...   \n",
       "11289  One of the areas obviously which has been a st...   \n",
       "11290  Well, let's start with the first part of your ...   \n",
       "11291  Well, let's start with the first part of your ...   \n",
       "11292  Well, let's start with the first part of your ...   \n",
       "11293  Well, let's start with the first part of your ...   \n",
       "\n",
       "                               Label  \n",
       "0                            [QID_1]  \n",
       "1      [Question_1_Company_specific]  \n",
       "2              [Question_2_specific]  \n",
       "3               [Question_3_neutral]  \n",
       "4                            [AID_1]  \n",
       "...                              ...  \n",
       "11289           [Question_3_neutral]  \n",
       "11290                        [AID_1]  \n",
       "11291            [Answer_1_specific]  \n",
       "11292            [Answer_2_positive]  \n",
       "11293            [Answer_3_no_blame]  \n",
       "\n",
       "[11294 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# filter the necessary texts with the corresponding label\n",
    "df = pd.DataFrame(columns = ['Text', 'Label',])\n",
    "\n",
    "for i in range(len(tasks)): \n",
    "    for j in range(len(tasks[i]['annotations'][0]['result'])): \n",
    "        df = df.append({\n",
    "            'Text' : tasks[i]['annotations'][0]['result'][j]['value']['text'],\n",
    "            'Label' : tasks[i]['annotations'][0]['result'][j]['value']['labels']\n",
    "                        }, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Label as unique ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[QID_1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_2_specific]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, I think that as it relates to the one, I t...</td>\n",
       "      <td>[AID_1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>One of the areas obviously which has been a st...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[AID_1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_1_specific]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11292</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_2_positive]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_3_no_blame]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11294 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my ...   \n",
       "1       Good afternoon and thanks a lot for taking my...   \n",
       "2      Good afternoon and thanks a lot for taking my ...   \n",
       "3      Good afternoon and thanks a lot for taking my ...   \n",
       "4      No, I think that as it relates to the one, I t...   \n",
       "...                                                  ...   \n",
       "11289  One of the areas obviously which has been a st...   \n",
       "11290  Well, let's start with the first part of your ...   \n",
       "11291  Well, let's start with the first part of your ...   \n",
       "11292  Well, let's start with the first part of your ...   \n",
       "11293  Well, let's start with the first part of your ...   \n",
       "\n",
       "                               Label  LabelNumber  \n",
       "0                            [QID_1]            0  \n",
       "1      [Question_1_Company_specific]            1  \n",
       "2              [Question_2_specific]            3  \n",
       "3               [Question_3_neutral]            7  \n",
       "4                            [AID_1]            0  \n",
       "...                              ...          ...  \n",
       "11289           [Question_3_neutral]            7  \n",
       "11290                        [AID_1]            0  \n",
       "11291            [Answer_1_specific]            8  \n",
       "11292            [Answer_2_positive]           10  \n",
       "11293            [Answer_3_no_blame]           13  \n",
       "\n",
       "[11294 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign a unique number to each label (can also be used later for the neural network)\n",
    "def categorise(row):  \n",
    "    if row['Label'] == ['Question_1_Company_specific']:\n",
    "        return 1\n",
    "    elif row['Label'] == ['Question_1_Market_related']:\n",
    "        return 2\n",
    "    elif row['Label'] == ['Question_2_specific']:\n",
    "        return 3\n",
    "    elif row['Label'] == ['Question_2_open']:\n",
    "        return 4\n",
    "    elif row['Label'] == ['Question_3_attack']:\n",
    "        return 5\n",
    "    elif row['Label'] == ['Question_3_support']:\n",
    "        return 6\n",
    "    elif row['Label'] == ['Question_3_neutral']:\n",
    "        return 7\n",
    "    elif row['Label'] == ['Answer_1_specific']:\n",
    "        return 8\n",
    "    elif row['Label'] == ['Answer_1_avoid_excuse']:\n",
    "        return 9\n",
    "    elif row['Label'] == ['Answer_2_positive']:\n",
    "        return 10\n",
    "    elif row['Label'] == ['Answer_2_negative']:\n",
    "        return 11\n",
    "    elif row['Label'] == ['Answer_3_blame']:\n",
    "        return 12\n",
    "    elif row['Label'] == ['Answer_3_no_blame']:\n",
    "        return 13\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "# call function and write results in a new column of the dataframe\n",
    "df['LabelNumber'] = df.apply(\n",
    "    lambda row: categorise(row), axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter one ID stage (e.g. Question_1_XX)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \n",
       "0     [Question_1_Company_specific]            1  \n",
       "1     [Question_1_Company_specific]            1  \n",
       "2       [Question_1_Market_related]            2  \n",
       "3     [Question_1_Company_specific]            1  \n",
       "4     [Question_1_Company_specific]            1  \n",
       "...                             ...          ...  \n",
       "1304  [Question_1_Company_specific]            1  \n",
       "1305  [Question_1_Company_specific]            1  \n",
       "1306  [Question_1_Company_specific]            1  \n",
       "1307  [Question_1_Company_specific]            1  \n",
       "1308  [Question_1_Company_specific]            1  \n",
       "\n",
       "[1309 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set desired labels\n",
    "idStage = [1, 2]\n",
    "\n",
    "# filter the entries in the dataframe with the specified ID stage\n",
    "df = df[df['LabelNumber'].isin(idStage)]\n",
    "\n",
    "# set new index\n",
    "df = df.reset_index()\n",
    "# delete old indices\n",
    "df.pop('index')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement different steps to preprocess the downloaded data. Therefore we have to make some imports first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Packages:\n",
    "> re: This package can be used to work with Regular Expressions. Here it is used remove the numbers in the text.\n",
    "\n",
    "> string: This package is necessary for common string operations. Here it is used to remove the punctuation in the text.\n",
    "\n",
    "> nltk: Natural Language Toolkit is a package which is used for Natural Language Processing. Here it is used for stemming, lemmatization, tokenization and the removal of stopwords.\n",
    "\n",
    "Further Imports:\n",
    "> For Stemmer: The module \"nltk.stem.snowball\" provides a port of the Snowball stemmers. The word stemmer is based on the original Porter stemming algorithm for suffix stripping. In this case it is used for the English language.\n",
    "\n",
    "> For Stopwords: Here the English stopwords are imported from the \"nltk.corpus\" package. It includes words which does not add much meaning to a sentence and are not important for further text analysis.\n",
    "\n",
    "> For Tokenization: The word_tokenize module is imported from the nltk library for tokenization.\n",
    "\n",
    "> For Lemmatizer: The \"WordNetLemmatizer\" module as well as \"pos_tag\" is imported here. Therefore the system requires some further downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we start with the preprocessing. Therefore, we implemented a function called \"do_preprocessing\" with text that has to be preprocessed as input. Furthermore, it is possible to set two boolean values to specify if the removal of stopwords and the stemming should be executed. \n",
    "Within the function we implemented the following steps:\n",
    "1. Removal of Numbers: delete all numbers, e.g. 1, 2 of the text\n",
    "2. Lowercasing: convert the text into the same casing, so that the different versions of the word can be treated as one\n",
    "3. Stemming: process to reduce the words to their root forms, but the stem itself may not be a valid word in the language (here we use the PorterStemmer)\n",
    "4. Stopwords Removal: stopwords are trivial words which appear very frequently in the text without adding much valuable information and therefore not necessary for the further analysis\n",
    "5. Removing Punctuations: the punctuation does not add any value, so it is not necessary or helpful for the analysis and can therefore be deleted as well\n",
    "6. Removing Extra Whitespaces: additional whitespaces do not add any value to the data and just increase the text size, so they can be deleted as well\n",
    "<p>\n",
    "For Lemmatization we implemented another function \"do_lemmatization\" as it is necessary to do a tokenization first.\n",
    "\n",
    "1. Tokenization: process of splitting the text into pieces called tokens (here the tokens are the single words)\n",
    "2. Lemmatization: This can be done instead of stemming. Lemmatization is also a process to convert the word to its base form. Before the text is lemmatized a POS tagging is necessary to tag the tokens as noun, verb, adverb or adjective. As it causes noticeable improvement we implement the lemmatization in addition to the already implemented preprocessing.\n",
    "<p>\n",
    "Finally we use the resulting tokens to create a dictionary which is necessary for the following steps as for building the neural network the column size of the training and test dataset have to be identical. This can be ensured by using this dictionary. To be able to execute the next implementations it is necessary to convert the tokens back to normal strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# necessary imports for preprocessing steps\n",
    "\n",
    "# import necessary packages\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# import a stemmer for english words\n",
    "snowStem = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "# import for tokenization\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>good afternoon thanks lot taking question so r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay thats helpful growth china surpass expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "      <td>hi richard so gross margin looked pretty solid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>coreoncore pretty normal well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay followup ebit dollar growth looked like c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi mark pat thank much taking question could t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>yes im wondering could talk little bit custome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi thank you congrats quarter followup toms qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks squeezing guys question you john know g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>one areas obviously strong growth driver team ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \\\n",
       "0     [Question_1_Company_specific]            1   \n",
       "1     [Question_1_Company_specific]            1   \n",
       "2       [Question_1_Market_related]            2   \n",
       "3     [Question_1_Company_specific]            1   \n",
       "4     [Question_1_Company_specific]            1   \n",
       "...                             ...          ...   \n",
       "1304  [Question_1_Company_specific]            1   \n",
       "1305  [Question_1_Company_specific]            1   \n",
       "1306  [Question_1_Company_specific]            1   \n",
       "1307  [Question_1_Company_specific]            1   \n",
       "1308  [Question_1_Company_specific]            1   \n",
       "\n",
       "                                              CleanText  \n",
       "0     good afternoon thanks lot taking question so r...  \n",
       "1     okay thats helpful growth china surpass expect...  \n",
       "2     hi richard so gross margin looked pretty solid...  \n",
       "3                         coreoncore pretty normal well  \n",
       "4     okay followup ebit dollar growth looked like c...  \n",
       "...                                                 ...  \n",
       "1304  hi mark pat thank much taking question could t...  \n",
       "1305  yes im wondering could talk little bit custome...  \n",
       "1306  hi thank you congrats quarter followup toms qu...  \n",
       "1307  thanks squeezing guys question you john know g...  \n",
       "1308  one areas obviously strong growth driver team ...  \n",
       "\n",
       "[1309 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function that includes all preprocessing steps\n",
    "\n",
    "def do_preprocessing(text_to_clean, remove_stopwords = True, stemming = True):\n",
    "    # remove numbers \n",
    "    text_to_clean = re.sub(r'\\d+', '', text_to_clean)\n",
    "    # transform text to lower case\n",
    "    text_to_clean = text_to_clean.lower()\n",
    "\n",
    "    if stemming:\n",
    "      # stemming\n",
    "      text_to_clean = snowStem.stem(text_to_clean)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        text_to_clean = ' '.join([w for w in text_to_clean.split() if not(w in en_stopwords)])\n",
    "\n",
    "    # remove punctuation\n",
    "    text_to_clean = text_to_clean.translate(str.maketrans('','', string.punctuation))\n",
    "    # remove leading and ending white spaces\n",
    "    text_to_clean = text_to_clean.strip()\n",
    "    \n",
    "    return text_to_clean\n",
    "\n",
    "\n",
    "# call function and write results in new column of the dataframe\n",
    "df.loc[:, 'CleanText'] = df['Text'].apply(\n",
    "    lambda x: do_preprocessing(x, True, True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[good, afternoon, thanks, lot, take, question,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[okay, thats, helpful, growth, china, surpass,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "      <td>[hi, richard, so, gross, margin, look, pretty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[coreoncore, pretty, normal, well]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[okay, followup, ebit, dollar, growth, look, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[hi, mark, pat, thank, much, take, question, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[yes, im, wondering, could, talk, little, bit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[hi, thank, you, congrats, quarter, followup, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[thanks, squeeze, guy, question, you, john, kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, area, obviously, strong, growth, driver,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \\\n",
       "0     [Question_1_Company_specific]            1   \n",
       "1     [Question_1_Company_specific]            1   \n",
       "2       [Question_1_Market_related]            2   \n",
       "3     [Question_1_Company_specific]            1   \n",
       "4     [Question_1_Company_specific]            1   \n",
       "...                             ...          ...   \n",
       "1304  [Question_1_Company_specific]            1   \n",
       "1305  [Question_1_Company_specific]            1   \n",
       "1306  [Question_1_Company_specific]            1   \n",
       "1307  [Question_1_Company_specific]            1   \n",
       "1308  [Question_1_Company_specific]            1   \n",
       "\n",
       "                                              CleanText  \n",
       "0     [good, afternoon, thanks, lot, take, question,...  \n",
       "1     [okay, thats, helpful, growth, china, surpass,...  \n",
       "2     [hi, richard, so, gross, margin, look, pretty,...  \n",
       "3                    [coreoncore, pretty, normal, well]  \n",
       "4     [okay, followup, ebit, dollar, growth, look, l...  \n",
       "...                                                 ...  \n",
       "1304  [hi, mark, pat, thank, much, take, question, c...  \n",
       "1305  [yes, im, wondering, could, talk, little, bit,...  \n",
       "1306  [hi, thank, you, congrats, quarter, followup, ...  \n",
       "1307  [thanks, squeeze, guy, question, you, john, kn...  \n",
       "1308  [one, area, obviously, strong, growth, driver,...  \n",
       "\n",
       "[1309 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for lemmatization\n",
    "def do_lemmatization(text):\n",
    "    \n",
    "  text = word_tokenize(text)\n",
    "  \n",
    "  result=[]\n",
    "  wordnet = WordNetLemmatizer()\n",
    "  for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.lemmatize(token,pos))\n",
    "\n",
    "  return result\n",
    "\n",
    "# call function\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: do_lemmatization(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# generate the gensim dictionary\n",
    "dct = corpora.dictionary.Dictionary(df['CleanText']).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert tokens to strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>good afternoon thanks lot take question so rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay thats helpful growth china surpass expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "      <td>hi richard so gross margin look pretty solid w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>coreoncore pretty normal well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay followup ebit dollar growth look like com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi mark pat thank much take question could tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>yes im wondering could talk little bit custome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi thank you congrats quarter followup tom que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks squeeze guy question you john know guy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>one area obviously strong growth driver team m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \\\n",
       "0     [Question_1_Company_specific]            1   \n",
       "1     [Question_1_Company_specific]            1   \n",
       "2       [Question_1_Market_related]            2   \n",
       "3     [Question_1_Company_specific]            1   \n",
       "4     [Question_1_Company_specific]            1   \n",
       "...                             ...          ...   \n",
       "1304  [Question_1_Company_specific]            1   \n",
       "1305  [Question_1_Company_specific]            1   \n",
       "1306  [Question_1_Company_specific]            1   \n",
       "1307  [Question_1_Company_specific]            1   \n",
       "1308  [Question_1_Company_specific]            1   \n",
       "\n",
       "                                              CleanText  \n",
       "0     good afternoon thanks lot take question so rec...  \n",
       "1     okay thats helpful growth china surpass expect...  \n",
       "2     hi richard so gross margin look pretty solid w...  \n",
       "3                         coreoncore pretty normal well  \n",
       "4     okay followup ebit dollar growth look like com...  \n",
       "...                                                 ...  \n",
       "1304  hi mark pat thank much take question could tal...  \n",
       "1305  yes im wondering could talk little bit custome...  \n",
       "1306  hi thank you congrats quarter followup tom que...  \n",
       "1307  thanks squeeze guy question you john know guy ...  \n",
       "1308  one area obviously strong growth driver team m...  \n",
       "\n",
       "[1309 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tokens back to strings for further processing\n",
    "whitespace = \" \"\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: whitespace.join(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split of training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network which we build later on, it is necessary to split the available data in a training and a test dataset. We therefore use the 70/30-approach which means that 70% of the data is used for training and 30% of the data is used for testing. \n",
    "We do this split already in this step to ensure that there are no dependencies between training and test dataset in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary package:\n",
    "> numpy: Provides operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, basic linear algebra and much more. We use it here to compute the training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_fraction = 0.70\n",
    "training_size = int(np.floor(len(df) * training_fraction))\n",
    "\n",
    "data_train, data_test = df[:training_size], df[training_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Frequency Method: Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach the number of terms per document are counted. As we have a big corpora in this case, the resulting document-term matrix is a sparse matrix which means that it has many zeros and only a few non-zeor entries.\n",
    "The method \"CountVectorizer()\" converts a collection of text documents to a matrix of token counts. The result out of this is a sparse representation of the counts which then have to be transformed into an array. Finally, this array is converted in a DataFrame. \n",
    "As already mentioned before, we have created a dictionary out of all the existing tokens after the preprocessing. This dictionary can now be used as input for the CountVectorizer() to get BOW-matrices with the same amount of columns.\n",
    "<p>\n",
    "Necessary Package: \n",
    "\n",
    "> sklearn.feature_extraction.text.CountVectorizer: Used to convert a collection of text documents to a matrix of token counts. It produces a sparse representation of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary  benefit  decision  different  do  drive  give  \\\n",
       "0            1            1        1         1          1   1      2     1   \n",
       "1            0            0        0         0          0   0      0     0   \n",
       "2            0            0        0         0          0   0      0     0   \n",
       "3            0            0        0         0          0   0      0     0   \n",
       "4            0            0        0         0          0   0      0     0   \n",
       "..         ...          ...      ...       ...        ...  ..    ...   ...   \n",
       "911          0            0        0         0          0   0      0     0   \n",
       "912          0            0        0         0          0   0      0     2   \n",
       "913          0            0        0         0          0   0      0     0   \n",
       "914          0            0        0         0          0   0      0     1   \n",
       "915          0            0        0         0          0   0      0     1   \n",
       "\n",
       "     good  growth  ...  realignment  rpo  shed  asics  hock  jericho  optical  \\\n",
       "0       1       2  ...            0    0     0      0     0        0        0   \n",
       "1       0       1  ...            0    0     0      0     0        0        0   \n",
       "2       0       0  ...            0    0     0      0     0        0        0   \n",
       "3       0       0  ...            0    0     0      0     0        0        0   \n",
       "4       0       2  ...            0    0     0      0     0        0        0   \n",
       "..    ...     ...  ...          ...  ...   ...    ...   ...      ...      ...   \n",
       "911     0       0  ...            0    0     0      0     0        0        0   \n",
       "912     0       0  ...            0    0     0      0     0        0        0   \n",
       "913     0       0  ...            0    0     0      0     0        0        0   \n",
       "914     0       0  ...            0    0     0      0     0        0        0   \n",
       "915     0       0  ...            0    0     0      0     0        0        0   \n",
       "\n",
       "     performing  tomahawk  trident  \n",
       "0             0         0        0  \n",
       "1             0         0        0  \n",
       "2             0         0        0  \n",
       "3             0         0        0  \n",
       "4             0         0        0  \n",
       "..          ...       ...      ...  \n",
       "911           0         0        0  \n",
       "912           0         0        0  \n",
       "913           0         0        0  \n",
       "914           0         0        0  \n",
       "915           0         0        0  \n",
       "\n",
       "[916 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary  benefit  decision  different  do  drive  give  \\\n",
       "0            0            0        0         0          0   0      0     0   \n",
       "1            0            0        0         0          0   0      0     0   \n",
       "2            0            0        0         0          0   0      0     0   \n",
       "3            0            0        0         0          0   0      0     0   \n",
       "4            0            0        0         0          1   0      0     1   \n",
       "..         ...          ...      ...       ...        ...  ..    ...   ...   \n",
       "388          0            0        0         0          0   0      0     0   \n",
       "389          0            0        0         0          0   0      0     0   \n",
       "390          0            0        0         0          0   0      0     0   \n",
       "391          0            0        0         0          0   0      0     1   \n",
       "392          0            0        0         0          0   0      0     0   \n",
       "\n",
       "     good  growth  ...  realignment  rpo  shed  asics  hock  jericho  optical  \\\n",
       "0       0       0  ...            0    0     0      0     0        0        0   \n",
       "1       0       0  ...            0    0     0      0     0        0        0   \n",
       "2       0       0  ...            0    0     0      0     0        0        0   \n",
       "3       0       0  ...            0    0     0      0     0        0        0   \n",
       "4       0       0  ...            0    0     0      0     0        0        0   \n",
       "..    ...     ...  ...          ...  ...   ...    ...   ...      ...      ...   \n",
       "388     0       0  ...            0    0     0      0     0        0        0   \n",
       "389     0       0  ...            0    0     0      0     0        0        0   \n",
       "390     0       0  ...            1    0     0      0     0        0        0   \n",
       "391     0       0  ...            0    2     1      0     0        0        0   \n",
       "392     0       1  ...            0    0     0      1     1        1        1   \n",
       "\n",
       "     performing  tomahawk  trident  \n",
       "0             0         0        0  \n",
       "1             0         0        0  \n",
       "2             0         0        0  \n",
       "3             0         0        0  \n",
       "4             0         0        0  \n",
       "..          ...       ...      ...  \n",
       "388           0         0        0  \n",
       "389           0         0        0  \n",
       "390           0         0        0  \n",
       "391           0         0        0  \n",
       "392           1         1        1  \n",
       "\n",
       "[393 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def do_bow(data):\n",
    "    count_vec = CountVectorizer(vocabulary = dct)\n",
    "\n",
    "    bow = count_vec.fit_transform(data['CleanText'])\n",
    "    bow_matrix = pd.DataFrame(data = bow.toarray(), columns = count_vec.get_feature_names_out())\n",
    "\n",
    "    display(bow_matrix)\n",
    "    \n",
    "    return(bow_matrix)\n",
    "\n",
    "# call function\n",
    "train_bow_matrix = do_bow(data_train)\n",
    "test_bow_matrix = do_bow(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Frequency Method: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another raw frequency-based approach is TF-IDF (= term-frequency inverse-docmument-frequency). Instead of using the absolute term frequencies, we use weighted frequencies for this method.\n",
    "It is working very similar to the implementation of BOW. For this method the \"TfidfVectorizer()\" can be used which has again the parameter vocabulary which we set to the created dictionary. The TfidfVectorizer converts a collection of raw documents to a matrix of TF-IDF features. We again get a sparse matrix which we then transform into an array and pass it into a DataFrame which is then given as output of the function.\n",
    "<p>\n",
    "Necessary package: \n",
    "\n",
    "> sklearn.feature_extraction.text.TfidfVectorizer: used to transform a count matrix to a normalized tf or tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.153021</td>\n",
       "      <td>0.236999</td>\n",
       "      <td>0.155793</td>\n",
       "      <td>0.177425</td>\n",
       "      <td>0.126805</td>\n",
       "      <td>0.153021</td>\n",
       "      <td>0.243532</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.115618</td>\n",
       "      <td>0.194891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary   benefit  decision  different        do  \\\n",
       "0     0.153021     0.236999  0.155793  0.177425   0.126805  0.153021   \n",
       "1     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "2     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "3     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "4     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "..         ...          ...       ...       ...        ...       ...   \n",
       "911   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "912   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "913   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "914   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "915   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "        drive      give      good    growth  ...  realignment  rpo  shed  \\\n",
       "0    0.243532  0.085938  0.115618  0.194891  ...          0.0  0.0   0.0   \n",
       "1    0.000000  0.000000  0.000000  0.174402  ...          0.0  0.0   0.0   \n",
       "2    0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "3    0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "4    0.000000  0.000000  0.000000  0.152685  ...          0.0  0.0   0.0   \n",
       "..        ...       ...       ...       ...  ...          ...  ...   ...   \n",
       "911  0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "912  0.000000  0.237623  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "913  0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "914  0.000000  0.093197  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "915  0.000000  0.085258  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "\n",
       "     asics  hock  jericho  optical  performing  tomahawk  trident  \n",
       "0      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "1      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "2      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "3      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "4      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "..     ...   ...      ...      ...         ...       ...      ...  \n",
       "911    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "912    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "913    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "914    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "915    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "\n",
       "[916 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464447</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary  benefit  decision  different   do  drive  \\\n",
       "0          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "1          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "2          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "3          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "4          0.0          0.0      0.0       0.0   0.145685  0.0    0.0   \n",
       "..         ...          ...      ...       ...        ...  ...    ...   \n",
       "388        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "389        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "390        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "391        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "392        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "\n",
       "         give  good    growth  ...  realignment       rpo      shed     asics  \\\n",
       "0    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "2    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.087808   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "..        ...   ...       ...  ...          ...       ...       ...       ...   \n",
       "388  0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "389  0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "390  0.000000   0.0  0.000000  ...     0.180413  0.000000  0.000000  0.000000   \n",
       "391  0.093644   0.0  0.000000  ...     0.000000  0.464447  0.232224  0.000000   \n",
       "392  0.000000   0.0  0.067947  ...     0.000000  0.000000  0.000000  0.157323   \n",
       "\n",
       "         hock   jericho   optical  performing  tomahawk   trident  \n",
       "0    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...         ...       ...       ...  \n",
       "388  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "389  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "390  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "391  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "392  0.157323  0.157323  0.157323    0.157323  0.157323  0.157323  \n",
       "\n",
       "[393 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def do_tfidf(data):\n",
    "    tfidf_vec = TfidfVectorizer(vocabulary = dct)\n",
    "\n",
    "    tfidf = tfidf_vec.fit_transform(data['CleanText'])\n",
    "    tfidf_matrix = pd.DataFrame(data = tfidf.toarray(), columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "    display(tfidf_matrix)\n",
    "    \n",
    "    return(tfidf_matrix)\n",
    "\n",
    "# call function\n",
    "train_tfidf_matrix = do_tfidf(data_train)\n",
    "test_tfidf_matrix = do_tfidf(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Topic Method: LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA (= latent semantic analysis) is one representative of topic-based approaches. It is a mathematical decomposition technique using raw frequency matrices. As already mentioned, BOW or TF-IDF matrices can be very spare and high dimensional. LSA is facing this problem by reducing the dimensionality.\n",
    "Therefore, the resulting matrix of BOW or TF-IDF is necessary which we already get as output of the corresponding functions. So, we use for example the tf-idf-matrix as input for this function. Then we set the number of topics and apply singular value decomposition to the matrix which can be done for example by the function \"svds\".\n",
    "<p>\n",
    "Necessary Package:\n",
    "\n",
    "> scipy.sparse.linalg.svds: It is used to make a partial singular value decomposition of a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063852</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.017864</td>\n",
       "      <td>0.101711</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>0.051261</td>\n",
       "      <td>-0.006285</td>\n",
       "      <td>0.017381</td>\n",
       "      <td>0.044309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047987</td>\n",
       "      <td>0.020022</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>-0.035792</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.039789</td>\n",
       "      <td>-0.006914</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>0.018086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026689</td>\n",
       "      <td>0.022862</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.027197</td>\n",
       "      <td>-0.011017</td>\n",
       "      <td>-0.065299</td>\n",
       "      <td>-0.003146</td>\n",
       "      <td>-0.033844</td>\n",
       "      <td>0.049235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003002</td>\n",
       "      <td>-0.003607</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.019330</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>-0.007566</td>\n",
       "      <td>-0.002486</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>-0.011877</td>\n",
       "      <td>0.008377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076328</td>\n",
       "      <td>0.032857</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>0.011290</td>\n",
       "      <td>-0.007317</td>\n",
       "      <td>-0.074875</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>-0.090881</td>\n",
       "      <td>-0.057036</td>\n",
       "      <td>0.062475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.009924</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>-0.045003</td>\n",
       "      <td>-0.012187</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>-0.042534</td>\n",
       "      <td>-0.017822</td>\n",
       "      <td>0.017820</td>\n",
       "      <td>0.038597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>-0.016243</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>-0.003163</td>\n",
       "      <td>-0.036650</td>\n",
       "      <td>-0.014578</td>\n",
       "      <td>0.031850</td>\n",
       "      <td>-0.014996</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>0.022267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.031451</td>\n",
       "      <td>0.037696</td>\n",
       "      <td>-0.015868</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>-0.008951</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>-0.017709</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.017045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>-0.011292</td>\n",
       "      <td>0.023081</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>-0.018354</td>\n",
       "      <td>-0.007153</td>\n",
       "      <td>0.027412</td>\n",
       "      <td>-0.009124</td>\n",
       "      <td>-0.012126</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>0.014872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.024987</td>\n",
       "      <td>-0.024399</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>0.041918</td>\n",
       "      <td>-0.013547</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>-0.024155</td>\n",
       "      <td>-0.016189</td>\n",
       "      <td>0.033829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.063852  0.001987  0.017864  0.101711  0.045427  0.036281  0.051261   \n",
       "1    0.047987  0.020022  0.008672  0.021375 -0.035792  0.001680  0.039789   \n",
       "2    0.026689  0.022862  0.013200  0.021622  0.027197 -0.011017 -0.065299   \n",
       "3   -0.003002 -0.003607 -0.005458  0.019330  0.013714 -0.007566 -0.002486   \n",
       "4    0.076328  0.032857  0.035851  0.011290 -0.007317 -0.074875 -0.005319   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "911  0.012822  0.009924  0.009228 -0.045003 -0.012187  0.011660 -0.042534   \n",
       "912 -0.016243  0.016886 -0.003163 -0.036650 -0.014578  0.031850 -0.014996   \n",
       "913  0.004968  0.031451  0.037696 -0.015868  0.002805 -0.008951 -0.007917   \n",
       "914 -0.011292  0.023081  0.005894 -0.018354 -0.007153  0.027412 -0.009124   \n",
       "915  0.005831  0.024987 -0.024399  0.022537  0.041918 -0.013547  0.013452   \n",
       "\n",
       "            7         8         9  \n",
       "0   -0.006285  0.017381  0.044309  \n",
       "1   -0.006914  0.015449  0.018086  \n",
       "2   -0.003146 -0.033844  0.049235  \n",
       "3    0.010854 -0.011877  0.008377  \n",
       "4   -0.090881 -0.057036  0.062475  \n",
       "..        ...       ...       ...  \n",
       "911 -0.017822  0.017820  0.038597  \n",
       "912  0.002250  0.003314  0.022267  \n",
       "913 -0.017709  0.000577  0.017045  \n",
       "914 -0.012126 -0.001633  0.014872  \n",
       "915 -0.024155 -0.016189  0.033829  \n",
       "\n",
       "[916 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003787</td>\n",
       "      <td>0.036151</td>\n",
       "      <td>-0.007546</td>\n",
       "      <td>-0.012739</td>\n",
       "      <td>-0.003463</td>\n",
       "      <td>0.007904</td>\n",
       "      <td>-0.004376</td>\n",
       "      <td>-0.007969</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>-0.017286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003623</td>\n",
       "      <td>-0.010465</td>\n",
       "      <td>-0.016374</td>\n",
       "      <td>0.023581</td>\n",
       "      <td>0.063199</td>\n",
       "      <td>0.061785</td>\n",
       "      <td>-0.001506</td>\n",
       "      <td>0.028464</td>\n",
       "      <td>-0.013361</td>\n",
       "      <td>-0.032609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100248</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.033927</td>\n",
       "      <td>0.012630</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>-0.027062</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>-0.041443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.048395</td>\n",
       "      <td>0.064332</td>\n",
       "      <td>0.107475</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>0.091670</td>\n",
       "      <td>0.121379</td>\n",
       "      <td>0.023276</td>\n",
       "      <td>-0.035205</td>\n",
       "      <td>0.030819</td>\n",
       "      <td>-0.048567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.056217</td>\n",
       "      <td>0.127080</td>\n",
       "      <td>0.056279</td>\n",
       "      <td>0.160083</td>\n",
       "      <td>0.100818</td>\n",
       "      <td>0.076546</td>\n",
       "      <td>-0.001372</td>\n",
       "      <td>-0.058783</td>\n",
       "      <td>-0.028786</td>\n",
       "      <td>-0.032502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-0.049372</td>\n",
       "      <td>-0.106876</td>\n",
       "      <td>-0.133016</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-0.119362</td>\n",
       "      <td>0.064732</td>\n",
       "      <td>0.067216</td>\n",
       "      <td>-0.153516</td>\n",
       "      <td>-0.105204</td>\n",
       "      <td>-0.070174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>-0.082668</td>\n",
       "      <td>-0.025952</td>\n",
       "      <td>-0.129962</td>\n",
       "      <td>-0.023329</td>\n",
       "      <td>-0.034594</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.077375</td>\n",
       "      <td>-0.149159</td>\n",
       "      <td>-0.101139</td>\n",
       "      <td>-0.048324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.077289</td>\n",
       "      <td>-0.033382</td>\n",
       "      <td>0.031542</td>\n",
       "      <td>-0.057291</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>0.019505</td>\n",
       "      <td>0.065074</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>-0.052925</td>\n",
       "      <td>-0.054011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.000596</td>\n",
       "      <td>-0.030342</td>\n",
       "      <td>-0.037173</td>\n",
       "      <td>-0.004155</td>\n",
       "      <td>-0.058917</td>\n",
       "      <td>-0.033699</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>-0.106086</td>\n",
       "      <td>-0.040183</td>\n",
       "      <td>-0.072156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>-0.038896</td>\n",
       "      <td>-0.055331</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>-0.017200</td>\n",
       "      <td>0.023066</td>\n",
       "      <td>-0.027363</td>\n",
       "      <td>-0.049996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.003787  0.036151 -0.007546 -0.012739 -0.003463  0.007904 -0.004376   \n",
       "1   -0.003623 -0.010465 -0.016374  0.023581  0.063199  0.061785 -0.001506   \n",
       "2    0.100248  0.064394  0.033927  0.012630  0.018216  0.003165 -0.027062   \n",
       "3   -0.048395  0.064332  0.107475  0.191095  0.091670  0.121379  0.023276   \n",
       "4   -0.056217  0.127080  0.056279  0.160083  0.100818  0.076546 -0.001372   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "388 -0.049372 -0.106876 -0.133016 -0.001300 -0.119362  0.064732  0.067216   \n",
       "389 -0.082668 -0.025952 -0.129962 -0.023329 -0.034594 -0.000819  0.077375   \n",
       "390  0.077289 -0.033382  0.031542 -0.057291  0.020551  0.019505  0.065074   \n",
       "391  0.000596 -0.030342 -0.037173 -0.004155 -0.058917 -0.033699  0.011284   \n",
       "392 -0.038896 -0.055331  0.037653  0.025691  0.014925  0.009770 -0.017200   \n",
       "\n",
       "            7         8         9  \n",
       "0   -0.007969  0.003698 -0.017286  \n",
       "1    0.028464 -0.013361 -0.032609  \n",
       "2    0.008933  0.002267 -0.041443  \n",
       "3   -0.035205  0.030819 -0.048567  \n",
       "4   -0.058783 -0.028786 -0.032502  \n",
       "..        ...       ...       ...  \n",
       "388 -0.153516 -0.105204 -0.070174  \n",
       "389 -0.149159 -0.101139 -0.048324  \n",
       "390  0.020076 -0.052925 -0.054011  \n",
       "391 -0.106086 -0.040183 -0.072156  \n",
       "392  0.023066 -0.027363 -0.049996  \n",
       "\n",
       "[393 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def do_lsa(data):\n",
    "    num_components = 10\n",
    "    q, s, p = svds(data, k = num_components)\n",
    "\n",
    "    lsa_doc_matrix = pd.DataFrame(data=q)\n",
    "\n",
    "    display(lsa_doc_matrix)\n",
    "\n",
    "    return(lsa_doc_matrix)\n",
    "\n",
    "# call function\n",
    "train_lsa_matrix = do_lsa(train_tfidf_matrix)\n",
    "test_lsa_matrix = do_lsa(test_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Word Embedding Method: Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Doc2Vec method it is possible to derive document vectors. As opposite to Word2Vec it is used to create a vectorized representation of a group of words taken collectively as a single unit.\n",
    "First of all we have to train the model. In order to do this we need the tagged document which can be created by using \"models.doc2vec.TaggedDocument()\". In the next step we initialise the model, build the vocabulary and train the Doc2Vec model. Finally, we can use the model.infer_vector() to analyse the output and save the matrix as a DataFrame.\n",
    "\n",
    "<p>\n",
    "Necessary Package:\n",
    "\n",
    "> gensim: Used for topic modelling, document indexing and similarity retrieval with large corpora. The target audience is the natural language processing and information retrieval community.\n",
    "\n",
    "> gensim.models.doc2vec.Doc2Vec: Necessary to build the Doc2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120934</td>\n",
       "      <td>-0.272334</td>\n",
       "      <td>-0.213747</td>\n",
       "      <td>0.160538</td>\n",
       "      <td>-0.362386</td>\n",
       "      <td>-0.023036</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.385390</td>\n",
       "      <td>-0.240347</td>\n",
       "      <td>-0.166854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441486</td>\n",
       "      <td>-0.144993</td>\n",
       "      <td>0.273198</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.362145</td>\n",
       "      <td>-0.225160</td>\n",
       "      <td>-0.283453</td>\n",
       "      <td>-0.693637</td>\n",
       "      <td>0.617757</td>\n",
       "      <td>0.262902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.342089</td>\n",
       "      <td>0.158022</td>\n",
       "      <td>0.298481</td>\n",
       "      <td>-0.433624</td>\n",
       "      <td>-0.210815</td>\n",
       "      <td>0.029429</td>\n",
       "      <td>0.254305</td>\n",
       "      <td>0.263531</td>\n",
       "      <td>-0.083532</td>\n",
       "      <td>-0.193804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192146</td>\n",
       "      <td>-0.099054</td>\n",
       "      <td>-0.170128</td>\n",
       "      <td>0.122793</td>\n",
       "      <td>0.608490</td>\n",
       "      <td>-0.163630</td>\n",
       "      <td>-0.056677</td>\n",
       "      <td>-0.193171</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.068438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.119676</td>\n",
       "      <td>-0.043937</td>\n",
       "      <td>-0.627608</td>\n",
       "      <td>0.607642</td>\n",
       "      <td>0.104763</td>\n",
       "      <td>-1.073921</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>0.139610</td>\n",
       "      <td>-0.922248</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199974</td>\n",
       "      <td>-0.341996</td>\n",
       "      <td>-0.461439</td>\n",
       "      <td>0.509295</td>\n",
       "      <td>0.667607</td>\n",
       "      <td>0.925786</td>\n",
       "      <td>-1.403193</td>\n",
       "      <td>-0.151907</td>\n",
       "      <td>0.095858</td>\n",
       "      <td>0.589328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.078530</td>\n",
       "      <td>-0.080045</td>\n",
       "      <td>0.133523</td>\n",
       "      <td>0.022122</td>\n",
       "      <td>-0.026604</td>\n",
       "      <td>-0.240486</td>\n",
       "      <td>0.235613</td>\n",
       "      <td>0.069911</td>\n",
       "      <td>-0.279483</td>\n",
       "      <td>-0.042784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281703</td>\n",
       "      <td>-0.036707</td>\n",
       "      <td>-0.024702</td>\n",
       "      <td>0.031256</td>\n",
       "      <td>0.310230</td>\n",
       "      <td>0.247846</td>\n",
       "      <td>-0.137188</td>\n",
       "      <td>-0.032623</td>\n",
       "      <td>0.140527</td>\n",
       "      <td>0.178841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.661295</td>\n",
       "      <td>-0.194641</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>-0.617419</td>\n",
       "      <td>-0.199014</td>\n",
       "      <td>-0.805690</td>\n",
       "      <td>-0.169027</td>\n",
       "      <td>0.589883</td>\n",
       "      <td>-0.567419</td>\n",
       "      <td>-0.610936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210333</td>\n",
       "      <td>-0.124826</td>\n",
       "      <td>0.087683</td>\n",
       "      <td>0.125406</td>\n",
       "      <td>-0.345094</td>\n",
       "      <td>0.625434</td>\n",
       "      <td>0.348995</td>\n",
       "      <td>0.012247</td>\n",
       "      <td>-0.238092</td>\n",
       "      <td>0.482757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>-0.104743</td>\n",
       "      <td>-0.415565</td>\n",
       "      <td>0.055697</td>\n",
       "      <td>-0.332032</td>\n",
       "      <td>0.066925</td>\n",
       "      <td>-0.111400</td>\n",
       "      <td>0.077429</td>\n",
       "      <td>0.212885</td>\n",
       "      <td>-0.477265</td>\n",
       "      <td>-0.064670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.382504</td>\n",
       "      <td>-0.237654</td>\n",
       "      <td>-0.339050</td>\n",
       "      <td>-0.134597</td>\n",
       "      <td>-0.088889</td>\n",
       "      <td>0.019855</td>\n",
       "      <td>-0.021929</td>\n",
       "      <td>0.088538</td>\n",
       "      <td>-0.163266</td>\n",
       "      <td>-0.125755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.127402</td>\n",
       "      <td>-0.160474</td>\n",
       "      <td>-0.285295</td>\n",
       "      <td>-0.338116</td>\n",
       "      <td>-0.312949</td>\n",
       "      <td>0.082670</td>\n",
       "      <td>0.185925</td>\n",
       "      <td>0.372064</td>\n",
       "      <td>-0.337510</td>\n",
       "      <td>-0.193883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175798</td>\n",
       "      <td>0.103974</td>\n",
       "      <td>-0.091314</td>\n",
       "      <td>-0.064058</td>\n",
       "      <td>0.782014</td>\n",
       "      <td>-0.255459</td>\n",
       "      <td>0.272057</td>\n",
       "      <td>-0.090552</td>\n",
       "      <td>0.239165</td>\n",
       "      <td>-0.263405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>-0.131718</td>\n",
       "      <td>-0.028493</td>\n",
       "      <td>-0.081210</td>\n",
       "      <td>0.061198</td>\n",
       "      <td>0.148535</td>\n",
       "      <td>-0.283429</td>\n",
       "      <td>0.099778</td>\n",
       "      <td>0.138635</td>\n",
       "      <td>-0.270641</td>\n",
       "      <td>-0.029851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019922</td>\n",
       "      <td>-0.075652</td>\n",
       "      <td>-0.227786</td>\n",
       "      <td>0.210498</td>\n",
       "      <td>0.046898</td>\n",
       "      <td>0.099748</td>\n",
       "      <td>-0.272432</td>\n",
       "      <td>0.123259</td>\n",
       "      <td>0.119854</td>\n",
       "      <td>0.177666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.163080</td>\n",
       "      <td>-0.291890</td>\n",
       "      <td>0.263199</td>\n",
       "      <td>-0.252177</td>\n",
       "      <td>-0.298799</td>\n",
       "      <td>-0.030710</td>\n",
       "      <td>0.146966</td>\n",
       "      <td>0.290129</td>\n",
       "      <td>-0.757914</td>\n",
       "      <td>-0.040616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220033</td>\n",
       "      <td>-0.130803</td>\n",
       "      <td>-0.144771</td>\n",
       "      <td>-0.220663</td>\n",
       "      <td>0.313438</td>\n",
       "      <td>0.263070</td>\n",
       "      <td>-0.054996</td>\n",
       "      <td>-0.580409</td>\n",
       "      <td>0.117052</td>\n",
       "      <td>0.397756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>-0.293795</td>\n",
       "      <td>-0.535065</td>\n",
       "      <td>0.150585</td>\n",
       "      <td>-0.111425</td>\n",
       "      <td>-0.313781</td>\n",
       "      <td>-0.219696</td>\n",
       "      <td>0.357552</td>\n",
       "      <td>0.481115</td>\n",
       "      <td>-0.318925</td>\n",
       "      <td>-0.014001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184683</td>\n",
       "      <td>0.135488</td>\n",
       "      <td>-0.313899</td>\n",
       "      <td>-0.055588</td>\n",
       "      <td>0.167130</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.447767</td>\n",
       "      <td>-0.326400</td>\n",
       "      <td>0.288974</td>\n",
       "      <td>0.053092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.120934 -0.272334 -0.213747  0.160538 -0.362386 -0.023036  0.053300   \n",
       "1    0.342089  0.158022  0.298481 -0.433624 -0.210815  0.029429  0.254305   \n",
       "2   -0.119676 -0.043937 -0.627608  0.607642  0.104763 -1.073921  0.028449   \n",
       "3   -0.078530 -0.080045  0.133523  0.022122 -0.026604 -0.240486  0.235613   \n",
       "4   -0.661295 -0.194641  0.002163 -0.617419 -0.199014 -0.805690 -0.169027   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "911 -0.104743 -0.415565  0.055697 -0.332032  0.066925 -0.111400  0.077429   \n",
       "912  0.127402 -0.160474 -0.285295 -0.338116 -0.312949  0.082670  0.185925   \n",
       "913 -0.131718 -0.028493 -0.081210  0.061198  0.148535 -0.283429  0.099778   \n",
       "914  0.163080 -0.291890  0.263199 -0.252177 -0.298799 -0.030710  0.146966   \n",
       "915 -0.293795 -0.535065  0.150585 -0.111425 -0.313781 -0.219696  0.357552   \n",
       "\n",
       "           7         8         9   ...        40        41        42  \\\n",
       "0    0.385390 -0.240347 -0.166854  ...  0.441486 -0.144993  0.273198   \n",
       "1    0.263531 -0.083532 -0.193804  ...  0.192146 -0.099054 -0.170128   \n",
       "2    0.139610 -0.922248  0.001086  ...  0.199974 -0.341996 -0.461439   \n",
       "3    0.069911 -0.279483 -0.042784  ...  0.281703 -0.036707 -0.024702   \n",
       "4    0.589883 -0.567419 -0.610936  ...  0.210333 -0.124826  0.087683   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "911  0.212885 -0.477265 -0.064670  ... -0.382504 -0.237654 -0.339050   \n",
       "912  0.372064 -0.337510 -0.193883  ... -0.175798  0.103974 -0.091314   \n",
       "913  0.138635 -0.270641 -0.029851  ... -0.019922 -0.075652 -0.227786   \n",
       "914  0.290129 -0.757914 -0.040616  ...  0.220033 -0.130803 -0.144771   \n",
       "915  0.481115 -0.318925 -0.014001  ...  0.184683  0.135488 -0.313899   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0    0.254902  0.362145 -0.225160 -0.283453 -0.693637  0.617757  0.262902  \n",
       "1    0.122793  0.608490 -0.163630 -0.056677 -0.193171  0.023658  0.068438  \n",
       "2    0.509295  0.667607  0.925786 -1.403193 -0.151907  0.095858  0.589328  \n",
       "3    0.031256  0.310230  0.247846 -0.137188 -0.032623  0.140527  0.178841  \n",
       "4    0.125406 -0.345094  0.625434  0.348995  0.012247 -0.238092  0.482757  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "911 -0.134597 -0.088889  0.019855 -0.021929  0.088538 -0.163266 -0.125755  \n",
       "912 -0.064058  0.782014 -0.255459  0.272057 -0.090552  0.239165 -0.263405  \n",
       "913  0.210498  0.046898  0.099748 -0.272432  0.123259  0.119854  0.177666  \n",
       "914 -0.220663  0.313438  0.263070 -0.054996 -0.580409  0.117052  0.397756  \n",
       "915 -0.055588  0.167130  0.803591  0.447767 -0.326400  0.288974  0.053092  \n",
       "\n",
       "[916 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089501</td>\n",
       "      <td>-0.068648</td>\n",
       "      <td>0.106524</td>\n",
       "      <td>-0.031095</td>\n",
       "      <td>-0.056103</td>\n",
       "      <td>-0.177080</td>\n",
       "      <td>0.079694</td>\n",
       "      <td>0.146501</td>\n",
       "      <td>-0.282229</td>\n",
       "      <td>-0.053514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346693</td>\n",
       "      <td>0.049139</td>\n",
       "      <td>-0.078478</td>\n",
       "      <td>-0.142771</td>\n",
       "      <td>0.434282</td>\n",
       "      <td>0.142178</td>\n",
       "      <td>-0.107108</td>\n",
       "      <td>-0.269205</td>\n",
       "      <td>0.275132</td>\n",
       "      <td>-0.019120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.073935</td>\n",
       "      <td>-0.095224</td>\n",
       "      <td>0.104653</td>\n",
       "      <td>-0.047193</td>\n",
       "      <td>0.061377</td>\n",
       "      <td>-0.188006</td>\n",
       "      <td>0.031351</td>\n",
       "      <td>0.210131</td>\n",
       "      <td>-0.136192</td>\n",
       "      <td>0.054938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353324</td>\n",
       "      <td>0.046131</td>\n",
       "      <td>-0.159116</td>\n",
       "      <td>-0.114294</td>\n",
       "      <td>0.309045</td>\n",
       "      <td>0.205111</td>\n",
       "      <td>-0.109338</td>\n",
       "      <td>-0.273062</td>\n",
       "      <td>0.329051</td>\n",
       "      <td>0.023312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087000</td>\n",
       "      <td>-0.134009</td>\n",
       "      <td>0.099369</td>\n",
       "      <td>-0.090534</td>\n",
       "      <td>-0.115121</td>\n",
       "      <td>-0.152952</td>\n",
       "      <td>0.125228</td>\n",
       "      <td>0.368317</td>\n",
       "      <td>-0.137210</td>\n",
       "      <td>-0.186718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273294</td>\n",
       "      <td>0.136558</td>\n",
       "      <td>-0.162705</td>\n",
       "      <td>-0.069292</td>\n",
       "      <td>0.413839</td>\n",
       "      <td>0.139553</td>\n",
       "      <td>-0.045889</td>\n",
       "      <td>-0.297150</td>\n",
       "      <td>0.277098</td>\n",
       "      <td>0.136596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039053</td>\n",
       "      <td>-0.035644</td>\n",
       "      <td>-0.039971</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>-0.104959</td>\n",
       "      <td>0.086723</td>\n",
       "      <td>0.051067</td>\n",
       "      <td>0.044136</td>\n",
       "      <td>-0.034401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116721</td>\n",
       "      <td>0.059777</td>\n",
       "      <td>-0.092917</td>\n",
       "      <td>-0.054090</td>\n",
       "      <td>0.181757</td>\n",
       "      <td>0.181376</td>\n",
       "      <td>-0.168256</td>\n",
       "      <td>-0.178243</td>\n",
       "      <td>0.117216</td>\n",
       "      <td>-0.072450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130935</td>\n",
       "      <td>-0.109966</td>\n",
       "      <td>0.069441</td>\n",
       "      <td>-0.060627</td>\n",
       "      <td>-0.151118</td>\n",
       "      <td>-0.268295</td>\n",
       "      <td>0.103973</td>\n",
       "      <td>0.118531</td>\n",
       "      <td>-0.154136</td>\n",
       "      <td>-0.074524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403310</td>\n",
       "      <td>0.145778</td>\n",
       "      <td>-0.069893</td>\n",
       "      <td>-0.291150</td>\n",
       "      <td>0.441288</td>\n",
       "      <td>0.099511</td>\n",
       "      <td>-0.284807</td>\n",
       "      <td>-0.303876</td>\n",
       "      <td>0.100681</td>\n",
       "      <td>-0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-0.008875</td>\n",
       "      <td>0.014244</td>\n",
       "      <td>0.021254</td>\n",
       "      <td>0.039276</td>\n",
       "      <td>-0.190709</td>\n",
       "      <td>-0.076930</td>\n",
       "      <td>0.048018</td>\n",
       "      <td>0.206657</td>\n",
       "      <td>-0.578938</td>\n",
       "      <td>-0.086353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278364</td>\n",
       "      <td>0.033156</td>\n",
       "      <td>0.073966</td>\n",
       "      <td>-0.247049</td>\n",
       "      <td>0.220574</td>\n",
       "      <td>-0.200218</td>\n",
       "      <td>0.090612</td>\n",
       "      <td>-0.050317</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.182068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.040343</td>\n",
       "      <td>-0.040289</td>\n",
       "      <td>0.035690</td>\n",
       "      <td>0.101325</td>\n",
       "      <td>-0.216127</td>\n",
       "      <td>-0.025764</td>\n",
       "      <td>-0.034955</td>\n",
       "      <td>0.151634</td>\n",
       "      <td>-0.377170</td>\n",
       "      <td>-0.096361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176631</td>\n",
       "      <td>0.049414</td>\n",
       "      <td>0.043454</td>\n",
       "      <td>-0.245958</td>\n",
       "      <td>0.099932</td>\n",
       "      <td>-0.316820</td>\n",
       "      <td>0.026989</td>\n",
       "      <td>0.034259</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.102119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.265513</td>\n",
       "      <td>0.126118</td>\n",
       "      <td>0.138410</td>\n",
       "      <td>-0.231560</td>\n",
       "      <td>-0.199830</td>\n",
       "      <td>-0.215398</td>\n",
       "      <td>-0.084431</td>\n",
       "      <td>0.548127</td>\n",
       "      <td>-0.981181</td>\n",
       "      <td>-0.094674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477548</td>\n",
       "      <td>0.128695</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>-0.337244</td>\n",
       "      <td>0.741574</td>\n",
       "      <td>0.147342</td>\n",
       "      <td>0.228597</td>\n",
       "      <td>-0.268356</td>\n",
       "      <td>0.095343</td>\n",
       "      <td>0.388851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>-0.019089</td>\n",
       "      <td>-0.100172</td>\n",
       "      <td>-0.336044</td>\n",
       "      <td>-0.043550</td>\n",
       "      <td>0.168797</td>\n",
       "      <td>0.410069</td>\n",
       "      <td>-0.846937</td>\n",
       "      <td>-0.274739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479199</td>\n",
       "      <td>0.061845</td>\n",
       "      <td>-0.001897</td>\n",
       "      <td>-0.319464</td>\n",
       "      <td>0.449779</td>\n",
       "      <td>-0.039880</td>\n",
       "      <td>0.037208</td>\n",
       "      <td>-0.222805</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.157301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>-0.064387</td>\n",
       "      <td>-0.110328</td>\n",
       "      <td>-0.030307</td>\n",
       "      <td>-0.063829</td>\n",
       "      <td>-0.095372</td>\n",
       "      <td>-0.321008</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>-0.067160</td>\n",
       "      <td>-0.206245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300381</td>\n",
       "      <td>0.148529</td>\n",
       "      <td>-0.054585</td>\n",
       "      <td>-0.078517</td>\n",
       "      <td>0.493299</td>\n",
       "      <td>0.244905</td>\n",
       "      <td>-0.096812</td>\n",
       "      <td>-0.444426</td>\n",
       "      <td>0.301596</td>\n",
       "      <td>0.202210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.089501 -0.068648  0.106524 -0.031095 -0.056103 -0.177080  0.079694   \n",
       "1    0.073935 -0.095224  0.104653 -0.047193  0.061377 -0.188006  0.031351   \n",
       "2    0.087000 -0.134009  0.099369 -0.090534 -0.115121 -0.152952  0.125228   \n",
       "3   -0.039053 -0.035644 -0.039971  0.010325  0.045264 -0.104959  0.086723   \n",
       "4    0.130935 -0.109966  0.069441 -0.060627 -0.151118 -0.268295  0.103973   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "388 -0.008875  0.014244  0.021254  0.039276 -0.190709 -0.076930  0.048018   \n",
       "389  0.040343 -0.040289  0.035690  0.101325 -0.216127 -0.025764 -0.034955   \n",
       "390  0.265513  0.126118  0.138410 -0.231560 -0.199830 -0.215398 -0.084431   \n",
       "391  0.047730  0.011927 -0.019089 -0.100172 -0.336044 -0.043550  0.168797   \n",
       "392 -0.064387 -0.110328 -0.030307 -0.063829 -0.095372 -0.321008  0.256809   \n",
       "\n",
       "           7         8         9   ...        40        41        42  \\\n",
       "0    0.146501 -0.282229 -0.053514  ...  0.346693  0.049139 -0.078478   \n",
       "1    0.210131 -0.136192  0.054938  ...  0.353324  0.046131 -0.159116   \n",
       "2    0.368317 -0.137210 -0.186718  ...  0.273294  0.136558 -0.162705   \n",
       "3    0.051067  0.044136 -0.034401  ...  0.116721  0.059777 -0.092917   \n",
       "4    0.118531 -0.154136 -0.074524  ...  0.403310  0.145778 -0.069893   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "388  0.206657 -0.578938 -0.086353  ...  0.278364  0.033156  0.073966   \n",
       "389  0.151634 -0.377170 -0.096361  ...  0.176631  0.049414  0.043454   \n",
       "390  0.548127 -0.981181 -0.094674  ...  0.477548  0.128695  0.058308   \n",
       "391  0.410069 -0.846937 -0.274739  ...  0.479199  0.061845 -0.001897   \n",
       "392  0.403070 -0.067160 -0.206245  ...  0.300381  0.148529 -0.054585   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0   -0.142771  0.434282  0.142178 -0.107108 -0.269205  0.275132 -0.019120  \n",
       "1   -0.114294  0.309045  0.205111 -0.109338 -0.273062  0.329051  0.023312  \n",
       "2   -0.069292  0.413839  0.139553 -0.045889 -0.297150  0.277098  0.136596  \n",
       "3   -0.054090  0.181757  0.181376 -0.168256 -0.178243  0.117216 -0.072450  \n",
       "4   -0.291150  0.441288  0.099511 -0.284807 -0.303876  0.100681 -0.033898  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "388 -0.247049  0.220574 -0.200218  0.090612 -0.050317  0.006070  0.182068  \n",
       "389 -0.245958  0.099932 -0.316820  0.026989  0.034259  0.008262  0.102119  \n",
       "390 -0.337244  0.741574  0.147342  0.228597 -0.268356  0.095343  0.388851  \n",
       "391 -0.319464  0.449779 -0.039880  0.037208 -0.222805  0.009237  0.157301  \n",
       "392 -0.078517  0.493299  0.244905 -0.096812 -0.444426  0.301596  0.202210  \n",
       "\n",
       "[393 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def do_doc2vec(data):\n",
    "    tagged_documents = []\n",
    "    sentences = [text.split() for text in data['CleanText']]\n",
    "    doc2vec_alldocs = []\n",
    "\n",
    "    # train the doc2vec\n",
    "    for i, doc in enumerate(sentences):\n",
    "        tagged_documents.append(gensim.models.doc2vec.TaggedDocument(doc, [i]))\n",
    "\n",
    "    # initialize the model\n",
    "    d2v = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    # build the vocabulary\n",
    "    d2v.build_vocab(tagged_documents)\n",
    "\n",
    "    # train the doc2vec model\n",
    "    d2v.train(tagged_documents, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "\n",
    "    # create Doc2Vec-matrix out of all documents and pass it into a DataFrame\n",
    "    for i in range(len(tagged_documents)):\n",
    "        doc2vec_alldocs.append(d2v.infer_vector(tagged_documents[i].words))\n",
    "    doc2vec_alldocs_matrix = pd.DataFrame(doc2vec_alldocs)\n",
    "\n",
    "    display(doc2vec_alldocs_matrix)\n",
    "    \n",
    "    return(doc2vec_alldocs_matrix)\n",
    "\n",
    "# call function\n",
    "train_doc2vec_matrix = do_doc2vec(data_train) \n",
    "test_doc2vec_matrix = do_doc2vec(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generating a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step we have to create a neural network to generate a target model that tries to predict the correct label for a text. Therefore we first define the neural network. By compilation we define the loww function which is supposed to be minimized and which optimization method should be used. Before we can fit the model, we have to convert the independent variables to an array. In the end we plot the results and calculate the correlation coefficients for the training and test data. <p>\n",
    "As input for the function we use the prepared dataframe with the preprocessed data and the output of BOW/TF-IDF/LSA/Doc2Vec.\n",
    "\n",
    "\n",
    "<p>\n",
    "Necessary Packages:\n",
    "\n",
    "> tensorflow: Provides various tools for machine learning applications. We need it in the following to generate the neural networks.\n",
    "\n",
    "> matplotlib.pyplot: It is used for plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define function to create a neural network\n",
    "def build_nn(train_X, train_y, test_X, test_y):\n",
    "\n",
    "    # first we define the network, units is the number of neurons, in the first layer we need to tell the model the input shape\n",
    "    neural_network = tf.keras.Sequential([\n",
    "                        # hidden layer\n",
    "                        tf.keras.layers.Dense(units = 100, input_shape = [train_X.shape[1]], activation = 'selu'),\n",
    "                        # output layer - as we want probability predictions, it is important to use the sigmoid activation\n",
    "                        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    # compilation\n",
    "    neural_network.compile(loss = 'binary_crossentropy', optimizer = 'sgd',  metrics = ['accuracy', tf.keras.metrics.Recall()])\n",
    "\n",
    "    # a summary for all parameters which need to be estimated\n",
    "    neural_network.summary()\n",
    "\n",
    "    # convert input to array for the model fit\n",
    "    train_X = np.asarray(train_X)\n",
    "    test_X = np.asarray(test_X)\n",
    "\n",
    "    # we fit the model, epochs is the number of steps which are repeated using gradient descent\n",
    "    history = neural_network.fit(train_X, train_y, epochs = 100, validation_data = (test_X, test_y))\n",
    "    plt.plot(history.history['loss'], label = 'training'), plt.plot(history.history['val_loss'], label = 'test'), plt.legend(loc='lower left'), plt.show()\n",
    "\n",
    "    # correlation coefficient of training and test data\n",
    "    corrcoef_train = np.corrcoef(neural_network.predict(train_X).flatten(), train_y)[0, 1]    \n",
    "    print(\"Correlation coefficient train-data:\", corrcoef_train)\n",
    "    \n",
    "    corrcoef_test = np.corrcoef(neural_network.predict(test_X).flatten(), test_y)[0, 1]\n",
    "    print(\"Correlation coefficient test-data:\", corrcoef_test)\n",
    "\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               449600    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 449,701\n",
      "Trainable params: 449,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 14ms/step - loss: -0.1254 - accuracy: 0.6801 - recall: 0.9891 - val_loss: -0.5774 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1.0248 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1.2407 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1.7906 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1.9567 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2.7035 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -2.8589 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3.8745 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -4.0165 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5.3785 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -5.5074 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -7.3011 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -7.3783 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -9.7029 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -9.7587 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -12.7477 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -12.6987 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -16.5074 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -16.3927 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -21.2339 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -21.0479 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -27.2064 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -26.8971 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -34.7489 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -34.4179 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -44.4945 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -44.1092 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -57.1308 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -56.9098 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -73.9322 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -74.1012 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -96.6078 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -97.4294 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -127.5595 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -129.0895 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -169.6814 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -173.0120 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -228.4014 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -234.0882 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -310.1603 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -320.3018 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -425.9366 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -441.2073 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -588.2828 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -611.3542 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -817.3257 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -854.9489 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1145.4349 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1195.2321 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1604.2120 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1682.7920 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2261.9238 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -2378.5408 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3199.1389 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -3365.2751 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -4529.3164 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -4781.7979 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -6439.6792 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -6801.5356 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -9169.6455 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -9682.5557 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -13056.1006 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -13807.3477 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -18622.8125 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -19658.1992 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -26512.3828 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -27964.8145 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -37723.1484 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -39773.0469 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -53664.0430 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -56733.6094 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -76557.9766 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -80848.5312 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -109081.8203 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -115226.9844 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -155452.2500 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -164281.9375 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -221705.1406 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -234447.5938 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -316492.9062 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -334113.2500 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -450997.1250 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -476176.8125 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -642700.3750 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -680653.5000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -918738.7500 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -970019.3125 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1309501.2500 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1383035.7500 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1866811.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1969857.5000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2659169.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -2812827.7500 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3796053.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -4010892.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5413404.5000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -5713593.5000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -7713326.5000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -8160902.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -11018663.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -11635192.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -15701482.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -16616633.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -22432592.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -23750356.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -32061464.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -33833800.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -45676432.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -48285316.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -65184516.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -68793840.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -92848456.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -98089688.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -132364288.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -139636512.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -188414832.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -198798688.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -268335264.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -283065376.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -382010752.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -403141696.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -544010496.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -574430144.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -775076032.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -817912640.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1103719296.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1165329920.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1573131264.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1666683008.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2249352704.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -2374727680.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3205234688.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -3391124480.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -4577489920.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -4839187456.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -6532954112.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -6916901376.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -9333772288.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -9883909120.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -13343944704.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -14096515072.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -19022415872.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -20121200640.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -27151613952.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -28783628288.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -38836060160.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -40968863744.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -55292354560.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -58604687360.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -79089999872.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -83681083392.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -112933126144.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -119486636032.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -161234960384.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -170206527488.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -229678989312.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -242155339776.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -326702202880.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -344896503808.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -465345544192.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -491333484544.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -662991011840.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -699003633664.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -943151579136.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -995207544832.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1343417090048.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -1420136415232.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1916470558720.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -2027456430080.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2736108863488.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -2895031566336.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3906755362816.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -4119709614080.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5559193960448.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -5873384030208.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -7924924022784.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -8347253211136.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -11262043881472.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -11926901882880.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -16096101924864.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -17007998140416.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -22953804693504.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -24275161448448.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -32750243414016.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -34602007658496.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -46708599816192.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -49362667307008.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -66623536889856.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -70400805437440.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -95005070852096.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -100259250307072.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -135267075751936.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -142700238077952.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -192546588327936.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -203863609049088.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -274988603015168.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -290621176676352.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -392086692036608.0000 - accuracy: 0.6889 - recall: 1.0000 - val_loss: -415264717930496.0000 - val_accuracy: 0.7328 - val_recall: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEDCAYAAAAoWo9tAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAb6klEQVR4nO3de3SV9Z3v8fd372Qn2UkgIeEeMIgUvLSiplRGab21gmOlOtQ5Wk/1HHvocezUmePY6rHVRWfNLGfZcVpXp3a0Y1tbp9ZrtUpbxOqyHq8BEUGQi1oI1xAIZOe6k/07f+wHGjGBkP08+9l75/NaK4vsS57f9+HBj7/89vP7/cw5h4iI5K9I2AWIiEhmFOQiInlOQS4ikucU5CIieU5BLiKS5xTkIiJ5LrQgN7P7zWy3ma0Zwns/bWYrzazXzBYN8PooM2sysx8EU62ISO4Ks0f+U2D+EN+7BbgG+K9BXv9H4MXMSxIRyT+hBblz7kVgb//nzGy6mf3OzFaY2R/NbJb33g+cc6uB1OHHMbMzgPHAsmzULSKSa3JtjPxe4G+dc2cA/wD88EhvNrMI8K/ee0VERqSisAs4yMwqgL8AHjGzg0+XHOXH/gZY6pxr6vczIiIjSs4EOenfDlqdc7OP4WfmAvPM7G+ACiBmZgnn3M1BFCgikotyZmjFOXcAeN/Mvghgaace5We+5Jyb6pyrJz288oBCXERGmjBvP/wl8Aow07t18FrgS8C1ZvYWsBZY6L33k2bWBHwR+A8zWxtW3SIiuca0jK2ISH7LmaEVEREZnlA+7KytrXX19fVhNC0ikrdWrFixxzk39vDnQwny+vp6Ghsbw2haRCRvmdmfBnpeQysiInlOQS4ikucU5CIieU5BLiKS5xTkIiJ5zpcgN7P5ZvaumW0yM02RFxHJooyD3MyiwL8DC4CTgCvM7KRMjysiIkPjx33kc4BNzrn3AMzsIdJrpLzjw7E/ZNVzD9H1/ut+H1Yk7zhfl20e5FgfasMGeN7S35v9+XsMLIJZBOf9iUUgEoVIFItEIVJEJFqERYuxaIxIcYxIcQnRWBlFsTKKSsopLa+gJD6KeEUV8bJStEz1kfkR5JOBrf0eNwGfOvxNZrYYWAwwderUYTXUve73zGl+Ylg/K1IoIjay1kfqcCW0WTkJq6SteAydsVqS8XG4quOIjT2eqskzOW76LOIlsbBLDU3WZnY65+4lvQMQDQ0Nw/qX+Kmv/QT4iZ9licgAXOrPuyr2X1gv/b3DpVI4HC7lcKRwqYPPO1KpPhyOVCqF6+vFpVL0pXpxvX30pXrp6+3FpXrpS3bTm0ySSvbQ29NBX7KbvmQnfV0d9HW347oTuO42rKuVSPd+irv3Ud6zlwntbzEmsZfY7l7YkK6rzZWxOlpPS+UsUsedzZQzLuSEKXVEIiOjJ+9HkG8DpvR7XOc9JyJ5yiJ//vgsJ6MwlSKxZystTetpa3qXvh2rKd+3jhP2/5b46ifoe+sfWGMz2DJpPnWfuZpTPza9oIdnMl7G1syKSP9/8XzSAf4GcKVzbtA1wxsaGpzWWhER3/UlaV7/Es2rfkfllj8wpXsDPS7Ky0VzaG+4ngs+exElRdGwqxw2M1vhnGs4/PmMe+TOuV4z+xrweyAK3H+kEBcRCUy0mLEnn8vYk88FoGPrarY9fx+nv/9rRr12Fc++cTZdn76V+fPOpDhaONNoQtlYQj1yEckm13WArU/fwfg1PwbXx68qruK8a/+ZujHlYZd2TAbrkRfO/5JERAZhpaOYuuifif39m+ypu4Avt/+Ud+9eyHOrNoVdmi8U5CIyYtjoyUz+ykPsnfcdzmEF9Y9fzANPPRt2WRlTkIvIyGLGmPNvoO+/P8W44i4+t+J/8eCyl8KuKiMKchEZkWLT5xH/ytOMivbwyZcW88hLb4dd0rApyEVkxIpOPIXiKx9iWmQ3xy37CkvffD/skoZFQS4iI1rxCZ/GXXoPcyLr6f7137Fjf2fYJR0zBbmIjHixU7/I/oavc6m9wAMP/owwbsvOhIJcRAQYfeGtHCibwuU77+KJNzaHXc4xUZCLiAAUl1LxVz9gWmQXzUv/id0HusKuaMgU5CIinsgJ59A2cxH/wz3Jfzy2NOxyhkxBLiLST+Ul/0JfcTnnvH8Xm5sTYZczJApyEZH+ymtJzf068yJv89Sy/Jj1qSAXETlM+dxr6bES6t79WV6MlSvIRUQOFx9D98l/zSX2Er984c2wqzkqBbmIyAAqP/M1SixJZMVPONCVDLucI1KQi4gMZOxM2urO4a/5Pb96JbfvK1eQi4gMovKcv2WctbL95f/K6dmeCnIRkcFMP58DFcdzSfczrG7aH3Y1g1KQi4gMxozi06/ktMgm/rhyddjVDEpBLiJyBGUfvwSAnrVP5+zwioJcRORIaj/GgfhUTu98lfU728KuZkAKchGRIzGj6MS/ZG5kLctX5ebdKwpyEZGjiH/885RYL61v/zbsUgakIBcROZopn6KruIqT2/5fTi6kpSAXETmaaBGpGZ/jvMibLFu9NexqPkJBLiIyBPFTPk+VtbPlrT+EXcpHKMhFRIZi+nn0WowT9v6R/Z25tfaKglxEZChKKmib+BecH1nJqq2tYVfzIQpyEZEhis86j/rILtZt2Bh2KR+iIBcRGaKS4+YA0P7+ayFX8mEKchGRoZp4Kn1EqdjzFr19qbCrOURBLiIyVLE4baNnckpqA+/uyp3p+gpyEZFjUDT1k3wi8h4rP9gTdimHKMhFRI5B+fQzqbROmja+FXYphyjIRUSOgdV9Mv1NU2O4hfSjIBcRORZjptNdVMlxne+w60BX2NUACnIRkWMTidA9/jROi2xixZ/2hV0NoCAXETlm5cefycesidXvbQu7FEBBLiJyzKJT5xA1R9t7b4RdCpBhkJvZF81srZmlzKzBr6JERHLa5DMAqGpZRU9v+BODMu2RrwEuA170oRYRkfwQH0OifCqfsE1s2dsedjWZBblzbp1z7l2/ihERyRfJiWcwO7KJTbvD3zEoa2PkZrbYzBrNrLG5uTlbzYqIBCI+9TTGWytN25rCLuXoQW5my81szQBfC4+lIefcvc65Budcw9ixY4dfsYhIDigZPxOAxLb1IVcCRUd7g3PugmwUIiKSV2pnpP9sCX9tct1+KCIyHFXH0WdFlLd9QCrlQi0l09sPLzWzJmAu8IyZ/d6fskREcly0iER8ClPdNrbv7wy1lEzvWnnCOVfnnCtxzo13zl3oV2EiIrmub8wJHG87Qr9zRUMrIiLDVDZhJsfZTjbvbA21DgW5iMgwlU2cRcz62Lt9c6h1KMhFRIbLu3MluTvceZEKchGR4apJB3ms9T2cC+/OFQW5iMhwldfQVTyaickmWtp7QitDQS4ikoHu0ceHfueKglxEJANF4z7G8ZHtCnIRkXwVnziL8dbK1h27QqtBQS4ikgHz7lzp3BnenSsKchGRTHh3rliIi2cpyEVEMjFmGikijOnaQnt3byglKMhFRDJRVEJH+WSm2w52hLR4loJcRCRDvVXpxbO2t3aF0r6CXEQkQ9GxM5hmO9jRGs5GzApyEZEMxSfOoMx62N+8PZT2FeQiIhmKjp4MQNfecDZiVpCLiGSqciIAffu3hdK8glxEJFOjJgEQTewMpXkFuYhIpsrHkiJKSeeuUJazVZCLiGQqEqWjpJaaVAsHurI/KUhBLiLig2R8POPZG8qkIAW5iIgfRk1mgu1jRwiTghTkIiI+iFVPZrztZcd+BbmISF4qraljlHWyZ29L1ttWkIuI+ODgpKCOPdmfFKQgFxHxgzcpqDeESUEKchERP3iTgiJtO7LetIJcRMQPXo881pH9SUEKchERP8TidBdVUuta2NeRzGrTCnIREZ/0xCcwwfaxvTW7k4IU5CIifqmcyATby84s30uuIBcR8UlR1STG276sT9NXkIuI+KR0TB1jac36lm8KchERn9ioSRRZisSe7G75piAXEfGLdy95sjW7k4IU5CIifvHuJbcsTwpSkIuI+MXrkcc6d5FKZW9SkIJcRMQv8Vr6rIixroU97d1ZazajIDezO81svZmtNrMnzKzKp7pERPJPJEJP2Tgm2D5aEj3ZazbDn38WOMU59wlgA3BL5iWJiOSv3vIJjGcv+9rzJMidc8uccwd3Gn0VqMu8JBGRPFY5kQm2j70deRLkh/mfwG8He9HMFptZo5k1Njc3+9isiEjuiFZNZoLlWI/czJab2ZoBvhb2e8+tQC/w4GDHcc7d65xrcM41jB071p/qRURyTEn1ZCqsi8T+fVlrs+hob3DOXXCk183sGuBi4HyX7UV4RURyTHR0+hbE3gM7s9bmUYP8SMxsPvAN4DPOuQ5/ShIRyWNlYwBIJrK3CXOmY+Q/ACqBZ81slZn9yIeaRETyV7waANeRvSDPqEfunDvBr0JERAqC1yO3jr1Za1IzO0VE/BRPB3lRd/Y+7FSQi4j4qWQUfRYllmzN2ibMCnIRET+Z0V00mlGpNjqTfVlpUkEuIuKzZEkVVZZgb5YmBSnIRUR8liqtppoE+9qTWWlPQS4i4jOLj6HK2rK23oqCXETEZ5HyGqotkbX1VjK6j1xERD4qVllLCQn2JrKzuYSCXETEZ7HKWiKWpC1xICvtaWhFRMRnEW9SUPeB7CzZrSAXEfGbF+S9WVo4S0EuIuI3b72VVJYWzlKQi4j4zeuRRzpas9KcglxExG/xGgCi3dlZAVFBLiLit7L0muSxnuwsnKUgFxHxW7SYnmgFVbRxoKs38OYU5CIiATi4cFY2ZncqyEVEAtDnLZyVjfVWFOQiIkEoSy+cpR65iEieilaMSffIFeQiIvmpuKI2vQJiFoZWtGiWiEgAiitqiFkH+xKdgbelHrmISADMmxTUfSD4afoKchGRIHjT9JOJPYE3pSAXEQmCN7vTtatHLiKSn7weOZ37Am9KQS4iEgRvKduibgW5iEh+8nrkpclW+lLBLpylIBcRCUKsgj4roooErQHfS64gFxEJghnJWBVVtAU+KUhBLiISkL7SaqotQWtHMtB2FOQiIgFJlVVTZQkS3cGuSa4gFxEJiJWNoZo2BbmISL6KlNdQbQnaAt4lSEEuIhKQaEUNVSRIdAY7Rq7VD0VEAlJcUUvE+uju2B9oO+qRi4gEJOJNCkq17w22nUCPLiIykh1abyXYhbMU5CIiQSkZBUCq60CgzWQU5Gb2j2a22sxWmdkyM5vkV2EiInmvpAIA150ItJlMe+R3Ouc+4ZybDTwN3JZ5SSIiBSJWCYD15HCQO+f6/75QDgS7xJeISD7xeuRBB3nGtx+a2T8BXwb2A+ce4X2LgcUAU6dOzbRZEZHcV5LukRclQ+6Rm9lyM1szwNdCAOfcrc65KcCDwNcGO45z7l7nXINzrmHs2LH+nYGISK4qjpMiQlFvR6DNHLVH7py7YIjHehBYCtyeUUUiIoXCjGS0jFh3O6mUIxKxQJrJ9K6VGf0eLgTWZ1aOiEhhSRZVUEEn7T3BrbeS6Rj5HWY2E0gBfwL+d+YliYgUjr7icsqti0R3L5WlxYG0kVGQO+f+yq9CREQKUao43SNPdPXC6GDa0MxOEZEgxSooty7aAlyTXEEuIhKkkso/98gDoiAXEQlQpLTy0Bh5YG0EdmQRESFalu6Rt3UFt7mENpYQEQlQUdkoSuiiLcBdghTkIiIBisVHE7E+OjvbA2tDQysiIgGKeOutJDuCW5NcQS4iEiRvBcTezrbAmlCQi4gEKZYO8r4uBbmISH7yhlZcgNu9KchFRIJ0MMgD3FxCQS4iEiRvaMUC3LdTQS4iEiSvRx4JcJcgBbmISJC8u1aivQpyEZH85A2tFPe241ww+9MryEVEghSJkoyUUk4XHT19wTQRyFFFROSQ3oObSwS0AqKCXEQkYH1F5VRYJ20BrUmuIBcRCZiLVVBOcGuSK8hFRALmvO3egtolSEEuIhIwK62kkk4S3cGsSZ4z65Enk0mampro6uoKu5ScVlpaSl1dHcXFxWGXIiJDFCmtpJzgxshzJsibmpqorKykvr4eMwu7nJzknKOlpYWmpiamTZsWdjkiMkRFpaMot67C/7Czq6uLmpoahfgRmBk1NTX6rUUkzxTFKwO9/TBneuSAQnwI9Hckkn+ipaOJWpKOzmA6YTnTIxcRKVjeNP2egLZ7U5B7Wltb+eEPf3jMP3fRRRfR2tp6xPfcdtttLF++fJiViUjeO7jdW0CbSyjIPYMFeW/vkce0li5dSlVV1RHf853vfIcLLrggk/JEJJ95S9mmOoMJ8pwaIz9oyW/W8s52f0/4pEmjuP3zJw/6+s0338zmzZuZPXs2xcXFlJaWUl1dzfr169mwYQNf+MIX2Lp1K11dXdxwww0sXrwYgPr6ehobG0kkEixYsICzzz6bl19+mcmTJ/Pkk09SVlbGNddcw8UXX8yiRYuor6/n6quv5je/+Q3JZJJHHnmEWbNm0dzczJVXXsn27duZO3cuzz77LCtWrKC2ttbXvwcRCUHM2yWoO5h9O9Uj99xxxx1Mnz6dVatWceedd7Jy5Uq+//3vs2HDBgDuv/9+VqxYQWNjI3fffTctLS0fOcbGjRu5/vrrWbt2LVVVVTz22GMDtlVbW8vKlSu57rrr+O53vwvAkiVLOO+881i7di2LFi1iy5YtwZ2siGSXN7RCQLsE5WSP/Eg952yZM2fOh+7Vvvvuu3niiScA2Lp1Kxs3bqSmpuZDPzNt2jRmz54NwBlnnMEHH3ww4LEvu+yyQ+95/PHHAXjppZcOHX/+/PlUV1f7eToiEqaD270FtEtQTgZ5LigvLz/0/QsvvMDy5ct55ZVXiMfjnHPOOQPey11SUnLo+2g0Smdn54DHPvi+aDR61DF4ESkAB7d762kP5PAaWvFUVlbS1jbw+NX+/fuprq4mHo+zfv16Xn31Vd/bP+uss3j44YcBWLZsGfv27fO9DREJiRfkxb2JQHYJUo/cU1NTw1lnncUpp5xCWVkZ48ePP/Ta/Pnz+dGPfsSJJ57IzJkzOfPMM31v//bbb+eKK67g5z//OXPnzmXChAlUVlb63o6IhMAbWilznXQlU5TFor4e3oLaQ+5IGhoaXGNj44eeW7duHSeeeGLWa8kV3d3dRKNRioqKeOWVV7juuutYtWrVgO8d6X9XIvmob0kt9yXnc9k3f8y4ytJhHcPMVjjnGg5/Xj3yHLFlyxYuv/xyUqkUsViM++67L+ySRMRHvcUVVCQ7SHT1Ms7nX7YV5DlixowZvPnmm2GXISIBSRWXpzeXCGDhLH3YKSKSBalYBRUEs0uQL0FuZjeamTMzTUMUERlAtLSS+soUVfGY78fOeGjFzKYAnwM0FVFEZBCl5VXMoBkmjfL92H70yP8N+AaQ/dtfRETyRUkF9AQzszOjIDezhcA259xbPtUTmuEuYwvwve99j46ODp8rEpGCEqsIbK2Vowa5mS03szUDfC0E/i9w21AaMrPFZtZoZo3Nzc2Z1u07BbmIBKpkVGA98qOOkTvnBlxI28w+DkwD3vK2H6sDVprZHOfczgGOcy9wL6QnBB2x0d/eDDvfPmrxx2TCx2HBHYO+3H8Z289+9rOMGzeOhx9+mO7ubi699FKWLFlCe3s7l19+OU1NTfT19fHtb3+bXbt2sX37ds4991xqa2t5/vnn/a1bRArDwaGVVAoi/t4wOOwPO51zbwPjDj42sw+ABufcHh/qyro77riDNWvWsGrVKpYtW8ajjz7K66+/jnOOSy65hBdffJHm5mYmTZrEM888A6TXYBk9ejR33XUXzz//vNYOF5HBedP06UlAqb8feObmhKAj9JyzYdmyZSxbtozTTjsNgEQiwcaNG5k3bx433ngj3/zmN7n44ouZN29eqHWKSB4pyYMgd87V+3WssDnnuOWWW/jqV7/6kddWrlzJ0qVL+da3vsX555/PbbcN6SMCERnpSrzwDuADT83s9PRfxvbCCy/k/vvvJ5FI/4Vv27aN3bt3s337duLxOFdddRU33XQTK1eu/MjPiogM6ODQSgDbveXm0EoI+i9ju2DBAq688krmzp0LQEVFBb/4xS/YtGkTN910E5FIhOLiYu655x4AFi9ezPz585k0aZI+7BSRgR0aWvE/yLWMbR7S35VIHmrZDM8tgbP/D0yaPaxDaBlbEZEw1UyHyx8I5NAaIxcRyXM5FeRhDPPkG/0dicjhcibIS0tLaWlpUVAdgXOOlpYWSkuHt02UiBSmnBkjr6uro6mpiVxchyWXlJaWUldXF3YZIpJDcibIi4uLmTZtWthliIjknZwZWhERkeFRkIuI5DkFuYhIngtlZqeZNQN/GuaP1wJ5uVRuhkbieY/Ec4aRed4j8Zzh2M/7OOfc2MOfDCXIM2FmjQNNUS10I/G8R+I5w8g875F4zuDfeWtoRUQkzynIRUTyXD4G+b1hFxCSkXjeI/GcYWSe90g8Z/DpvPNujFxERD4sH3vkIiLSj4JcRCTP5VWQm9l8M3vXzDaZ2c1h1xMEM5tiZs+b2TtmttbMbvCeH2Nmz5rZRu/P6rBr9ZuZRc3sTTN72ns8zcxe8673r8wsFnaNfjOzKjN71MzWm9k6M5tb6NfazP7e+7e9xsx+aWalhXitzex+M9ttZmv6PTfgtbW0u73zX21mpx9LW3kT5GYWBf4dWACcBFxhZieFW1UgeoEbnXMnAWcC13vneTPwnHNuBvCc97jQ3ACs6/f4X4B/c86dAOwDrg2lqmB9H/idc24WcCrp8y/Ya21mk4GvAw3OuVOAKPDfKMxr/VNg/mHPDXZtFwAzvK/FwD3H0lDeBDkwB9jknHvPOdcDPAQsDLkm3znndjjnVnrft5H+D3sy6XP9mfe2nwFfCKXAgJhZHfCXwI+9xwacBzzqvaUQz3k08GngPwGccz3OuVYK/FqTXnW1zMyKgDiwgwK81s65F4G9hz092LVdCDzg0l4Fqsxs4lDbyqcgnwxs7fe4yXuuYJlZPXAa8Bow3jm3w3tpJzA+rLoC8j3gG0DKe1wDtDrner3HhXi9pwHNwE+8IaUfm1k5BXytnXPbgO8CW0gH+H5gBYV/rQ8a7NpmlG/5FOQjiplVAI8Bf+ecO9D/NZe+Z7Rg7hs1s4uB3c65FWHXkmVFwOnAPc6504B2DhtGKcBrXU269zkNmASU89HhhxHBz2ubT0G+DZjS73Gd91zBMbNi0iH+oHPuce/pXQd/1fL+3B1WfQE4C7jEzD4gPWR2Humx4yrv128ozOvdBDQ5517zHj9KOtgL+VpfALzvnGt2ziWBx0lf/0K/1gcNdm0zyrd8CvI3gBnep9sx0h+QPBVyTb7zxob/E1jnnLur30tPAVd7318NPJnt2oLinLvFOVfnnKsnfV3/4Jz7EvA8sMh7W0GdM4Bzbiew1cxmek+dD7xDAV9r0kMqZ5pZ3Pu3fvCcC/pa9zPYtX0K+LJ398qZwP5+QzBH55zLmy/gImADsBm4Nex6AjrHs0n/urUaWOV9XUR6zPg5YCOwHBgTdq0Bnf85wNPe98cDrwObgEeAkrDrC+B8ZwON3vX+NVBd6NcaWAKsB9YAPwdKCvFaA78k/TlAkvRvX9cOdm0BI31X3mbgbdJ39Qy5LU3RFxHJc/k0tCIiIgNQkIuI5DkFuYhInlOQi4jkOQW5iEieU5CLiOQ5BbmISJ77/4uaTyJpM6qoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ts23\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\ts23\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x261b5282ec8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_bow_matrix, data_train['LabelNumber'], test_bow_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 100)               449600    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 449,701\n",
      "Trainable params: 449,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 10ms/step - loss: 0.3415 - accuracy: 0.6769 - recall_3: 0.9825 - val_loss: 0.0504 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -0.2132 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -0.3555 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -0.6059 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -0.6689 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -0.9423 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -0.9548 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1.2746 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1.2535 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1.6399 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1.5924 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2.0658 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1.9894 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2.5687 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -2.4624 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3.1689 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -3.0270 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3.8835 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -3.6932 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -4.7235 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -4.4783 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5.7103 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -5.3909 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -6.8520 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -6.4456 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -8.1695 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -7.6719 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -9.6952 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -9.0590 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -11.4155 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -10.6408 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -13.3718 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -12.4279 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -15.5777 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -14.4332 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -18.0480 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -16.6890 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -20.8209 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -19.1928 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -23.8987 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -21.9826 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -27.3255 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -25.1150 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -31.1695 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -28.6039 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -35.4531 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -32.4583 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -40.1875 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -36.7765 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -45.4979 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -41.5957 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -51.4355 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -47.0254 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -58.1415 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -53.0894 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -65.6482 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -60.0050 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -74.2185 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -67.8335 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -83.9492 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -76.8339 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -95.1496 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -87.1295 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -108.0333 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -99.0428 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -122.9466 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -112.9314 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -140.3816 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -129.1070 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -160.7540 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -147.9855 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -184.5580 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -170.0489 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -212.4529 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -196.4943 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -245.9407 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -227.6048 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -285.3930 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -264.4334 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -332.1761 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -308.6136 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -388.3449 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -361.5617 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -455.7221 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -424.4668 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -535.8496 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -501.0885 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -633.5739 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -592.1152 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -749.7712 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -701.8789 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -889.9769 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -834.1342 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1058.8325 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -993.9969 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1263.2834 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1186.1466 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1508.9969 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1419.7231 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1807.7294 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1702.2388 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2168.9199 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -2042.4707 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2604.1704 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -2454.0056 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3130.5039 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -2949.6345 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3765.3350 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -3553.1646 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -4537.9751 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -4282.3286 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5471.0264 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -5167.2051 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -6603.6431 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -6229.4478 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -7963.7446 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -7523.0938 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -9620.9795 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -9090.0576 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -11626.8027 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -10997.0703 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -14066.8389 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -13313.6943 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -17036.3125 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -16105.0176 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -20610.7832 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -19502.2734 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -24959.0293 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -23631.1191 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -30246.7695 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -28616.3965 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -36634.1094 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -34646.5312 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -44357.4336 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -42036.1484 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -53819.2305 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -50866.6914 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -65119.6914 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -61656.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -78946.4531 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -74770.7266 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -95732.3594 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -90572.9297 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -115985.3906 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -109813.9922 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -140626.6250 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -133093.2812 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -170420.7031 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -161247.3750 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -206505.4375 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -195443.9219 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -250272.1875 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -236429.5312 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -302794.0312 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -287160.9688 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -367705.9062 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -348085.6875 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -445777.3125 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -421953.8438 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -540358.2500 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -511101.1875 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -654548.6875 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -619564.5625 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -793392.3750 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -750760.8125 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -961549.8750 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -910852.2500 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1166520.6250 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1104642.8750 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1414573.8750 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1336998.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1712458.5000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1622090.8750 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2077481.1250 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -1965664.2500 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2517602.7500 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -2383927.7500 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3052805.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -2887711.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3698135.2500 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -3497906.2500 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -4479877.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -4243959.5000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5435407.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -5138936.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -6581936.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -6222736.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -7969172.5000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -7540960.5000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -9657909.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -9142034.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -11707877.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -11087442.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -14198829.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -13425436.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -17193942.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -16308371.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -20888136.0000 - accuracy: 0.6889 - recall_3: 1.0000 - val_loss: -19778876.0000 - val_accuracy: 0.7328 - val_recall_3: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x261b771f508>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_tfidf_matrix, data_train['LabelNumber'], test_tfidf_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 100)               1100      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,201\n",
      "Trainable params: 1,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 8ms/step - loss: 0.2906 - accuracy: 0.6725 - recall_2: 0.9793 - val_loss: -0.0043 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -0.2953 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -0.4195 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -0.6948 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -0.7368 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1.0401 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1.0344 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1.3924 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1.3508 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1.7841 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1.7124 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2.2421 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -2.1400 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2.7858 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -2.6496 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3.4336 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -3.2528 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4.1983 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -3.9700 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5.1038 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -4.8023 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -6.1502 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -5.7765 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7.3719 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -6.9043 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -8.7801 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -8.1978 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -10.3891 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -9.6678 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -12.2103 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -11.3272 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -14.2624 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -13.2103 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -16.5836 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -15.3018 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -19.1581 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -17.6391 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -22.0296 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -20.2266 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -25.2040 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -23.1213 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -28.7546 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -26.3166 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -32.6717 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -29.8874 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -37.0464 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -33.8146 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -41.8690 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -38.1744 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -47.2211 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -43.0573 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -53.2219 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -48.4865 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -59.9157 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -54.5563 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -67.4085 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -61.3840 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -75.8591 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -69.1173 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -85.4514 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -77.9027 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -96.3789 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -87.9128 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -108.8549 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -99.3996 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -123.2154 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -112.6790 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -139.8552 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -127.9398 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -159.0278 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -145.8982 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -181.6185 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -166.9673 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -208.2083 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -191.5217 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -239.2442 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -220.3984 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -275.8070 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -254.5366 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -319.1000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -295.5665 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -371.1794 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -344.0095 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -432.7583 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -401.6414 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -506.0910 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -470.7851 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -594.1205 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -553.7524 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -699.8723 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -652.5925 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -825.9897 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -771.9758 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -978.3332 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -915.8281 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1161.8766 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1087.4205 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1381.0917 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1295.7089 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1647.0420 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1545.6456 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1966.4130 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1844.8066 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2348.9324 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -2209.2205 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2814.8030 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -2647.1162 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3374.6311 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -3171.8435 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4045.1426 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -3811.5662 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4863.6714 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -4586.1650 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5854.1050 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -5520.3252 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7049.3745 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -6644.1401 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -8486.9844 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -7995.1592 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -10214.4404 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -9629.2900 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -12304.8887 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -11615.8447 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -14846.2715 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -13995.6875 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -17889.7305 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -16868.3047 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -21568.1367 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -20352.1289 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -26020.4336 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -24579.8516 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -31432.5879 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -29657.3711 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -37928.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -35859.5273 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -45862.2344 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -43280.4492 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -55364.9844 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -52285.0273 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -66887.1562 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -63144.9922 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -80768.1406 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -76236.2734 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -97528.8359 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -92085.5781 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -117806.1797 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -111367.6406 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -142479.9531 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -134431.2656 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -171985.9531 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -162596.3594 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -208046.8594 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -196672.0625 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -251618.0469 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -237801.3906 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -304249.5000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -287089.9688 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -367297.3125 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -346600.7188 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -443385.8750 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -417958.7500 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -534724.3125 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -505209.1875 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -646452.6250 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -610690.1875 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -781330.3750 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -737327.6250 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -943451.7500 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -892336.3750 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1141906.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1079528.5000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1381221.2500 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1303423.3750 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1667625.6250 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1574375.1250 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2014475.7500 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -1904660.3750 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2437214.7500 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -2304256.7500 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2947907.7500 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -2784369.0000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3562803.7500 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -3364587.2500 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4304953.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -4064100.7500 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5200376.5000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -4918712.5000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -6293858.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -5957698.5000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7623401.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -7207715.0000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -9221549.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -8709659.0000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -11143489.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -10520493.0000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -13460073.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -12702816.0000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -16251275.0000 - accuracy: 0.6889 - recall_2: 1.0000 - val_loss: -15349807.0000 - val_accuracy: 0.7328 - val_recall_2: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x261b5907e48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_lsa_matrix, data_train['LabelNumber'], test_lsa_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 100)               5100      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 8ms/step - loss: 0.1005 - accuracy: 0.6397 - recall_1: 0.9170 - val_loss: -0.3053 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -0.6741 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -0.8017 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1.2067 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1.2471 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1.7618 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1.7475 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2.4263 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -2.3665 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3.2604 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -3.1471 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4.3140 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -4.1221 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -5.6268 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -5.3316 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7.2507 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -6.8272 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -9.2531 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -8.6848 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -11.7362 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -10.9415 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -14.7480 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -13.7305 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -18.4705 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -17.1197 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -23.0002 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -21.2816 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -28.5768 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -26.3261 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -35.3611 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -32.5524 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -43.7707 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -40.3516 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -54.3460 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -49.9902 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -67.4877 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -62.0722 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -84.0367 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -77.3015 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -104.9995 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -96.7914 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -131.9570 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -121.5766 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -166.3969 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -153.6152 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -211.0807 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -195.4863 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -269.6711 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -249.2844 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -345.1312 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -320.1986 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -444.8947 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -413.3185 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -576.0676 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -536.4543 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -749.7823 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -698.2426 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -978.3883 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -910.8585 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1278.7823 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1191.3898 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1675.9357 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1562.6497 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2201.3135 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -2058.4561 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2903.8000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -2713.4402 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3831.6367 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -3585.2605 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5067.8735 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -4732.7412 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -6694.1914 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -6256.4683 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -8853.1191 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -8269.7744 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -11707.7539 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -10940.6162 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -15496.2666 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -14497.5049 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -20542.1836 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -19230.1191 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -27247.6836 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -25520.9922 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -36165.5977 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -33861.7266 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -47997.5156 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -44956.7578 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -63733.0156 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -59601.6484 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -84488.9922 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -79020.7109 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -112028.9844 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -104790.8281 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -148577.0625 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -138967.6875 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -197100.0938 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -184729.4688 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -261940.8281 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -245229.0312 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -347755.6562 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -325999.0625 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -462397.6250 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -432989.0625 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -614180.6250 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -574740.8750 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -815014.1250 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -762374.2500 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1081533.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1013166.4375 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1437437.2500 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1348765.8750 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1913624.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1789800.7500 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2539066.7500 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -2377120.5000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3372473.5000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -3149332.5000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4468187.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -4177672.7500 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5928285.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -5545897.5000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7869386.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -7353603.5000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -10435093.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -9763176.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -13852198.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -12972893.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -18404524.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -17227532.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -24443726.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -22862248.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -32431956.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -30384354.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -43098960.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -40377536.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -57275376.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -53644640.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -76090760.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -71389120.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -101244600.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -94837120.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -134496816.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -126109440.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -178833072.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -167376624.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -237344656.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -221767984.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -314549056.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -294946496.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -418215808.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -391010176.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -554491072.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -519143168.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -735968256.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -688695744.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -976422720.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -916293440.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1299607168.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1217628672.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1726658688.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -1614884352.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2290796032.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -2143554048.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3040166912.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -2843408896.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4032861696.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -3772078080.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5350289920.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -5004936704.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7098736640.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -6640067072.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -9417939968.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -8806585344.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -12490254336.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -11687723008.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -16576755712.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -15501501440.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -21983553536.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -20634990592.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -29269370880.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -27370459136.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -38819008512.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -36344954880.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -51546943488.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -48334090240.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -68558135296.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -64285650944.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -91162796032.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -85229977600.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -120888238080.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -113205542912.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -160608534528.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -150339174400.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -213279866880.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -199839858688.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -283451555840.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -265338912768.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -376392908800.0000 - accuracy: 0.6889 - recall_1: 1.0000 - val_loss: -351842631680.0000 - val_accuracy: 0.7328 - val_recall_1: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x261b711fec8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_doc2vec_matrix, data_train['LabelNumber'], test_doc2vec_matrix, data_test['LabelNumber'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d39680e86271bf209db9aab97c2f2e95b8272895522c86f00c9979eddd8c4165"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
