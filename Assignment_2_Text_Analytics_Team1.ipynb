{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team information\n",
    "\n",
    "|Team-number :| 1|\n",
    "|:----:|:----:|\n",
    "\n",
    "\n",
    "|Name|    E-Mail        |matriculation-nr.|\n",
    "|:----:|:----:|:----:|\n",
    "|Tamara Scherer| schere21@ads.uni-passau.de|104218|\n",
    "|Felix Müller| muell518@ads.uni-passau.de|104227|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from label_studio_sdk import Client\n",
    "#from label_studio_sdk import project\n",
    "#from label_studio_sdk import project\n",
    "#import pandas as pd\n",
    "#LABEL_STUDIO_URL = 'http://132.231.59.226:8080' #this address needs to be the same as the address the label-studio is hosted on.\n",
    "#API_KEY = '1655a8922f821195356a17a3224c0532b091c61d' #please add your personal API_Key here to get your API_Key follow the Pictures below\n",
    "\n",
    "#ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "#ls.check_connection()\n",
    "#pro = project.Project.get_from_id(ls,\"1\")\n",
    "#tasks = project.Project.get_labeled_tasks(pro)\n",
    "#tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labeled data\n",
    "import pickle\n",
    "with open(\"Files/tasks3\", 'rb') as f:\n",
    "    tasks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[QID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_2_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, I think that as it relates to the one, I t...</td>\n",
       "      <td>[AID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>One of the areas obviously which has been a st...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[AID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_1_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11292</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_2_positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_3_no_blame]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11294 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my ...   \n",
       "1       Good afternoon and thanks a lot for taking my...   \n",
       "2      Good afternoon and thanks a lot for taking my ...   \n",
       "3      Good afternoon and thanks a lot for taking my ...   \n",
       "4      No, I think that as it relates to the one, I t...   \n",
       "...                                                  ...   \n",
       "11289  One of the areas obviously which has been a st...   \n",
       "11290  Well, let's start with the first part of your ...   \n",
       "11291  Well, let's start with the first part of your ...   \n",
       "11292  Well, let's start with the first part of your ...   \n",
       "11293  Well, let's start with the first part of your ...   \n",
       "\n",
       "                               Label  \n",
       "0                            [QID_1]  \n",
       "1      [Question_1_Company_specific]  \n",
       "2              [Question_2_specific]  \n",
       "3               [Question_3_neutral]  \n",
       "4                            [AID_1]  \n",
       "...                              ...  \n",
       "11289           [Question_3_neutral]  \n",
       "11290                        [AID_1]  \n",
       "11291            [Answer_1_specific]  \n",
       "11292            [Answer_2_positive]  \n",
       "11293            [Answer_3_no_blame]  \n",
       "\n",
       "[11294 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the necessary texts with the corresponding label\n",
    "df = pd.DataFrame(columns = ['Text', 'Label',])\n",
    "\n",
    "for i in range(len(tasks)): \n",
    "    for j in range(len(tasks[i]['annotations'][0]['result'])): \n",
    "        df = df.append({\n",
    "            'Text' : tasks[i]['annotations'][0]['result'][j]['value']['text'],\n",
    "            'Label' : tasks[i]['annotations'][0]['result'][j]['value']['labels']\n",
    "                        }, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[QID_1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_2_specific]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, I think that as it relates to the one, I t...</td>\n",
       "      <td>[AID_1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>One of the areas obviously which has been a st...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[AID_1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_1_specific]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11292</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_2_positive]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_3_no_blame]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11294 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my ...   \n",
       "1       Good afternoon and thanks a lot for taking my...   \n",
       "2      Good afternoon and thanks a lot for taking my ...   \n",
       "3      Good afternoon and thanks a lot for taking my ...   \n",
       "4      No, I think that as it relates to the one, I t...   \n",
       "...                                                  ...   \n",
       "11289  One of the areas obviously which has been a st...   \n",
       "11290  Well, let's start with the first part of your ...   \n",
       "11291  Well, let's start with the first part of your ...   \n",
       "11292  Well, let's start with the first part of your ...   \n",
       "11293  Well, let's start with the first part of your ...   \n",
       "\n",
       "                               Label  LabelNumber  \n",
       "0                            [QID_1]            0  \n",
       "1      [Question_1_Company_specific]            1  \n",
       "2              [Question_2_specific]            3  \n",
       "3               [Question_3_neutral]            7  \n",
       "4                            [AID_1]            0  \n",
       "...                              ...          ...  \n",
       "11289           [Question_3_neutral]            7  \n",
       "11290                        [AID_1]            0  \n",
       "11291            [Answer_1_specific]            8  \n",
       "11292            [Answer_2_positive]           10  \n",
       "11293            [Answer_3_no_blame]           13  \n",
       "\n",
       "[11294 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign a unique number to each label\n",
    "def categorise(row):  \n",
    "    if row['Label'] == ['Question_1_Company_specific']:\n",
    "        return 1\n",
    "    elif row['Label'] == ['Question_1_Market_related']:\n",
    "        return 2\n",
    "    elif row['Label'] == ['Question_2_specific']:\n",
    "        return 3\n",
    "    elif row['Label'] == ['Question_2_open']:\n",
    "        return 4\n",
    "    elif row['Label'] == ['Question_3_attack']:\n",
    "        return 5\n",
    "    elif row['Label'] == ['Question_3_support']:\n",
    "        return 6\n",
    "    elif row['Label'] == ['Question_3_neutral']:\n",
    "        return 7\n",
    "    elif row['Label'] == ['Answer_1_specific']:\n",
    "        return 8\n",
    "    elif row['Label'] == ['Answer_1_avoid_excuse']:\n",
    "        return 9\n",
    "    elif row['Label'] == ['Answer_2_positive']:\n",
    "        return 10\n",
    "    elif row['Label'] == ['Answer_2_negative']:\n",
    "        return 11\n",
    "    elif row['Label'] == ['Answer_3_blame']:\n",
    "        return 12\n",
    "    elif row['Label'] == ['Answer_3_no_blame']:\n",
    "        return 13\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "# call function and write results in a new column of the dataframe\n",
    "df['LabelNumber'] = df.apply(lambda row: categorise(row), axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter one ID stage (e.g. Question_1_XX)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \n",
       "0     [Question_1_Company_specific]            1  \n",
       "1     [Question_1_Company_specific]            1  \n",
       "2       [Question_1_Market_related]            2  \n",
       "3     [Question_1_Company_specific]            1  \n",
       "4     [Question_1_Company_specific]            1  \n",
       "...                             ...          ...  \n",
       "1304  [Question_1_Company_specific]            1  \n",
       "1305  [Question_1_Company_specific]            1  \n",
       "1306  [Question_1_Company_specific]            1  \n",
       "1307  [Question_1_Company_specific]            1  \n",
       "1308  [Question_1_Company_specific]            1  \n",
       "\n",
       "[1309 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idStage = [1, 2]\n",
    "df = df[df['LabelNumber'].isin(idStage)]\n",
    "\n",
    "# set new index\n",
    "df = df.reset_index()\n",
    "# delete old indices\n",
    "df.pop('index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ts23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# necessary imports for preprocessing steps\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# import a stemmer for english words\n",
    "snowStem = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "# import for tokenization\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>good afternoon thanks lot taking question so r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay thats helpful growth china surpass expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "      <td>hi richard so gross margin looked pretty solid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>coreoncore pretty normal well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay followup ebit dollar growth looked like c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi mark pat thank much taking question could t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>yes im wondering could talk little bit custome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi thank you congrats quarter followup toms qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks squeezing guys question you john know g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>one areas obviously strong growth driver team ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \\\n",
       "0     [Question_1_Company_specific]            1   \n",
       "1     [Question_1_Company_specific]            1   \n",
       "2       [Question_1_Market_related]            2   \n",
       "3     [Question_1_Company_specific]            1   \n",
       "4     [Question_1_Company_specific]            1   \n",
       "...                             ...          ...   \n",
       "1304  [Question_1_Company_specific]            1   \n",
       "1305  [Question_1_Company_specific]            1   \n",
       "1306  [Question_1_Company_specific]            1   \n",
       "1307  [Question_1_Company_specific]            1   \n",
       "1308  [Question_1_Company_specific]            1   \n",
       "\n",
       "                                              CleanText  \n",
       "0     good afternoon thanks lot taking question so r...  \n",
       "1     okay thats helpful growth china surpass expect...  \n",
       "2     hi richard so gross margin looked pretty solid...  \n",
       "3                         coreoncore pretty normal well  \n",
       "4     okay followup ebit dollar growth looked like c...  \n",
       "...                                                 ...  \n",
       "1304  hi mark pat thank much taking question could t...  \n",
       "1305  yes im wondering could talk little bit custome...  \n",
       "1306  hi thank you congrats quarter followup toms qu...  \n",
       "1307  thanks squeezing guys question you john know g...  \n",
       "1308  one areas obviously strong growth driver team ...  \n",
       "\n",
       "[1309 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function that includes all preprocessing steps\n",
    "\n",
    "def do_preprocessing(text_to_clean, remove_stopwords = True, stemming = True):\n",
    "    # remove numbers \n",
    "    text_to_clean = re.sub(r'\\d+', '', text_to_clean)\n",
    "    # remove leading and ending white spaces\n",
    "    text_to_clean = text_to_clean.strip()\n",
    "    # transform text to lower case\n",
    "    text_to_clean = text_to_clean.lower()\n",
    "\n",
    "    if stemming:\n",
    "      # stemming\n",
    "      text_to_clean = snowStem.stem(text_to_clean)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        text_to_clean = ' '.join([w for w in text_to_clean.split() if not(w in en_stopwords)])\n",
    "\n",
    "    # remove punctuation\n",
    "    text_to_clean = text_to_clean.translate(str.maketrans('','', string.punctuation))\n",
    "    \n",
    "    return text_to_clean\n",
    "\n",
    "\n",
    "# call function and write results in new column of the dataframe\n",
    "df.loc[:, 'CleanText'] = df['Text'].apply(\n",
    "    lambda x: do_preprocessing(x, True, True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[good, afternoon, thanks, lot, take, question,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[okay, thats, helpful, growth, china, surpass,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "      <td>[hi, richard, so, gross, margin, look, pretty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[coreoncore, pretty, normal, well]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[okay, followup, ebit, dollar, growth, look, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[hi, mark, pat, thank, much, take, question, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[yes, im, wondering, could, talk, little, bit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[hi, thank, you, congrats, quarter, followup, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[thanks, squeeze, guy, question, you, john, kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, area, obviously, strong, growth, driver,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \\\n",
       "0     [Question_1_Company_specific]            1   \n",
       "1     [Question_1_Company_specific]            1   \n",
       "2       [Question_1_Market_related]            2   \n",
       "3     [Question_1_Company_specific]            1   \n",
       "4     [Question_1_Company_specific]            1   \n",
       "...                             ...          ...   \n",
       "1304  [Question_1_Company_specific]            1   \n",
       "1305  [Question_1_Company_specific]            1   \n",
       "1306  [Question_1_Company_specific]            1   \n",
       "1307  [Question_1_Company_specific]            1   \n",
       "1308  [Question_1_Company_specific]            1   \n",
       "\n",
       "                                              CleanText  \n",
       "0     [good, afternoon, thanks, lot, take, question,...  \n",
       "1     [okay, thats, helpful, growth, china, surpass,...  \n",
       "2     [hi, richard, so, gross, margin, look, pretty,...  \n",
       "3                    [coreoncore, pretty, normal, well]  \n",
       "4     [okay, followup, ebit, dollar, growth, look, l...  \n",
       "...                                                 ...  \n",
       "1304  [hi, mark, pat, thank, much, take, question, c...  \n",
       "1305  [yes, im, wondering, could, talk, little, bit,...  \n",
       "1306  [hi, thank, you, congrats, quarter, followup, ...  \n",
       "1307  [thanks, squeeze, guy, question, you, john, kn...  \n",
       "1308  [one, area, obviously, strong, growth, driver,...  \n",
       "\n",
       "[1309 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for lemmatization\n",
    "def lemmatization(text):\n",
    "    \n",
    "  text = word_tokenize(text)\n",
    "  \n",
    "  result=[]\n",
    "  wordnet = WordNetLemmatizer()\n",
    "  for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.lemmatize(token,pos))\n",
    "\n",
    "  return result\n",
    "\n",
    "# call function\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: lemmatization(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# generate the gensim dictionary\n",
    "dct = corpora.dictionary.Dictionary(df['CleanText']).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNumber</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>good afternoon thanks lot take question so rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, that's very helpful. And then on your gr...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay thats helpful growth china surpass expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Richard. So, on gross margin, it looked pre...</td>\n",
       "      <td>[Question_1_Market_related]</td>\n",
       "      <td>2</td>\n",
       "      <td>hi richard so gross margin look pretty solid w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And the core-on-core was pretty normal for you...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>coreoncore pretty normal well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. And my follow-up is on the EBIT dollar ...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>okay followup ebit dollar growth look like com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Hi. This is Mark for Pat. Thank you so much fo...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi mark pat thank much take question could tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>Yes. I'm just wondering if you could talk a li...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>yes im wondering could talk little bit custome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>Hi, thank you. Congrats on the quarter. Just a...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>hi thank you congrats quarter followup tom que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Thanks for squeezing me in guys. This question...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks squeeze guy question you john know guy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>One of the areas obviously which has been a s...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "      <td>1</td>\n",
       "      <td>one area obviously strong growth driver team m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my...   \n",
       "1     Okay, that's very helpful. And then on your gr...   \n",
       "2     Hi Richard. So, on gross margin, it looked pre...   \n",
       "3     And the core-on-core was pretty normal for you...   \n",
       "4      Okay. And my follow-up is on the EBIT dollar ...   \n",
       "...                                                 ...   \n",
       "1304  Hi. This is Mark for Pat. Thank you so much fo...   \n",
       "1305  Yes. I'm just wondering if you could talk a li...   \n",
       "1306  Hi, thank you. Congrats on the quarter. Just a...   \n",
       "1307  Thanks for squeezing me in guys. This question...   \n",
       "1308   One of the areas obviously which has been a s...   \n",
       "\n",
       "                              Label  LabelNumber  \\\n",
       "0     [Question_1_Company_specific]            1   \n",
       "1     [Question_1_Company_specific]            1   \n",
       "2       [Question_1_Market_related]            2   \n",
       "3     [Question_1_Company_specific]            1   \n",
       "4     [Question_1_Company_specific]            1   \n",
       "...                             ...          ...   \n",
       "1304  [Question_1_Company_specific]            1   \n",
       "1305  [Question_1_Company_specific]            1   \n",
       "1306  [Question_1_Company_specific]            1   \n",
       "1307  [Question_1_Company_specific]            1   \n",
       "1308  [Question_1_Company_specific]            1   \n",
       "\n",
       "                                              CleanText  \n",
       "0     good afternoon thanks lot take question so rec...  \n",
       "1     okay thats helpful growth china surpass expect...  \n",
       "2     hi richard so gross margin look pretty solid w...  \n",
       "3                         coreoncore pretty normal well  \n",
       "4     okay followup ebit dollar growth look like com...  \n",
       "...                                                 ...  \n",
       "1304  hi mark pat thank much take question could tal...  \n",
       "1305  yes im wondering could talk little bit custome...  \n",
       "1306  hi thank you congrats quarter followup tom que...  \n",
       "1307  thanks squeeze guy question you john know guy ...  \n",
       "1308  one area obviously strong growth driver team m...  \n",
       "\n",
       "[1309 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tokens back to strings for further processing\n",
    "string1=\" \"\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: string1.join(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split of training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_fraction = 0.70\n",
    "training_size = int(np.floor(len(df) * training_fraction))\n",
    "\n",
    "data_train, data_test = df[:training_size], df[training_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary  benefit  decision  different  do  drive  give  \\\n",
       "0            1            1        1         1          1   1      2     1   \n",
       "1            0            0        0         0          0   0      0     0   \n",
       "2            0            0        0         0          0   0      0     0   \n",
       "3            0            0        0         0          0   0      0     0   \n",
       "4            0            0        0         0          0   0      0     0   \n",
       "..         ...          ...      ...       ...        ...  ..    ...   ...   \n",
       "911          0            0        0         0          0   0      0     0   \n",
       "912          0            0        0         0          0   0      0     2   \n",
       "913          0            0        0         0          0   0      0     0   \n",
       "914          0            0        0         0          0   0      0     1   \n",
       "915          0            0        0         0          0   0      0     1   \n",
       "\n",
       "     good  growth  ...  realignment  rpo  shed  asics  hock  jericho  optical  \\\n",
       "0       1       2  ...            0    0     0      0     0        0        0   \n",
       "1       0       1  ...            0    0     0      0     0        0        0   \n",
       "2       0       0  ...            0    0     0      0     0        0        0   \n",
       "3       0       0  ...            0    0     0      0     0        0        0   \n",
       "4       0       2  ...            0    0     0      0     0        0        0   \n",
       "..    ...     ...  ...          ...  ...   ...    ...   ...      ...      ...   \n",
       "911     0       0  ...            0    0     0      0     0        0        0   \n",
       "912     0       0  ...            0    0     0      0     0        0        0   \n",
       "913     0       0  ...            0    0     0      0     0        0        0   \n",
       "914     0       0  ...            0    0     0      0     0        0        0   \n",
       "915     0       0  ...            0    0     0      0     0        0        0   \n",
       "\n",
       "     performing  tomahawk  trident  \n",
       "0             0         0        0  \n",
       "1             0         0        0  \n",
       "2             0         0        0  \n",
       "3             0         0        0  \n",
       "4             0         0        0  \n",
       "..          ...       ...      ...  \n",
       "911           0         0        0  \n",
       "912           0         0        0  \n",
       "913           0         0        0  \n",
       "914           0         0        0  \n",
       "915           0         0        0  \n",
       "\n",
       "[916 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary  benefit  decision  different  do  drive  give  \\\n",
       "0            0            0        0         0          0   0      0     0   \n",
       "1            0            0        0         0          0   0      0     0   \n",
       "2            0            0        0         0          0   0      0     0   \n",
       "3            0            0        0         0          0   0      0     0   \n",
       "4            0            0        0         0          1   0      0     1   \n",
       "..         ...          ...      ...       ...        ...  ..    ...   ...   \n",
       "388          0            0        0         0          0   0      0     0   \n",
       "389          0            0        0         0          0   0      0     0   \n",
       "390          0            0        0         0          0   0      0     0   \n",
       "391          0            0        0         0          0   0      0     1   \n",
       "392          0            0        0         0          0   0      0     0   \n",
       "\n",
       "     good  growth  ...  realignment  rpo  shed  asics  hock  jericho  optical  \\\n",
       "0       0       0  ...            0    0     0      0     0        0        0   \n",
       "1       0       0  ...            0    0     0      0     0        0        0   \n",
       "2       0       0  ...            0    0     0      0     0        0        0   \n",
       "3       0       0  ...            0    0     0      0     0        0        0   \n",
       "4       0       0  ...            0    0     0      0     0        0        0   \n",
       "..    ...     ...  ...          ...  ...   ...    ...   ...      ...      ...   \n",
       "388     0       0  ...            0    0     0      0     0        0        0   \n",
       "389     0       0  ...            0    0     0      0     0        0        0   \n",
       "390     0       0  ...            1    0     0      0     0        0        0   \n",
       "391     0       0  ...            0    2     1      0     0        0        0   \n",
       "392     0       1  ...            0    0     0      1     1        1        1   \n",
       "\n",
       "     performing  tomahawk  trident  \n",
       "0             0         0        0  \n",
       "1             0         0        0  \n",
       "2             0         0        0  \n",
       "3             0         0        0  \n",
       "4             0         0        0  \n",
       "..          ...       ...      ...  \n",
       "388           0         0        0  \n",
       "389           0         0        0  \n",
       "390           0         0        0  \n",
       "391           0         0        0  \n",
       "392           1         1        1  \n",
       "\n",
       "[393 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def do_bow(data):\n",
    "    count_vec = CountVectorizer(vocabulary = dct)\n",
    "\n",
    "    bow = count_vec.fit_transform(data['CleanText'])\n",
    "    bow_matrix = pd.DataFrame(data = bow.toarray(), columns = count_vec.get_feature_names_out())\n",
    "\n",
    "    display(bow_matrix)\n",
    "    \n",
    "    return(bow_matrix)\n",
    "\n",
    "# call function\n",
    "train_bow_matrix = do_bow(data_train)\n",
    "test_bow_matrix = do_bow(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.153021</td>\n",
       "      <td>0.236999</td>\n",
       "      <td>0.155793</td>\n",
       "      <td>0.177425</td>\n",
       "      <td>0.126805</td>\n",
       "      <td>0.153021</td>\n",
       "      <td>0.243532</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.115618</td>\n",
       "      <td>0.194891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary   benefit  decision  different        do  \\\n",
       "0     0.153021     0.236999  0.155793  0.177425   0.126805  0.153021   \n",
       "1     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "2     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "3     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "4     0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "..         ...          ...       ...       ...        ...       ...   \n",
       "911   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "912   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "913   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "914   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "915   0.000000     0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "        drive      give      good    growth  ...  realignment  rpo  shed  \\\n",
       "0    0.243532  0.085938  0.115618  0.194891  ...          0.0  0.0   0.0   \n",
       "1    0.000000  0.000000  0.000000  0.174402  ...          0.0  0.0   0.0   \n",
       "2    0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "3    0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "4    0.000000  0.000000  0.000000  0.152685  ...          0.0  0.0   0.0   \n",
       "..        ...       ...       ...       ...  ...          ...  ...   ...   \n",
       "911  0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "912  0.000000  0.237623  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "913  0.000000  0.000000  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "914  0.000000  0.093197  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "915  0.000000  0.085258  0.000000  0.000000  ...          0.0  0.0   0.0   \n",
       "\n",
       "     asics  hock  jericho  optical  performing  tomahawk  trident  \n",
       "0      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "1      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "2      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "3      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "4      0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "..     ...   ...      ...      ...         ...       ...      ...  \n",
       "911    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "912    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "913    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "914    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "915    0.0   0.0      0.0      0.0         0.0       0.0      0.0  \n",
       "\n",
       "[916 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afternoon</th>\n",
       "      <th>anniversary</th>\n",
       "      <th>benefit</th>\n",
       "      <th>decision</th>\n",
       "      <th>different</th>\n",
       "      <th>do</th>\n",
       "      <th>drive</th>\n",
       "      <th>give</th>\n",
       "      <th>good</th>\n",
       "      <th>growth</th>\n",
       "      <th>...</th>\n",
       "      <th>realignment</th>\n",
       "      <th>rpo</th>\n",
       "      <th>shed</th>\n",
       "      <th>asics</th>\n",
       "      <th>hock</th>\n",
       "      <th>jericho</th>\n",
       "      <th>optical</th>\n",
       "      <th>performing</th>\n",
       "      <th>tomahawk</th>\n",
       "      <th>trident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464447</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.157323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 4495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afternoon  anniversary  benefit  decision  different   do  drive  \\\n",
       "0          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "1          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "2          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "3          0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "4          0.0          0.0      0.0       0.0   0.145685  0.0    0.0   \n",
       "..         ...          ...      ...       ...        ...  ...    ...   \n",
       "388        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "389        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "390        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "391        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "392        0.0          0.0      0.0       0.0   0.000000  0.0    0.0   \n",
       "\n",
       "         give  good    growth  ...  realignment       rpo      shed     asics  \\\n",
       "0    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "2    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.087808   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "..        ...   ...       ...  ...          ...       ...       ...       ...   \n",
       "388  0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "389  0.000000   0.0  0.000000  ...     0.000000  0.000000  0.000000  0.000000   \n",
       "390  0.000000   0.0  0.000000  ...     0.180413  0.000000  0.000000  0.000000   \n",
       "391  0.093644   0.0  0.000000  ...     0.000000  0.464447  0.232224  0.000000   \n",
       "392  0.000000   0.0  0.067947  ...     0.000000  0.000000  0.000000  0.157323   \n",
       "\n",
       "         hock   jericho   optical  performing  tomahawk   trident  \n",
       "0    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...         ...       ...       ...  \n",
       "388  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "389  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "390  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "391  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "392  0.157323  0.157323  0.157323    0.157323  0.157323  0.157323  \n",
       "\n",
       "[393 rows x 4495 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def do_tfidf(data):\n",
    "    tfidf_vec = TfidfVectorizer(vocabulary = dct)\n",
    "\n",
    "    tfidf = tfidf_vec.fit_transform(data['CleanText'])\n",
    "    tfidf_matrix = pd.DataFrame(data = tfidf.toarray(), columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "    display(tfidf_matrix)\n",
    "    \n",
    "    return(tfidf_matrix)\n",
    "\n",
    "# call function\n",
    "train_tfidf_matrix = do_tfidf(data_train)\n",
    "test_tfidf_matrix = do_tfidf(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063852</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.017864</td>\n",
       "      <td>0.101711</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>0.051261</td>\n",
       "      <td>-0.006285</td>\n",
       "      <td>0.017381</td>\n",
       "      <td>0.044309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047987</td>\n",
       "      <td>0.020022</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>-0.035792</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.039789</td>\n",
       "      <td>-0.006914</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>0.018086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026689</td>\n",
       "      <td>0.022862</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.027197</td>\n",
       "      <td>-0.011017</td>\n",
       "      <td>-0.065299</td>\n",
       "      <td>-0.003146</td>\n",
       "      <td>-0.033844</td>\n",
       "      <td>0.049235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003002</td>\n",
       "      <td>-0.003607</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.019330</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>-0.007566</td>\n",
       "      <td>-0.002486</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>-0.011877</td>\n",
       "      <td>0.008377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076328</td>\n",
       "      <td>0.032857</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>0.011290</td>\n",
       "      <td>-0.007317</td>\n",
       "      <td>-0.074875</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>-0.090881</td>\n",
       "      <td>-0.057036</td>\n",
       "      <td>0.062475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.009924</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>-0.045003</td>\n",
       "      <td>-0.012187</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>-0.042534</td>\n",
       "      <td>-0.017822</td>\n",
       "      <td>0.017820</td>\n",
       "      <td>0.038597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>-0.016243</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>-0.003163</td>\n",
       "      <td>-0.036650</td>\n",
       "      <td>-0.014578</td>\n",
       "      <td>0.031850</td>\n",
       "      <td>-0.014996</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>0.022267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.031451</td>\n",
       "      <td>0.037696</td>\n",
       "      <td>-0.015868</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>-0.008951</td>\n",
       "      <td>-0.007917</td>\n",
       "      <td>-0.017709</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.017045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>-0.011292</td>\n",
       "      <td>0.023081</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>-0.018354</td>\n",
       "      <td>-0.007153</td>\n",
       "      <td>0.027412</td>\n",
       "      <td>-0.009124</td>\n",
       "      <td>-0.012126</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>0.014872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.024987</td>\n",
       "      <td>-0.024399</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>0.041918</td>\n",
       "      <td>-0.013547</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>-0.024155</td>\n",
       "      <td>-0.016189</td>\n",
       "      <td>0.033829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.063852  0.001987  0.017864  0.101711  0.045427  0.036281  0.051261   \n",
       "1    0.047987  0.020022  0.008672  0.021375 -0.035792  0.001680  0.039789   \n",
       "2    0.026689  0.022862  0.013200  0.021622  0.027197 -0.011017 -0.065299   \n",
       "3   -0.003002 -0.003607 -0.005458  0.019330  0.013714 -0.007566 -0.002486   \n",
       "4    0.076328  0.032857  0.035851  0.011290 -0.007317 -0.074875 -0.005319   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "911  0.012822  0.009924  0.009228 -0.045003 -0.012187  0.011660 -0.042534   \n",
       "912 -0.016243  0.016886 -0.003163 -0.036650 -0.014578  0.031850 -0.014996   \n",
       "913  0.004968  0.031451  0.037696 -0.015868  0.002805 -0.008951 -0.007917   \n",
       "914 -0.011292  0.023081  0.005894 -0.018354 -0.007153  0.027412 -0.009124   \n",
       "915  0.005831  0.024987 -0.024399  0.022537  0.041918 -0.013547  0.013452   \n",
       "\n",
       "            7         8         9  \n",
       "0   -0.006285  0.017381  0.044309  \n",
       "1   -0.006914  0.015449  0.018086  \n",
       "2   -0.003146 -0.033844  0.049235  \n",
       "3    0.010854 -0.011877  0.008377  \n",
       "4   -0.090881 -0.057036  0.062475  \n",
       "..        ...       ...       ...  \n",
       "911 -0.017822  0.017820  0.038597  \n",
       "912  0.002250  0.003314  0.022267  \n",
       "913 -0.017709  0.000577  0.017045  \n",
       "914 -0.012126 -0.001633  0.014872  \n",
       "915 -0.024155 -0.016189  0.033829  \n",
       "\n",
       "[916 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003787</td>\n",
       "      <td>0.036151</td>\n",
       "      <td>-0.007546</td>\n",
       "      <td>-0.012739</td>\n",
       "      <td>-0.003463</td>\n",
       "      <td>0.007904</td>\n",
       "      <td>-0.004376</td>\n",
       "      <td>-0.007969</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>-0.017286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003623</td>\n",
       "      <td>-0.010465</td>\n",
       "      <td>-0.016374</td>\n",
       "      <td>0.023581</td>\n",
       "      <td>0.063199</td>\n",
       "      <td>0.061785</td>\n",
       "      <td>-0.001506</td>\n",
       "      <td>0.028464</td>\n",
       "      <td>-0.013361</td>\n",
       "      <td>-0.032609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100248</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.033927</td>\n",
       "      <td>0.012630</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>-0.027062</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>-0.041443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.048395</td>\n",
       "      <td>0.064332</td>\n",
       "      <td>0.107475</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>0.091670</td>\n",
       "      <td>0.121379</td>\n",
       "      <td>0.023276</td>\n",
       "      <td>-0.035205</td>\n",
       "      <td>0.030819</td>\n",
       "      <td>-0.048567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.056217</td>\n",
       "      <td>0.127080</td>\n",
       "      <td>0.056279</td>\n",
       "      <td>0.160083</td>\n",
       "      <td>0.100818</td>\n",
       "      <td>0.076546</td>\n",
       "      <td>-0.001372</td>\n",
       "      <td>-0.058783</td>\n",
       "      <td>-0.028786</td>\n",
       "      <td>-0.032502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-0.049372</td>\n",
       "      <td>-0.106876</td>\n",
       "      <td>-0.133016</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-0.119362</td>\n",
       "      <td>0.064732</td>\n",
       "      <td>0.067216</td>\n",
       "      <td>-0.153516</td>\n",
       "      <td>-0.105204</td>\n",
       "      <td>-0.070174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>-0.082668</td>\n",
       "      <td>-0.025952</td>\n",
       "      <td>-0.129962</td>\n",
       "      <td>-0.023329</td>\n",
       "      <td>-0.034594</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.077375</td>\n",
       "      <td>-0.149159</td>\n",
       "      <td>-0.101139</td>\n",
       "      <td>-0.048324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.077289</td>\n",
       "      <td>-0.033382</td>\n",
       "      <td>0.031542</td>\n",
       "      <td>-0.057291</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>0.019505</td>\n",
       "      <td>0.065074</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>-0.052925</td>\n",
       "      <td>-0.054011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.000596</td>\n",
       "      <td>-0.030342</td>\n",
       "      <td>-0.037173</td>\n",
       "      <td>-0.004155</td>\n",
       "      <td>-0.058917</td>\n",
       "      <td>-0.033699</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>-0.106086</td>\n",
       "      <td>-0.040183</td>\n",
       "      <td>-0.072156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>-0.038896</td>\n",
       "      <td>-0.055331</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>-0.017200</td>\n",
       "      <td>0.023066</td>\n",
       "      <td>-0.027363</td>\n",
       "      <td>-0.049996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.003787  0.036151 -0.007546 -0.012739 -0.003463  0.007904 -0.004376   \n",
       "1   -0.003623 -0.010465 -0.016374  0.023581  0.063199  0.061785 -0.001506   \n",
       "2    0.100248  0.064394  0.033927  0.012630  0.018216  0.003165 -0.027062   \n",
       "3   -0.048395  0.064332  0.107475  0.191095  0.091670  0.121379  0.023276   \n",
       "4   -0.056217  0.127080  0.056279  0.160083  0.100818  0.076546 -0.001372   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "388 -0.049372 -0.106876 -0.133016 -0.001300 -0.119362  0.064732  0.067216   \n",
       "389 -0.082668 -0.025952 -0.129962 -0.023329 -0.034594 -0.000819  0.077375   \n",
       "390  0.077289 -0.033382  0.031542 -0.057291  0.020551  0.019505  0.065074   \n",
       "391  0.000596 -0.030342 -0.037173 -0.004155 -0.058917 -0.033699  0.011284   \n",
       "392 -0.038896 -0.055331  0.037653  0.025691  0.014925  0.009770 -0.017200   \n",
       "\n",
       "            7         8         9  \n",
       "0   -0.007969  0.003698 -0.017286  \n",
       "1    0.028464 -0.013361 -0.032609  \n",
       "2    0.008933  0.002267 -0.041443  \n",
       "3   -0.035205  0.030819 -0.048567  \n",
       "4   -0.058783 -0.028786 -0.032502  \n",
       "..        ...       ...       ...  \n",
       "388 -0.153516 -0.105204 -0.070174  \n",
       "389 -0.149159 -0.101139 -0.048324  \n",
       "390  0.020076 -0.052925 -0.054011  \n",
       "391 -0.106086 -0.040183 -0.072156  \n",
       "392  0.023066 -0.027363 -0.049996  \n",
       "\n",
       "[393 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def do_lsa(data):\n",
    "    num_components = 10\n",
    "    q, s, p = svds(data, k = num_components)\n",
    "\n",
    "    lsa_doc_matrix = pd.DataFrame(data=q)\n",
    "\n",
    "    display(lsa_doc_matrix)\n",
    "\n",
    "    return(lsa_doc_matrix)\n",
    "\n",
    "# call function\n",
    "train_lsa_matrix = do_lsa(train_tfidf_matrix)\n",
    "test_lsa_matrix = do_lsa(test_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200016</td>\n",
       "      <td>-0.266835</td>\n",
       "      <td>-0.202771</td>\n",
       "      <td>-0.079907</td>\n",
       "      <td>-0.481641</td>\n",
       "      <td>-0.224505</td>\n",
       "      <td>0.018468</td>\n",
       "      <td>0.297934</td>\n",
       "      <td>-0.252071</td>\n",
       "      <td>-0.296664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413823</td>\n",
       "      <td>-0.046901</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.305346</td>\n",
       "      <td>0.388603</td>\n",
       "      <td>-0.047301</td>\n",
       "      <td>-0.254862</td>\n",
       "      <td>-0.508278</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.267213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395876</td>\n",
       "      <td>0.278250</td>\n",
       "      <td>0.064187</td>\n",
       "      <td>-0.344207</td>\n",
       "      <td>-0.070575</td>\n",
       "      <td>-0.029941</td>\n",
       "      <td>0.274239</td>\n",
       "      <td>0.178708</td>\n",
       "      <td>-0.034337</td>\n",
       "      <td>-0.244876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323575</td>\n",
       "      <td>-0.047063</td>\n",
       "      <td>-0.339047</td>\n",
       "      <td>0.100870</td>\n",
       "      <td>0.659238</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>-0.133802</td>\n",
       "      <td>-0.091845</td>\n",
       "      <td>0.030011</td>\n",
       "      <td>0.132194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.223808</td>\n",
       "      <td>-0.273745</td>\n",
       "      <td>-0.219473</td>\n",
       "      <td>0.737178</td>\n",
       "      <td>-0.055497</td>\n",
       "      <td>-0.770718</td>\n",
       "      <td>-0.351303</td>\n",
       "      <td>0.169209</td>\n",
       "      <td>-0.853221</td>\n",
       "      <td>-0.446385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239485</td>\n",
       "      <td>-0.803314</td>\n",
       "      <td>-0.184250</td>\n",
       "      <td>0.513825</td>\n",
       "      <td>0.447627</td>\n",
       "      <td>0.554507</td>\n",
       "      <td>-1.393644</td>\n",
       "      <td>0.128506</td>\n",
       "      <td>0.397678</td>\n",
       "      <td>0.663884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033675</td>\n",
       "      <td>-0.140648</td>\n",
       "      <td>0.179587</td>\n",
       "      <td>0.034676</td>\n",
       "      <td>-0.056587</td>\n",
       "      <td>-0.162967</td>\n",
       "      <td>0.167157</td>\n",
       "      <td>0.087846</td>\n",
       "      <td>-0.235652</td>\n",
       "      <td>-0.047186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257129</td>\n",
       "      <td>-0.044700</td>\n",
       "      <td>-0.011728</td>\n",
       "      <td>-0.031328</td>\n",
       "      <td>0.301128</td>\n",
       "      <td>0.237756</td>\n",
       "      <td>-0.159807</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>0.126591</td>\n",
       "      <td>0.187225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.497599</td>\n",
       "      <td>0.164381</td>\n",
       "      <td>0.124045</td>\n",
       "      <td>-0.500188</td>\n",
       "      <td>-0.262429</td>\n",
       "      <td>-0.919856</td>\n",
       "      <td>-0.329014</td>\n",
       "      <td>0.447325</td>\n",
       "      <td>-0.884699</td>\n",
       "      <td>-0.852631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629108</td>\n",
       "      <td>-0.170686</td>\n",
       "      <td>0.175457</td>\n",
       "      <td>0.430093</td>\n",
       "      <td>-0.398542</td>\n",
       "      <td>0.727709</td>\n",
       "      <td>0.266551</td>\n",
       "      <td>0.037141</td>\n",
       "      <td>-0.020469</td>\n",
       "      <td>0.553227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>-0.024980</td>\n",
       "      <td>-0.342036</td>\n",
       "      <td>0.107796</td>\n",
       "      <td>-0.261170</td>\n",
       "      <td>0.212517</td>\n",
       "      <td>-0.085273</td>\n",
       "      <td>0.239805</td>\n",
       "      <td>0.273253</td>\n",
       "      <td>-0.517089</td>\n",
       "      <td>0.055134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214391</td>\n",
       "      <td>-0.249838</td>\n",
       "      <td>-0.344393</td>\n",
       "      <td>-0.137296</td>\n",
       "      <td>0.076680</td>\n",
       "      <td>-0.073037</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>-0.021565</td>\n",
       "      <td>-0.013196</td>\n",
       "      <td>-0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.012540</td>\n",
       "      <td>-0.139558</td>\n",
       "      <td>-0.219331</td>\n",
       "      <td>-0.307537</td>\n",
       "      <td>-0.250954</td>\n",
       "      <td>0.022609</td>\n",
       "      <td>0.279431</td>\n",
       "      <td>0.301093</td>\n",
       "      <td>-0.389456</td>\n",
       "      <td>-0.212140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242337</td>\n",
       "      <td>0.104793</td>\n",
       "      <td>-0.043100</td>\n",
       "      <td>-0.003433</td>\n",
       "      <td>0.787824</td>\n",
       "      <td>-0.192636</td>\n",
       "      <td>0.148812</td>\n",
       "      <td>-0.158076</td>\n",
       "      <td>0.375436</td>\n",
       "      <td>-0.184489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>-0.153578</td>\n",
       "      <td>-0.111851</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.108829</td>\n",
       "      <td>0.214134</td>\n",
       "      <td>-0.207674</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>0.091311</td>\n",
       "      <td>-0.267190</td>\n",
       "      <td>-0.115238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012002</td>\n",
       "      <td>-0.187554</td>\n",
       "      <td>-0.169541</td>\n",
       "      <td>0.335971</td>\n",
       "      <td>-0.072328</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.392746</td>\n",
       "      <td>0.319814</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>0.189739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.254736</td>\n",
       "      <td>-0.195078</td>\n",
       "      <td>0.240308</td>\n",
       "      <td>-0.248409</td>\n",
       "      <td>-0.198231</td>\n",
       "      <td>-0.094390</td>\n",
       "      <td>0.376326</td>\n",
       "      <td>0.295407</td>\n",
       "      <td>-0.645206</td>\n",
       "      <td>0.056201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155870</td>\n",
       "      <td>-0.096942</td>\n",
       "      <td>-0.164312</td>\n",
       "      <td>-0.201865</td>\n",
       "      <td>0.551706</td>\n",
       "      <td>0.232849</td>\n",
       "      <td>-0.145245</td>\n",
       "      <td>-0.448189</td>\n",
       "      <td>0.033886</td>\n",
       "      <td>0.354401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>-0.143492</td>\n",
       "      <td>-0.609700</td>\n",
       "      <td>0.331317</td>\n",
       "      <td>-0.194064</td>\n",
       "      <td>-0.395265</td>\n",
       "      <td>-0.112965</td>\n",
       "      <td>0.413819</td>\n",
       "      <td>0.422376</td>\n",
       "      <td>-0.507184</td>\n",
       "      <td>0.158062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270924</td>\n",
       "      <td>-0.085845</td>\n",
       "      <td>-0.222353</td>\n",
       "      <td>-0.112073</td>\n",
       "      <td>0.085204</td>\n",
       "      <td>0.712144</td>\n",
       "      <td>0.451307</td>\n",
       "      <td>-0.275012</td>\n",
       "      <td>0.547725</td>\n",
       "      <td>0.137799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.200016 -0.266835 -0.202771 -0.079907 -0.481641 -0.224505  0.018468   \n",
       "1    0.395876  0.278250  0.064187 -0.344207 -0.070575 -0.029941  0.274239   \n",
       "2    0.223808 -0.273745 -0.219473  0.737178 -0.055497 -0.770718 -0.351303   \n",
       "3    0.033675 -0.140648  0.179587  0.034676 -0.056587 -0.162967  0.167157   \n",
       "4   -0.497599  0.164381  0.124045 -0.500188 -0.262429 -0.919856 -0.329014   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "911 -0.024980 -0.342036  0.107796 -0.261170  0.212517 -0.085273  0.239805   \n",
       "912  0.012540 -0.139558 -0.219331 -0.307537 -0.250954  0.022609  0.279431   \n",
       "913 -0.153578 -0.111851  0.016114  0.108829  0.214134 -0.207674  0.008858   \n",
       "914  0.254736 -0.195078  0.240308 -0.248409 -0.198231 -0.094390  0.376326   \n",
       "915 -0.143492 -0.609700  0.331317 -0.194064 -0.395265 -0.112965  0.413819   \n",
       "\n",
       "           7         8         9   ...        40        41        42  \\\n",
       "0    0.297934 -0.252071 -0.296664  ...  0.413823 -0.046901  0.225000   \n",
       "1    0.178708 -0.034337 -0.244876  ...  0.323575 -0.047063 -0.339047   \n",
       "2    0.169209 -0.853221 -0.446385  ...  0.239485 -0.803314 -0.184250   \n",
       "3    0.087846 -0.235652 -0.047186  ...  0.257129 -0.044700 -0.011728   \n",
       "4    0.447325 -0.884699 -0.852631  ...  0.629108 -0.170686  0.175457   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "911  0.273253 -0.517089  0.055134  ... -0.214391 -0.249838 -0.344393   \n",
       "912  0.301093 -0.389456 -0.212140  ... -0.242337  0.104793 -0.043100   \n",
       "913  0.091311 -0.267190 -0.115238  ... -0.012002 -0.187554 -0.169541   \n",
       "914  0.295407 -0.645206  0.056201  ...  0.155870 -0.096942 -0.164312   \n",
       "915  0.422376 -0.507184  0.158062  ...  0.270924 -0.085845 -0.222353   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0    0.305346  0.388603 -0.047301 -0.254862 -0.508278  0.560509  0.267213  \n",
       "1    0.100870  0.659238  0.033278 -0.133802 -0.091845  0.030011  0.132194  \n",
       "2    0.513825  0.447627  0.554507 -1.393644  0.128506  0.397678  0.663884  \n",
       "3   -0.031328  0.301128  0.237756 -0.159807  0.043635  0.126591  0.187225  \n",
       "4    0.430093 -0.398542  0.727709  0.266551  0.037141 -0.020469  0.553227  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "911 -0.137296  0.076680 -0.073037 -0.184570 -0.021565 -0.013196 -0.004472  \n",
       "912 -0.003433  0.787824 -0.192636  0.148812 -0.158076  0.375436 -0.184489  \n",
       "913  0.335971 -0.072328 -0.003498 -0.392746  0.319814  0.112957  0.189739  \n",
       "914 -0.201865  0.551706  0.232849 -0.145245 -0.448189  0.033886  0.354401  \n",
       "915 -0.112073  0.085204  0.712144  0.451307 -0.275012  0.547725  0.137799  \n",
       "\n",
       "[916 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020067</td>\n",
       "      <td>-0.101350</td>\n",
       "      <td>0.129504</td>\n",
       "      <td>-0.022527</td>\n",
       "      <td>-0.075322</td>\n",
       "      <td>-0.173469</td>\n",
       "      <td>0.070264</td>\n",
       "      <td>0.146942</td>\n",
       "      <td>-0.195092</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304480</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>-0.159397</td>\n",
       "      <td>-0.109282</td>\n",
       "      <td>0.413820</td>\n",
       "      <td>0.082638</td>\n",
       "      <td>-0.131411</td>\n",
       "      <td>-0.266550</td>\n",
       "      <td>0.328758</td>\n",
       "      <td>-0.017286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019581</td>\n",
       "      <td>-0.124779</td>\n",
       "      <td>0.150678</td>\n",
       "      <td>-0.056071</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>-0.214284</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>0.243574</td>\n",
       "      <td>-0.152522</td>\n",
       "      <td>0.108520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363346</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>-0.218834</td>\n",
       "      <td>-0.128898</td>\n",
       "      <td>0.317247</td>\n",
       "      <td>0.176807</td>\n",
       "      <td>-0.102440</td>\n",
       "      <td>-0.302083</td>\n",
       "      <td>0.385284</td>\n",
       "      <td>0.061138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041266</td>\n",
       "      <td>-0.178154</td>\n",
       "      <td>0.151083</td>\n",
       "      <td>-0.111217</td>\n",
       "      <td>-0.192240</td>\n",
       "      <td>-0.187795</td>\n",
       "      <td>0.132793</td>\n",
       "      <td>0.401743</td>\n",
       "      <td>-0.221176</td>\n",
       "      <td>-0.168583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308242</td>\n",
       "      <td>0.081699</td>\n",
       "      <td>-0.209440</td>\n",
       "      <td>-0.096391</td>\n",
       "      <td>0.382550</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>-0.010923</td>\n",
       "      <td>-0.304304</td>\n",
       "      <td>0.288351</td>\n",
       "      <td>0.179666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.104522</td>\n",
       "      <td>-0.065063</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.016936</td>\n",
       "      <td>0.049264</td>\n",
       "      <td>-0.141561</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.026229</td>\n",
       "      <td>0.019564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>-0.157384</td>\n",
       "      <td>-0.054034</td>\n",
       "      <td>0.152662</td>\n",
       "      <td>0.148895</td>\n",
       "      <td>-0.158399</td>\n",
       "      <td>-0.210276</td>\n",
       "      <td>0.165432</td>\n",
       "      <td>-0.033105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005300</td>\n",
       "      <td>-0.118808</td>\n",
       "      <td>0.144384</td>\n",
       "      <td>-0.112644</td>\n",
       "      <td>-0.199265</td>\n",
       "      <td>-0.300900</td>\n",
       "      <td>0.047582</td>\n",
       "      <td>0.224066</td>\n",
       "      <td>-0.051554</td>\n",
       "      <td>-0.012898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426282</td>\n",
       "      <td>0.121688</td>\n",
       "      <td>-0.168438</td>\n",
       "      <td>-0.242667</td>\n",
       "      <td>0.446508</td>\n",
       "      <td>0.025499</td>\n",
       "      <td>-0.238271</td>\n",
       "      <td>-0.333413</td>\n",
       "      <td>0.180452</td>\n",
       "      <td>-0.104449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-0.024806</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.099179</td>\n",
       "      <td>0.033357</td>\n",
       "      <td>-0.283247</td>\n",
       "      <td>-0.056122</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>0.238078</td>\n",
       "      <td>-0.549399</td>\n",
       "      <td>-0.072962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330090</td>\n",
       "      <td>0.057219</td>\n",
       "      <td>0.060683</td>\n",
       "      <td>-0.191506</td>\n",
       "      <td>0.250466</td>\n",
       "      <td>-0.094968</td>\n",
       "      <td>0.148278</td>\n",
       "      <td>-0.000366</td>\n",
       "      <td>0.016381</td>\n",
       "      <td>0.227941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.018643</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>0.118389</td>\n",
       "      <td>0.067912</td>\n",
       "      <td>-0.248830</td>\n",
       "      <td>-0.059910</td>\n",
       "      <td>-0.084202</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>-0.261869</td>\n",
       "      <td>-0.065116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232210</td>\n",
       "      <td>0.088578</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>-0.162620</td>\n",
       "      <td>0.173859</td>\n",
       "      <td>-0.236202</td>\n",
       "      <td>0.033965</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>0.047229</td>\n",
       "      <td>0.075214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.149242</td>\n",
       "      <td>-0.005128</td>\n",
       "      <td>0.240545</td>\n",
       "      <td>-0.172695</td>\n",
       "      <td>-0.375124</td>\n",
       "      <td>-0.220269</td>\n",
       "      <td>-0.055242</td>\n",
       "      <td>0.675123</td>\n",
       "      <td>-1.063848</td>\n",
       "      <td>-0.159650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564568</td>\n",
       "      <td>0.180823</td>\n",
       "      <td>-0.028139</td>\n",
       "      <td>-0.304117</td>\n",
       "      <td>0.711318</td>\n",
       "      <td>0.225356</td>\n",
       "      <td>0.346216</td>\n",
       "      <td>-0.234466</td>\n",
       "      <td>0.078718</td>\n",
       "      <td>0.400004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>-0.037833</td>\n",
       "      <td>-0.050435</td>\n",
       "      <td>0.076485</td>\n",
       "      <td>-0.083301</td>\n",
       "      <td>-0.426154</td>\n",
       "      <td>-0.038149</td>\n",
       "      <td>0.139153</td>\n",
       "      <td>0.420794</td>\n",
       "      <td>-0.857922</td>\n",
       "      <td>-0.251914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558851</td>\n",
       "      <td>0.111743</td>\n",
       "      <td>-0.040318</td>\n",
       "      <td>-0.252073</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.084354</td>\n",
       "      <td>-0.097071</td>\n",
       "      <td>0.053662</td>\n",
       "      <td>0.126651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>-0.145842</td>\n",
       "      <td>-0.073906</td>\n",
       "      <td>0.035006</td>\n",
       "      <td>-0.045979</td>\n",
       "      <td>-0.128046</td>\n",
       "      <td>-0.375469</td>\n",
       "      <td>0.252509</td>\n",
       "      <td>0.455487</td>\n",
       "      <td>-0.236849</td>\n",
       "      <td>-0.124329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397865</td>\n",
       "      <td>0.055199</td>\n",
       "      <td>-0.170804</td>\n",
       "      <td>-0.104571</td>\n",
       "      <td>0.474424</td>\n",
       "      <td>0.186225</td>\n",
       "      <td>-0.077310</td>\n",
       "      <td>-0.446512</td>\n",
       "      <td>0.375153</td>\n",
       "      <td>0.248749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.020067 -0.101350  0.129504 -0.022527 -0.075322 -0.173469  0.070264   \n",
       "1    0.019581 -0.124779  0.150678 -0.056071  0.018639 -0.214284  0.018052   \n",
       "2   -0.041266 -0.178154  0.151083 -0.111217 -0.192240 -0.187795  0.132793   \n",
       "3   -0.104522 -0.065063  0.022551  0.016936  0.049264 -0.141561  0.110174   \n",
       "4    0.005300 -0.118808  0.144384 -0.112644 -0.199265 -0.300900  0.047582   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "388 -0.024806  0.006797  0.099179  0.033357 -0.283247 -0.056122  0.016880   \n",
       "389  0.018643  0.008233  0.118389  0.067912 -0.248830 -0.059910 -0.084202   \n",
       "390  0.149242 -0.005128  0.240545 -0.172695 -0.375124 -0.220269 -0.055242   \n",
       "391 -0.037833 -0.050435  0.076485 -0.083301 -0.426154 -0.038149  0.139153   \n",
       "392 -0.145842 -0.073906  0.035006 -0.045979 -0.128046 -0.375469  0.252509   \n",
       "\n",
       "           7         8         9   ...        40        41        42  \\\n",
       "0    0.146942 -0.195092  0.004402  ...  0.304480  0.002518 -0.159397   \n",
       "1    0.243574 -0.152522  0.108520  ...  0.363346  0.001958 -0.218834   \n",
       "2    0.401743 -0.221176 -0.168583  ...  0.308242  0.081699 -0.209440   \n",
       "3    0.080300  0.026229  0.019564  ...  0.113504 -0.002047 -0.157384   \n",
       "4    0.224066 -0.051554 -0.012898  ...  0.426282  0.121688 -0.168438   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "388  0.238078 -0.549399 -0.072962  ...  0.330090  0.057219  0.060683   \n",
       "389  0.114286 -0.261869 -0.065116  ...  0.232210  0.088578  0.032357   \n",
       "390  0.675123 -1.063848 -0.159650  ...  0.564568  0.180823 -0.028139   \n",
       "391  0.420794 -0.857922 -0.251914  ...  0.558851  0.111743 -0.040318   \n",
       "392  0.455487 -0.236849 -0.124329  ...  0.397865  0.055199 -0.170804   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0   -0.109282  0.413820  0.082638 -0.131411 -0.266550  0.328758 -0.017286  \n",
       "1   -0.128898  0.317247  0.176807 -0.102440 -0.302083  0.385284  0.061138  \n",
       "2   -0.096391  0.382550  0.123500 -0.010923 -0.304304  0.288351  0.179666  \n",
       "3   -0.054034  0.152662  0.148895 -0.158399 -0.210276  0.165432 -0.033105  \n",
       "4   -0.242667  0.446508  0.025499 -0.238271 -0.333413  0.180452 -0.104449  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "388 -0.191506  0.250466 -0.094968  0.148278 -0.000366  0.016381  0.227941  \n",
       "389 -0.162620  0.173859 -0.236202  0.033965  0.058030  0.047229  0.075214  \n",
       "390 -0.304117  0.711318  0.225356  0.346216 -0.234466  0.078718  0.400004  \n",
       "391 -0.252073  0.435514  0.016302  0.084354 -0.097071  0.053662  0.126651  \n",
       "392 -0.104571  0.474424  0.186225 -0.077310 -0.446512  0.375153  0.248749  \n",
       "\n",
       "[393 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def do_doc2vec(data):\n",
    "    tagged_documents = []\n",
    "    sentences = [text.split() for text in data['CleanText']]\n",
    "\n",
    "    for i, doc in enumerate(sentences):\n",
    "        tagged_documents.append(gensim.models.doc2vec.TaggedDocument(doc, [i]))\n",
    "\n",
    "    d2v = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    d2v.build_vocab(tagged_documents)\n",
    "\n",
    "    d2v.train(tagged_documents, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "\n",
    "    doc2vec_alldocs = []\n",
    "    for i in range(len(tagged_documents)):\n",
    "        doc2vec_alldocs.append(d2v.infer_vector(tagged_documents[i].words))\n",
    "\n",
    "    doc2vec_alldocs_matrix = pd.DataFrame(doc2vec_alldocs)\n",
    "\n",
    "    display(doc2vec_alldocs_matrix)\n",
    "    \n",
    "    return(doc2vec_alldocs_matrix)\n",
    "\n",
    "# call function\n",
    "train_doc2vec_matrix = do_doc2vec(data_train) \n",
    "test_doc2vec_matrix = do_doc2vec(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_nn(train_X, train_y, test_X, test_y):\n",
    "\n",
    "    # first we define the network, units is the number of neurons, in the first layer we need to tell the model the input shape\n",
    "    neural_network = tf.keras.Sequential([\n",
    "                        # hidden layer\n",
    "                        tf.keras.layers.Dense(units = 100, input_shape = [train_X.shape[1]], activation = 'selu'),\n",
    "                        # output layer - as we want probability predictions, it is important to use the sigmoid activation\n",
    "                        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    # by compilation we define the loss function which is supposed to be minimized and which optimization method should be used\n",
    "    neural_network.compile(loss = 'binary_crossentropy', optimizer = 'sgd',  metrics = ['accuracy', tf.keras.metrics.Recall()])\n",
    "\n",
    "    # a summary for all parameters which need to be estimated\n",
    "    neural_network.summary()\n",
    "\n",
    "\n",
    "    train_X = np.asarray(train_X)\n",
    "    test_X = np.asarray(test_X)\n",
    "\n",
    "    # we fit the model, epochs is the number of steps which are repeated using gradient descent\n",
    "    history = neural_network.fit(train_X, train_y, epochs = 100, validation_data = (test_X, test_y))\n",
    "\n",
    "\n",
    "    plt.plot(history.history['loss'], label = 'training'), plt.plot(history.history['val_loss'], label = 'test'), plt.legend(loc='lower left'), plt.show()\n",
    "\n",
    "\n",
    "    corrcoef_train = np.corrcoef(neural_network.predict(train_X).flatten(), train_y)[0, 1]    \n",
    "    print(\"Correlation coefficient Traindata:\", corrcoef_train)\n",
    "    \n",
    "    corrcoef_test = np.corrcoef(neural_network.predict(test_X).flatten(), test_y)[0, 1]\n",
    "    print(\"Correlation coefficient Testdata:\", corrcoef_test)\n",
    "\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 100)               449600    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 449,701\n",
      "Trainable params: 449,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 12ms/step - loss: -0.0605 - accuracy: 0.6681 - recall_11: 0.9651 - val_loss: -0.5462 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -0.9670 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1.1919 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -1.7055 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1.8899 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -2.5934 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -2.7578 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -3.7243 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -3.8957 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -5.2116 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -5.3940 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -7.1614 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -7.3157 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -9.6561 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -9.8353 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -12.9128 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -13.0379 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -17.0590 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -17.2032 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -22.4542 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -22.5393 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -29.3880 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -29.5310 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -38.5144 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -38.7970 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -50.6740 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -51.1268 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -66.9064 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -67.5496 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -88.6636 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -90.1417 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -118.6896 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -121.0005 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -159.8059 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -164.6006 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -218.1824 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -225.1229 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: -299.3395 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -310.5598 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -414.1155 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -430.8120 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -575.7915 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -600.2000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -803.8690 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -841.1897 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1128.6576 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1185.6841 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1593.1959 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1673.8370 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2250.4634 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -2367.6804 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -3187.7412 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -3353.9021 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -4517.8975 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -4756.7710 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -6411.1465 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -6781.1787 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -9141.8047 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -9665.0469 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -13032.5371 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -13736.6299 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -18521.0996 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -19613.7363 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -26456.7969 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -27963.3340 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -37736.0430 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -39881.1133 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -53786.0781 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -56845.5430 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -76690.2031 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -81079.9453 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -109411.3594 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -115559.7578 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -155851.0625 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -164685.2656 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -222215.6406 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -235197.7031 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -317250.2812 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -335105.9062 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -452111.9375 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -478412.8125 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -645292.3125 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -683880.8125 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -922459.5000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -974605.1875 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -1314836.5000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1388437.6250 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -1873656.5000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1984460.3750 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2678308.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -2836838.2500 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -3828789.7500 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -4044948.5000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -5457053.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -5771731.5000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -7790792.5000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -8232350.5000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -11109095.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -11737319.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -15837658.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -16717096.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -22559346.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -23844830.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -32176544.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -34010004.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -45894744.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -48505520.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -65439856.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -69201432.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -93371536.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -98913928.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -133474240.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -141191408.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -190585328.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -201250224.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -271565632.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -286591168.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -386689184.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -408785120.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -551295744.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -581438720.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -784515648.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -828178240.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1117525504.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1180051200.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1591559168.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1677680384.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2263472896.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -2392221184.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -3227822336.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -3410663424.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -4601118208.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -4852830720.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -6548455936.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -6915868160.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -9331543040.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -9851096064.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: -13293849600.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -14058486784.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -18969065472.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -19980089344.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -26951614464.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -28456044544.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -38397640704.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -40472522752.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -54611718144.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -57728499712.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -77889380352.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -82196193280.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -110962008064.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -117652848640.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -158782488576.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -167676313600.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -226142797824.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -239832416256.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -323587047424.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -342706520064.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -462385283072.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -488888205312.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -659535429632.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -696927453184.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -940252135424.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -991609618432.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1338028589056.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -1413763039232.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1907235225600.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -2021806964736.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2728213348352.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -2895815114752.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -3906825617408.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -4135471546368.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -5579605016576.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -5901447069696.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -7962619805696.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -8432677027840.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -11378699010048.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -12023348854784.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -16225551777792.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -17123920314368.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -23107714678784.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -24405612691456.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -32924992798720.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -34716241625088.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -46858277748736.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -49630419091456.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -66994451775488.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -70623455870976.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -95306347708416.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -100648079065088.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -135849421307904.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -143729402839040.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -193937201430528.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -205164967362560.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -276957577084928.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -293195237818368.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -395568132128768.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -418037186428928.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -564312666013696.0000 - accuracy: 0.6889 - recall_11: 1.0000 - val_loss: -595307431723008.0000 - val_accuracy: 0.7328 - val_recall_11: 1.0000\n",
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x28594232a88>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_bow_matrix, data_train['LabelNumber'], test_bow_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 100)               449600    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 449,701\n",
      "Trainable params: 449,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 11ms/step - loss: 0.3468 - accuracy: 0.6627 - recall_7: 0.9629 - val_loss: 0.0629 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -0.1955 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -0.3304 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -0.5752 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -0.6341 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -0.8998 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -0.9084 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1.2162 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1.1908 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1.5593 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1.5076 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1.9551 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1.8716 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2.4150 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -2.3060 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -2.9651 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -2.8205 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3.6165 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -3.4291 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -4.3844 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -4.1481 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5.2888 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -4.9816 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -6.3344 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -5.9600 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -7.5582 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -7.0839 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -8.9598 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -8.3724 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -10.5651 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -9.8559 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -12.4077 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -11.5400 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -14.4959 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -13.4450 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -16.8563 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -15.5942 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -19.5188 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -18.0418 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -22.5489 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -20.8301 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -26.0003 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -23.9752 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -29.8980 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -27.5647 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -34.3461 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -31.6274 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -39.3939 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -36.2761 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -45.1722 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -41.5682 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -51.7597 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -47.6514 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -59.3530 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -54.6576 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -68.1124 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -62.7242 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -78.2258 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -72.1079 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -89.9996 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -83.0973 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -103.8273 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -95.8573 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -119.9131 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -110.9121 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -138.9217 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -128.5130 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -161.1863 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -149.4605 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -187.7389 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -174.1580 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -219.0749 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -203.5927 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -256.4786 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -238.8565 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -301.3459 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -280.5523 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -354.4372 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -330.7738 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -418.4840 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -391.2825 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -495.6181 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -463.6935 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -588.0658 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -551.7778 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -700.5261 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -656.6059 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -834.4568 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -784.9144 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -998.4494 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -939.3298 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1195.7941 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1126.3827 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1435.1630 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1351.1956 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1722.7883 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1621.7583 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2068.8450 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1949.5024 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2487.9141 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -2346.7429 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2996.7344 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -2826.4456 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -3610.4231 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -3409.5569 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -4357.1348 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -4114.2476 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -5258.8350 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -4967.4980 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -6351.0283 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -5993.7275 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -7666.1602 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -7249.6235 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: -9273.3604 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -8761.8057 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -11209.9014 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -10597.2051 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -13560.7773 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -12815.4531 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -16397.6152 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -15495.5322 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -19829.5664 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -18725.2383 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -23969.0762 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -22668.1836 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -29019.1777 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -27435.2715 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -35124.4492 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -33209.1055 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -42514.3711 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -40154.2383 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -51411.8984 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -48653.9141 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -62291.9570 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -58886.6211 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -75402.1016 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -71333.6641 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -91347.8359 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -86487.3359 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -110762.6641 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -104781.4453 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -134183.9688 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -127150.1484 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -162803.0469 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -153872.3906 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -197067.3750 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -186439.5156 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -238741.7969 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -225725.7969 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -289079.1562 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -274059.3750 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -350986.4688 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -332081.3125 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -425276.0625 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -402556.5000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -515518.9062 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -487795.9062 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 8ms/step - loss: -624754.5000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -591575.5625 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -757593.2500 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -716855.9375 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -918044.6250 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -868676.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -1112318.1250 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1052225.7500 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1347524.7500 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1273564.3750 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -1630902.2500 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1542658.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -1975820.3750 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -1868670.2500 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: -2393230.5000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -2265382.2500 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2901258.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -2747439.5000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -3518313.2500 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -3329370.5000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -4264093.5000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -4036265.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -5169526.5000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -4893170.5000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -6266514.5000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -5929741.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -7594643.5000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -7191674.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -9210234.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -8704629.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -11147139.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -10557041.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -13518915.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -12788275.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -16377177.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -15484894.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -19832128.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -18780208.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -24053818.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -22804292.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -29203358.0000 - accuracy: 0.6889 - recall_7: 1.0000 - val_loss: -27645984.0000 - val_accuracy: 0.7328 - val_recall_7: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x28598858388>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_tfidf_matrix, data_train['LabelNumber'], test_tfidf_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 100)               1100      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,201\n",
      "Trainable params: 1,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 10ms/step - loss: 0.3825 - accuracy: 0.6659 - recall_8: 0.9651 - val_loss: 0.1151 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -0.1231 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -0.2635 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -0.4908 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -0.5569 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -0.8006 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -0.8185 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -1.0976 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1.0815 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1.4127 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1.3709 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1.7709 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1.7014 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2.1866 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -2.0902 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2.6787 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -2.5523 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -3.2631 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -3.0974 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3.9518 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -3.7400 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4.7607 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -4.4952 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5.7097 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -5.3719 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -6.8097 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -6.3906 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -8.0836 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -7.5658 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -9.5494 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -8.9123 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -11.2255 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -10.4619 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -13.1510 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -12.2134 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -15.3249 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -14.2122 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -17.8040 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -16.4800 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -20.6165 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -19.0446 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -23.7930 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -21.9820 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -27.4340 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -25.2938 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -31.5396 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -29.0295 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -36.1765 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -33.2953 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -41.4819 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -38.1603 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -47.5394 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -43.7628 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -54.5215 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -50.1286 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -62.4769 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -57.5088 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -71.7090 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -66.0863 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -82.4745 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -76.0137 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -94.9488 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -87.5668 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -109.4925 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -101.0585 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -126.5116 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -117.0441 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -146.7001 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -135.8627 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -170.5100 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -157.9790 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -198.5489 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -184.2545 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -231.8799 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -215.7726 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -271.9364 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -253.2158 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -319.5280 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -297.8528 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -376.3351 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -350.5381 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -443.4710 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -414.1138 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -524.4762 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -490.6895 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -622.1711 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -582.0638 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -738.7703 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -692.0693 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -879.2900 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -824.9738 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1048.9832 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -984.3167 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1252.3514 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1175.3066 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1496.5807 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1405.7751 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1791.0795 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1684.0841 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2146.6494 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -2018.1082 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2574.2981 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -2427.1560 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3097.0366 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -2921.5723 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -3729.1526 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -3513.9336 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4487.0493 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -4229.1421 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5401.4531 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -5092.7412 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -6506.0850 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -6140.4263 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7847.0332 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -7395.3989 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -9452.0703 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -8913.5176 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -11394.8262 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -10750.8857 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -13744.1562 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -12950.2139 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -16557.6191 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -15639.5957 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -19998.8867 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -18884.0312 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -24150.9375 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -22805.5703 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -29167.7305 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -27545.5117 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -35232.0820 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -33248.8516 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -42530.7266 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -40153.2344 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -51366.3281 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -48571.8008 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -62137.4102 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -58691.2422 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -75083.0781 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -70895.7188 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -90704.4375 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -85710.5000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -109641.6875 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -103665.0312 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -132636.8750 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -125289.9141 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -160290.1094 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -151429.6562 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -193719.7344 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -182740.1719 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -233810.9531 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -220793.0625 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -282496.5625 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -266670.6875 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -341175.0938 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -322589.2812 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -412759.8750 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -390090.7500 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -499082.6250 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -471349.2188 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -603118.5625 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -569992.5625 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -729225.1250 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -688737.6250 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -881311.9375 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -831913.8750 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1064432.1250 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1005641.3750 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1286565.6250 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1213734.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1552932.6250 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1468931.3750 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1879488.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -1777098.7500 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2273772.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -2149926.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2750685.2500 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -2595871.5000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -3321685.5000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -3138055.7500 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4015537.5000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -3792010.7500 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4852052.5000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -4582226.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5862061.5000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -5532682.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -7078802.5000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -6680361.5000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -8547805.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -8072507.5000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -10328904.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -9758586.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -12485816.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -11806041.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -15106398.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -14288749.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -18282290.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -17273174.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -22101276.0000 - accuracy: 0.6889 - recall_8: 1.0000 - val_loss: -20872826.0000 - val_accuracy: 0.7328 - val_recall_8: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2859be5ae48>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_lsa_matrix, data_train['LabelNumber'], test_lsa_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 100)               5100      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 1s 10ms/step - loss: 0.1030 - accuracy: 0.6070 - recall_9: 0.8821 - val_loss: -0.3309 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -0.7934 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -0.8915 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1.3873 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1.3955 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2.0234 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1.9751 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2.7951 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -2.6934 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -3.7620 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -3.6037 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -4.9856 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -4.7367 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -6.5037 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -6.1449 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -8.3815 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -7.8799 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -10.6845 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -10.0069 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -13.5007 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -12.5632 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -16.8842 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -15.6776 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -21.0012 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -19.4734 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -26.0309 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -24.0276 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -32.0830 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -29.5936 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -39.5032 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -36.3466 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -48.5556 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -44.7421 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -59.8618 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -55.1813 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -73.9959 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -68.2493 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -91.7751 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -84.8369 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -114.4723 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -105.8139 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -143.2779 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -132.7138 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -180.4200 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -167.2693 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -228.2985 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -212.1695 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -290.6877 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -271.1090 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -372.7346 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -347.7715 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -479.7068 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -448.8801 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -621.0844 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -583.2103 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -809.1261 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -758.7047 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1055.2444 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -991.7773 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1381.7947 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1302.2317 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1817.3746 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1709.9108 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: -2390.0854 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -2254.7283 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -3155.6672 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -2972.0894 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -4163.0757 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -3922.4741 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -5497.4517 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -5182.3306 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -7268.9927 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -6852.6699 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -9618.4258 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -9064.7637 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -12727.8643 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -12025.8711 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -16889.0488 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -15943.1299 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -22394.1133 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -21123.5898 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -29678.1094 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -28000.0352 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -39339.3438 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -37168.1289 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -52231.8984 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -49311.6094 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -69313.6172 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -65490.1484 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -92051.7812 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -86886.6328 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -122122.3438 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -115302.0078 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -162091.6094 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -153276.8906 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -215448.1562 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -203172.6250 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -285617.6562 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -269497.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -378776.0625 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -357472.8125 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -502496.5938 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -474604.1875 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -667000.5000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -630229.5625 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -885824.5625 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -837712.3125 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1177541.6250 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1113032.2500 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -1564639.1250 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1479123.8750 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2079358.8750 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1965320.7500 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -2762592.2500 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -2605952.7500 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -3662641.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -3456632.7500 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -4858979.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -4594798.5000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -6457684.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -6088464.5000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -8558055.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -8089980.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -11370853.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -10755409.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -15118327.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -14267189.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -20058034.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -18923472.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: -26600160.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -25135644.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -35326268.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -33360912.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -46893988.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -44248228.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -62198092.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -58801348.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -82633624.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -78008128.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -109655392.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -103574064.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -145545968.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -137431408.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -193178032.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -182302928.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -256217728.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -241393600.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -339301952.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -321273216.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -451564800.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -426202752.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -598982080.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -564894400.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -794121472.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -751955712.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1056590464.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -998469696.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1403410560.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1325594112.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -1863027072.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -1762534528.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -2477683712.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -2337957888.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -3286575616.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -3109780480.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -4370152960.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -4135771904.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -5814471680.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -5493376000.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -7720721408.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -7281518592.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -10235324416.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -9680775168.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -13608986624.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -12864989184.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -18084534272.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -17088295936.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -24023785472.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -22691741696.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -31907891200.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -30131630080.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -42350710784.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -39969386496.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -56184270848.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -53133438976.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -74703708160.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -70603751424.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -99256819712.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -93650337792.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -131626868736.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -124416802816.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -174901116928.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -165526781952.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -232742813696.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -219723743232.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: -308869726208.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -291818143744.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: -410272923648.0000 - accuracy: 0.6889 - recall_9: 1.0000 - val_loss: -387429728256.0000 - val_accuracy: 0.7328 - val_recall_9: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient Traindata: nan\n",
      "Correlation coefficient Testdata: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2859efef688>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nn(train_doc2vec_matrix, data_train['LabelNumber'], test_doc2vec_matrix, data_test['LabelNumber'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d39680e86271bf209db9aab97c2f2e95b8272895522c86f00c9979eddd8c4165"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
