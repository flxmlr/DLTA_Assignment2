{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team information\n",
    "\n",
    "|Team-number :| 1|\n",
    "|:----:|:----:|\n",
    "\n",
    "\n",
    "|Name|    E-Mail        |matriculation-nr.|\n",
    "|:----:|:----:|:----:|\n",
    "|Tamara Scherer| schere21@ads.uni-passau.de|104218|\n",
    "|Felix Müller| muell518@ads.uni-passau.de|104227|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After labeling the data we now import it from the label-studio plattform. Therefore we used the provided workaround and downloaded the pickle file. In the next step we extract the necessary texts with the corresponding labels out of the whole downloaded data. A dataframe is created with the columns \"Text\" and \"Label\" in which the extracted data is saved. To be able to work more efficient with the data, we assign an unique ID to each label in the dataframe with the function \"categorise()\". This is especially helpful when we create the neural network in the end, as we can only use integer values as input. As stated in the task formulation we have to focus on one ID Stage. So, we implemented a filter to get only the entries with the desired ID Stage which can be specified before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary packages:\n",
    "> pickle: This package is used for serializing and de-serializing python object structures which means that any kind of python object is converted into byte streams or vice versa.\n",
    "\n",
    "> pandas: This package is usually used for data analysis and manipulation. It allows us to create and work with a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from label_studio_sdk import Client\n",
    "#from label_studio_sdk import project\n",
    "#from label_studio_sdk import project\n",
    "#import pandas as pd\n",
    "#LABEL_STUDIO_URL = 'http://132.231.59.226:8080' #this address needs to be the same as the address the label-studio is hosted on.\n",
    "#API_KEY = '1655a8922f821195356a17a3224c0532b091c61d' #please add your personal API_Key here to get your API_Key follow the Pictures below\n",
    "\n",
    "#ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "#ls.check_connection()\n",
    "#pro = project.Project.get_from_id(ls,\"1\")\n",
    "#tasks = project.Project.get_labeled_tasks(pro)\n",
    "#tasks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download labelled tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary package\n",
    "import pickle\n",
    "\n",
    "# import labeled data from downloaded pickle file\n",
    "with open(\"Files/tasks3\", 'rb') as f:\n",
    "    tasks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract text with label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[QID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my...</td>\n",
       "      <td>[Question_1_Company_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_2_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good afternoon and thanks a lot for taking my ...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, I think that as it relates to the one, I t...</td>\n",
       "      <td>[AID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>One of the areas obviously which has been a st...</td>\n",
       "      <td>[Question_3_neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[AID_1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_1_specific]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11292</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_2_positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>Well, let's start with the first part of your ...</td>\n",
       "      <td>[Answer_3_no_blame]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11294 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      Good afternoon and thanks a lot for taking my ...   \n",
       "1       Good afternoon and thanks a lot for taking my...   \n",
       "2      Good afternoon and thanks a lot for taking my ...   \n",
       "3      Good afternoon and thanks a lot for taking my ...   \n",
       "4      No, I think that as it relates to the one, I t...   \n",
       "...                                                  ...   \n",
       "11289  One of the areas obviously which has been a st...   \n",
       "11290  Well, let's start with the first part of your ...   \n",
       "11291  Well, let's start with the first part of your ...   \n",
       "11292  Well, let's start with the first part of your ...   \n",
       "11293  Well, let's start with the first part of your ...   \n",
       "\n",
       "                               Label  \n",
       "0                            [QID_1]  \n",
       "1      [Question_1_Company_specific]  \n",
       "2              [Question_2_specific]  \n",
       "3               [Question_3_neutral]  \n",
       "4                            [AID_1]  \n",
       "...                              ...  \n",
       "11289           [Question_3_neutral]  \n",
       "11290                        [AID_1]  \n",
       "11291            [Answer_1_specific]  \n",
       "11292            [Answer_2_positive]  \n",
       "11293            [Answer_3_no_blame]  \n",
       "\n",
       "[11294 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# filter the necessary texts with the corresponding label\n",
    "df = pd.DataFrame(columns = ['Text', 'Label',])\n",
    "\n",
    "for i in range(len(tasks)): \n",
    "    for j in range(len(tasks[i]['annotations'][0]['result'])): \n",
    "        df = df.append({\n",
    "            'Text' : tasks[i]['annotations'][0]['result'][j]['value']['text'],\n",
    "            'Label' : tasks[i]['annotations'][0]['result'][j]['value']['labels']\n",
    "                        }, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Label as unique ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a unique number to each label (can also be used later for the neural network)\n",
    "def categorise(row):  \n",
    "    if row['Label'] == ['Question_1_Company_specific']:\n",
    "        return 1\n",
    "    elif row['Label'] == ['Question_1_Market_related']:\n",
    "        return 2\n",
    "    elif row['Label'] == ['Question_2_specific']:\n",
    "        return 3\n",
    "    elif row['Label'] == ['Question_2_open']:\n",
    "        return 4\n",
    "    elif row['Label'] == ['Question_3_attack']:\n",
    "        return 5\n",
    "    elif row['Label'] == ['Question_3_support']:\n",
    "        return 6\n",
    "    elif row['Label'] == ['Question_3_neutral']:\n",
    "        return 7\n",
    "    elif row['Label'] == ['Answer_1_specific']:\n",
    "        return 8\n",
    "    elif row['Label'] == ['Answer_1_avoid_excuse']:\n",
    "        return 9\n",
    "    elif row['Label'] == ['Answer_2_positive']:\n",
    "        return 10\n",
    "    elif row['Label'] == ['Answer_2_negative']:\n",
    "        return 11\n",
    "    elif row['Label'] == ['Answer_3_blame']:\n",
    "        return 12\n",
    "    elif row['Label'] == ['Answer_3_no_blame']:\n",
    "        return 13\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "# call function and write results in a new column of the dataframe\n",
    "df['LabelNumber'] = df.apply(\n",
    "    lambda row: categorise(row), axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter one ID stage (e.g. Question_1_XX)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set desired labels\n",
    "idStage = [1, 2]\n",
    "\n",
    "# filter the entries in the dataframe with the specified ID stage\n",
    "df = df[df['LabelNumber'].isin(idStage)]\n",
    "\n",
    "# set new index\n",
    "df = df.reset_index()\n",
    "# delete old indices\n",
    "df.pop('index')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement different steps to preprocess the downloaded data. Therefore we have to make some imports first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Packages:\n",
    "> re: This package can be used to work with Regular Expressions. Here it is used remove the numbers in the text.\n",
    "\n",
    "> string: This package is necessary for common string operations. Here it is used to remove the punctuation in the text.\n",
    "\n",
    "> nltk: Natural Language Toolkit is a package which is used for Natural Language Processing. Here it is used for stemming, lemmatization, tokenization and the removal of stopwords.\n",
    "\n",
    "Further Imports:\n",
    "> For Stemmer: The module \"nltk.stem.snowball\" provides a port of the Snowball stemmers. The word stemmer is based on the original Porter stemming algorithm for suffix stripping. In this case it is used for the English language.\n",
    "\n",
    "> For Stopwords: Here the English stopwords are imported from the \"nltk.corpus\" package. It includes words which does not add much meaning to a sentence and are not important for further text analysis.\n",
    "\n",
    "> For Tokenization: The word_tokenize module is imported from the nltk library for tokenization.\n",
    "\n",
    "> For Lemmatizer: The \"WordNetLemmatizer\" module as well as \"pos_tag\" is imported here. Therefore the system requires some further downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we start with the preprocessing. Therefore, we implemented a function called \"do_preprocessing\" with text that has to be preprocessed as input. Furthermore, it is possible to set two boolean values to specify if the removal of stopwords and the stemming should be executed. \n",
    "Within the function we implemented the following steps:\n",
    "1. Removal of Numbers: delete all numbers, e.g. 1, 2 of the text\n",
    "2. Lowercasing: convert the text into the same casing, so that the different versions of the word can be treated as one\n",
    "3. Stemming: process to reduce the words to their root forms, but the stem itself may not be a valid word in the language (here we use the PorterStemmer)\n",
    "4. Stopwords Removal: stopwords are trivial words which appear very frequently in the text without adding much valuable information and therefore not necessary for the further analysis\n",
    "5. Removing Punctuations: the punctuation does not add any value, so it is not necessary or helpful for the analysis and can therefore be deleted as well\n",
    "6. Removing Extra Whitespaces: additional whitespaces do not add any value to the data and just increase the text size, so they can be deleted as well\n",
    "<p>\n",
    "For Lemmatization we implemented another function \"do_lemmatization\" as it is necessary to do a tokenization first.\n",
    "\n",
    "1. Tokenization: process of splitting the text into pieces called tokens (here the tokens are the single words)\n",
    "2. Lemmatization: This can be done instead of stemming. Lemmatization is also a process to convert the word to its base form. Before the text is lemmatized a POS tagging is necessary to tag the tokens as noun, verb, adverb or adjective. As it causes noticeable improvement we implement the lemmatization in addition to the already implemented preprocessing.\n",
    "<p>\n",
    "Finally we use the resulting tokens to create a dictionary which is necessary for the following steps as for building the neural network the column size of the training and test dataset have to be identical. This can be ensured by using this dictionary. To be able to execute the next implementations it is necessary to convert the tokens back to normal strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for preprocessing steps\n",
    "\n",
    "# import necessary packages\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# import a stemmer for english words\n",
    "snowStem = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "# import for tokenization\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that includes all preprocessing steps\n",
    "\n",
    "def do_preprocessing(text_to_clean, remove_stopwords = True, stemming = True):\n",
    "    # remove numbers \n",
    "    text_to_clean = re.sub(r'\\d+', '', text_to_clean)\n",
    "    # transform text to lower case\n",
    "    text_to_clean = text_to_clean.lower()\n",
    "\n",
    "    if stemming:\n",
    "      # stemming\n",
    "      text_to_clean = snowStem.stem(text_to_clean)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        text_to_clean = ' '.join([w for w in text_to_clean.split() if not(w in en_stopwords)])\n",
    "\n",
    "    # remove punctuation\n",
    "    text_to_clean = text_to_clean.translate(str.maketrans('','', string.punctuation))\n",
    "    # remove leading and ending white spaces\n",
    "    text_to_clean = text_to_clean.strip()\n",
    "    \n",
    "    return text_to_clean\n",
    "\n",
    "\n",
    "# call function and write results in new column of the dataframe\n",
    "df.loc[:, 'CleanText'] = df['Text'].apply(\n",
    "    lambda x: do_preprocessing(x, True, True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lemmatization\n",
    "def do_lemmatization(text):\n",
    "    \n",
    "  text = word_tokenize(text)\n",
    "  \n",
    "  result=[]\n",
    "  wordnet = WordNetLemmatizer()\n",
    "  for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.lemmatize(token,pos))\n",
    "\n",
    "  return result\n",
    "\n",
    "# call function\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: do_lemmatization(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# generate the gensim dictionary\n",
    "dct = corpora.dictionary.Dictionary(df['CleanText']).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert tokens to strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokens back to strings for further processing\n",
    "whitespace = \" \"\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: whitespace.join(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split of training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network which we build later on, it is necessary to split the available data in a training and a test dataset. We therefore use the 70/30-approach which means that 70% of the data is used for training and 30% of the data is used for testing. \n",
    "We do this split already in this step to ensure that there are no dependencies between training and test dataset in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary package:\n",
    "> numpy: Provides operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, basic linear algebra and much more. We use it here to compute the training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_fraction = 0.70\n",
    "training_size = int(np.floor(len(df) * training_fraction))\n",
    "\n",
    "data_train, data_test = df[:training_size], df[training_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Frequency Method: Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach the number of terms per document are counted. As we have a big corpora in this case, the resulting document-term matrix is a sparse matrix which means that it has many zeros and only a few non-zeor entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def do_bow(data):\n",
    "    count_vec = CountVectorizer(vocabulary = dct)\n",
    "\n",
    "    bow = count_vec.fit_transform(data['CleanText'])\n",
    "    bow_matrix = pd.DataFrame(data = bow.toarray(), columns = count_vec.get_feature_names_out())\n",
    "\n",
    "    display(bow_matrix)\n",
    "    \n",
    "    return(bow_matrix)\n",
    "\n",
    "# call function\n",
    "train_bow_matrix = do_bow(data_train)\n",
    "test_bow_matrix = do_bow(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Frequency Method: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another raw frequency-based approach is TF-IDF (= term-frequency incerse-docmument-frequency). Instead of using the absolute term frequencies, the term frequencies are weighted with the logarithm of the number of a term among all documents divided by the number of all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def do_tfidf(data):\n",
    "    tfidf_vec = TfidfVectorizer(vocabulary = dct)\n",
    "\n",
    "    tfidf = tfidf_vec.fit_transform(data['CleanText'])\n",
    "    tfidf_matrix = pd.DataFrame(data = tfidf.toarray(), columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "    display(tfidf_matrix)\n",
    "    \n",
    "    return(tfidf_matrix)\n",
    "\n",
    "# call function\n",
    "train_tfidf_matrix = do_tfidf(data_train)\n",
    "test_tfidf_matrix = do_tfidf(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Topic Method: LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA (= latent semantic analysis) is one representative of topic-based approaches. It is a mathematical decomposition technique using raw frequency matrices. As already mentioned, BOW or TF-IDF matrices can be very spare and high dimensional. LSA is facing this problem with reduction of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def do_lsa(data):\n",
    "    num_components = 10\n",
    "    q, s, p = svds(data, k = num_components)\n",
    "\n",
    "    lsa_doc_matrix = pd.DataFrame(data=q)\n",
    "\n",
    "    display(lsa_doc_matrix)\n",
    "\n",
    "    return(lsa_doc_matrix)\n",
    "\n",
    "# call function\n",
    "train_lsa_matrix = do_lsa(train_tfidf_matrix)\n",
    "test_lsa_matrix = do_lsa(test_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Word Embedding Method: Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Doc2Vec method it is possible to derive document vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def do_doc2vec(data):\n",
    "    tagged_documents = []\n",
    "    sentences = [text.split() for text in data['CleanText']]\n",
    "\n",
    "    for i, doc in enumerate(sentences):\n",
    "        tagged_documents.append(gensim.models.doc2vec.TaggedDocument(doc, [i]))\n",
    "\n",
    "    d2v = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    d2v.build_vocab(tagged_documents)\n",
    "\n",
    "    d2v.train(tagged_documents, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "\n",
    "    doc2vec_alldocs = []\n",
    "    for i in range(len(tagged_documents)):\n",
    "        doc2vec_alldocs.append(d2v.infer_vector(tagged_documents[i].words))\n",
    "\n",
    "    doc2vec_alldocs_matrix = pd.DataFrame(doc2vec_alldocs)\n",
    "\n",
    "    display(doc2vec_alldocs_matrix)\n",
    "    \n",
    "    return(doc2vec_alldocs_matrix)\n",
    "\n",
    "# call function\n",
    "train_doc2vec_matrix = do_doc2vec(data_train) \n",
    "test_doc2vec_matrix = do_doc2vec(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_nn(train_X, train_y, test_X, test_y):\n",
    "\n",
    "    # first we define the network, units is the number of neurons, in the first layer we need to tell the model the input shape\n",
    "    neural_network = tf.keras.Sequential([\n",
    "                        # hidden layer\n",
    "                        tf.keras.layers.Dense(units = 100, input_shape = [train_X.shape[1]], activation = 'selu'),\n",
    "                        # output layer - as we want probability predictions, it is important to use the sigmoid activation\n",
    "                        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    # by compilation we define the loss function which is supposed to be minimized and which optimization method should be used\n",
    "    neural_network.compile(loss = 'binary_crossentropy', optimizer = 'sgd',  metrics = ['accuracy', tf.keras.metrics.Recall()])\n",
    "\n",
    "    # a summary for all parameters which need to be estimated\n",
    "    neural_network.summary()\n",
    "\n",
    "\n",
    "    train_X = np.asarray(train_X)\n",
    "    test_X = np.asarray(test_X)\n",
    "\n",
    "    # we fit the model, epochs is the number of steps which are repeated using gradient descent\n",
    "    history = neural_network.fit(train_X, train_y, epochs = 100, validation_data = (test_X, test_y))\n",
    "\n",
    "\n",
    "    plt.plot(history.history['loss'], label = 'training'), plt.plot(history.history['val_loss'], label = 'test'), plt.legend(loc='lower left'), plt.show()\n",
    "\n",
    "\n",
    "    corrcoef_train = np.corrcoef(neural_network.predict(train_X).flatten(), train_y)[0, 1]    \n",
    "    print(\"Correlation coefficient Traindata:\", corrcoef_train)\n",
    "    \n",
    "    corrcoef_test = np.corrcoef(neural_network.predict(test_X).flatten(), test_y)[0, 1]\n",
    "    print(\"Correlation coefficient Testdata:\", corrcoef_test)\n",
    "\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_nn(train_bow_matrix, data_train['LabelNumber'], test_bow_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_nn(train_tfidf_matrix, data_train['LabelNumber'], test_tfidf_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_nn(train_lsa_matrix, data_train['LabelNumber'], test_lsa_matrix, data_test['LabelNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_nn(train_doc2vec_matrix, data_train['LabelNumber'], test_doc2vec_matrix, data_test['LabelNumber'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d39680e86271bf209db9aab97c2f2e95b8272895522c86f00c9979eddd8c4165"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
