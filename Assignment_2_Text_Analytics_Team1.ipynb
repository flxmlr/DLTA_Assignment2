{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team information\n",
    "\n",
    "|Team-number :| 1|\n",
    "|:----:|:----:|\n",
    "\n",
    "\n",
    "|Name|    E-Mail        |matriculation-nr.|\n",
    "|:----:|:----:|:----:|\n",
    "|Tamara Scherer| schere21@ads.uni-passau.de|104218|\n",
    "|Felix MÃ¼ller| muell518@ads.uni-passau.de|104227|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After labeling the data we now import it from the label-studio plattform. Therefore we used the provided workaround and downloaded the pickle file. In the next step we extract the necessary texts with the corresponding labels out of the whole downloaded data. A dataframe is created with the columns \"Text\" and \"Label\" in which the extracted data is saved. To be able to work more efficient with the data, we assign an unique ID to each label in the dataframe with the function \"categorise()\". This is especially helpful when we create the neural network in the end, as we can only use integer values as input. As stated in the task formulation we have to focus on one ID Stage. So, we implemented a filter to get only the entries with the desired ID Stage which can be specified before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary packages:\n",
    "> pickle: This package is used for serializing and de-serializing python object structures which means that any kind of python object is converted into byte streams or vice versa.\n",
    "\n",
    "> pandas: This package is usually used for data analysis and manipulation. It allows us to create and work with a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from label_studio_sdk import Client\n",
    "#from label_studio_sdk import project\n",
    "#from label_studio_sdk import project\n",
    "#import pandas as pd\n",
    "#LABEL_STUDIO_URL = 'http://132.231.59.226:8080' #this address needs to be the same as the address the label-studio is hosted on.\n",
    "#API_KEY = '1655a8922f821195356a17a3224c0532b091c61d' #please add your personal API_Key here to get your API_Key follow the Pictures below\n",
    "\n",
    "#ls = Client(url=LABEL_STUDIO_URL, api_key=API_KEY)\n",
    "#ls.check_connection()\n",
    "#pro = project.Project.get_from_id(ls,\"1\")\n",
    "#tasks = project.Project.get_labeled_tasks(pro)\n",
    "#tasks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download labelled tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary package\n",
    "import pickle\n",
    "\n",
    "# import labeled data from downloaded pickle file\n",
    "with open(\"Files/tasks3\", 'rb') as f:\n",
    "    tasks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract text with label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# filter the necessary texts with the corresponding label\n",
    "df = pd.DataFrame(columns = ['Text', 'Label',])\n",
    "\n",
    "for i in range(len(tasks)): \n",
    "    for j in range(len(tasks[i]['annotations'][0]['result'])): \n",
    "        df = df.append({\n",
    "            'Text' : tasks[i]['annotations'][0]['result'][j]['value']['text'],\n",
    "            'Label' : tasks[i]['annotations'][0]['result'][j]['value']['labels']\n",
    "                        }, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Label as unique ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a unique number to each label (can also be used later for the neural network)\n",
    "def categorise(row):  \n",
    "    if row['Label'] == ['Question_1_Company_specific']:\n",
    "        return 1\n",
    "    elif row['Label'] == ['Question_1_Market_related']:\n",
    "        return 2\n",
    "    elif row['Label'] == ['Question_2_specific']:\n",
    "        return 3\n",
    "    elif row['Label'] == ['Question_2_open']:\n",
    "        return 4\n",
    "    elif row['Label'] == ['Question_3_attack']:\n",
    "        return 5\n",
    "    elif row['Label'] == ['Question_3_support']:\n",
    "        return 6\n",
    "    elif row['Label'] == ['Question_3_neutral']:\n",
    "        return 7\n",
    "    elif row['Label'] == ['Answer_1_specific']:\n",
    "        return 8\n",
    "    elif row['Label'] == ['Answer_1_avoid_excuse']:\n",
    "        return 9\n",
    "    elif row['Label'] == ['Answer_2_positive']:\n",
    "        return 10\n",
    "    elif row['Label'] == ['Answer_2_negative']:\n",
    "        return 11\n",
    "    elif row['Label'] == ['Answer_3_blame']:\n",
    "        return 12\n",
    "    elif row['Label'] == ['Answer_3_no_blame']:\n",
    "        return 13\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "# call function and write results in a new column of the dataframe\n",
    "df['LabelNumber'] = df.apply(\n",
    "    lambda row: categorise(row), axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter one ID stage (e.g. Question_1_XX)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set desired labels\n",
    "idStage = [1, 2]\n",
    "\n",
    "# filter the entries in the dataframe with the specified ID stage\n",
    "df = df[df['LabelNumber'].isin(idStage)]\n",
    "\n",
    "# set new index\n",
    "df = df.reset_index()\n",
    "# delete old indices\n",
    "df.pop('index')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelID = 1 --> 919\n",
    "# LabelID = 2 --> 390"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement different steps to preprocess the downloaded data. Therefore we have to make some imports first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Packages:\n",
    "> re: This package can be used to work with Regular Expressions. Here it is used remove the numbers in the text.\n",
    "\n",
    "> string: This package is necessary for common string operations. Here it is used to remove the punctuation in the text.\n",
    "\n",
    "> nltk: Natural Language Toolkit is a package which is used for Natural Language Processing. Here it is used for stemming, lemmatization, tokenization and the removal of stopwords.\n",
    "\n",
    "Further Imports:\n",
    "> For Stemmer: The module \"nltk.stem.snowball\" provides a port of the Snowball stemmers. The word stemmer is based on the original Porter stemming algorithm for suffix stripping. In this case it is used for the English language.\n",
    "\n",
    "> For Stopwords: Here the English stopwords are imported from the \"nltk.corpus\" package. It includes words which does not add much meaning to a sentence and are not important for further text analysis.\n",
    "\n",
    "> For Tokenization: The word_tokenize module is imported from the nltk library for tokenization.\n",
    "\n",
    "> For Lemmatizer: The \"WordNetLemmatizer\" module as well as \"pos_tag\" is imported here. Therefore the system requires some further downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we start with the preprocessing. Therefore, we implemented a function called \"do_preprocessing\" with text that has to be preprocessed as input. Furthermore, it is possible to set two boolean values to specify if the removal of stopwords and the stemming should be executed. \n",
    "Within the function we implemented the following steps:\n",
    "1. Removal of Numbers: delete all numbers, e.g. 1, 2 of the text\n",
    "2. Lowercasing: convert the text into the same casing, so that the different versions of the word can be treated as one\n",
    "3. Stemming: process to reduce the words to their root forms, but the stem itself may not be a valid word in the language (here we use the PorterStemmer)\n",
    "4. Stopwords Removal: stopwords are trivial words which appear very frequently in the text without adding much valuable information and therefore not necessary for the further analysis\n",
    "5. Removing Punctuations: the punctuation does not add any value, so it is not necessary or helpful for the analysis and can therefore be deleted as well\n",
    "6. Removing Extra Whitespaces: additional whitespaces do not add any value to the data and just increase the text size, so they can be deleted as well\n",
    "<p>\n",
    "For Lemmatization we implemented another function \"do_lemmatization\" as it is necessary to do a tokenization first.\n",
    "\n",
    "1. Tokenization: process of splitting the text into pieces called tokens (here the tokens are the single words)\n",
    "2. Lemmatization: This can be done instead of stemming. Lemmatization is also a process to convert the word to its base form. Before the text is lemmatized a POS tagging is necessary to tag the tokens as noun, verb, adverb or adjective. As it causes noticeable improvement we implement the lemmatization in addition to the already implemented preprocessing.\n",
    "<p>\n",
    "Finally we use the resulting tokens to create a dictionary which is necessary for the following steps as for building the neural network the column size of the training and test dataset have to be identical. This can be ensured by using this dictionary. To be able to execute the next implementations it is necessary to convert the tokens back to normal strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for preprocessing steps\n",
    "\n",
    "# import necessary packages\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# import a stemmer for english words\n",
    "snowStem = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "# import for tokenization\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that includes all preprocessing steps\n",
    "\n",
    "def do_preprocessing(text_to_clean, remove_stopwords = True, stemming = True):\n",
    "    # remove numbers \n",
    "    text_to_clean = re.sub(r'\\d+', '', text_to_clean)\n",
    "    # transform text to lower case\n",
    "    text_to_clean = text_to_clean.lower()\n",
    "\n",
    "    if stemming:\n",
    "      # stemming\n",
    "      text_to_clean = snowStem.stem(text_to_clean)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        text_to_clean = ' '.join([w for w in text_to_clean.split() if not(w in en_stopwords)])\n",
    "\n",
    "    # remove punctuation\n",
    "    text_to_clean = text_to_clean.translate(str.maketrans('','', string.punctuation))\n",
    "    # remove leading and ending white spaces\n",
    "    text_to_clean = text_to_clean.strip()\n",
    "    \n",
    "    return text_to_clean\n",
    "\n",
    "\n",
    "# call function and write results in new column of the dataframe\n",
    "df.loc[:, 'CleanText'] = df['Text'].apply(\n",
    "    lambda x: do_preprocessing(x, True, True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lemmatization\n",
    "def do_lemmatization(text):\n",
    "    \n",
    "  text = word_tokenize(text)\n",
    "  \n",
    "  result=[]\n",
    "  wordnet = WordNetLemmatizer()\n",
    "  for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.lemmatize(token,pos))\n",
    "\n",
    "  return result\n",
    "\n",
    "# call function\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: do_lemmatization(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# generate the gensim dictionary\n",
    "dct = corpora.dictionary.Dictionary(df['CleanText']).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert tokens to strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokens back to strings for further processing\n",
    "whitespace = \" \"\n",
    "df['CleanText'] = df['CleanText'].apply(\n",
    "    lambda x: whitespace.join(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split of training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network which we build later on, it is necessary to split the available data in a training and a test dataset. We therefore use the 70/30-approach which means that 70% of the data is used for training and 30% of the data is used for testing. \n",
    "We do this split already in this step to ensure that there are no dependencies between training and test dataset in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary package:\n",
    "> numpy: Provides operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, basic linear algebra and much more. We use it here to compute the training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "#training_fraction = 0.70\n",
    "#training_size = int(np.floor(len(df) * training_fraction))\n",
    "\n",
    "#data_train, data_test = df[:training_size], df[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# we arbitrarily use a training fraction of 70%\n",
    "train_size = int(0.70 * len(df))\n",
    "\n",
    "# select random index numbers and split the data accordingly\n",
    "random.seed(42)\n",
    "train_select = random.sample(list(df.index), k = train_size)\n",
    "train_idx = [i for i in list(df.index) if i in train_select]\n",
    "test_idx = [i for i in list(df.index) if not(i in train_select)]\n",
    "data_train = df.iloc[train_idx, :]\n",
    "data_test = df.iloc[test_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Frequency Method: Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach the number of terms per document are counted. As we have a big corpora in this case, the resulting document-term matrix is a sparse matrix which means that it has many zeros and only a few non-zeor entries.\n",
    "The method \"CountVectorizer()\" converts a collection of text documents to a matrix of token counts. The result out of this is a sparse representation of the counts which then have to be transformed into an array. Finally, this array is converted in a DataFrame. \n",
    "As already mentioned before, we have created a dictionary out of all the existing tokens after the preprocessing. This dictionary can now be used as input for the CountVectorizer() to get BOW-matrices with the same amount of columns.\n",
    "<p>\n",
    "Necessary Package: \n",
    "\n",
    "> sklearn.feature_extraction.text.CountVectorizer: Used to convert a collection of text documents to a matrix of token counts. It produces a sparse representation of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def do_bow(data):\n",
    "    count_vec = CountVectorizer(vocabulary = dct)\n",
    "\n",
    "    bow = count_vec.fit_transform(data['CleanText'])\n",
    "    bow_matrix = pd.DataFrame(data = bow.toarray(), columns = count_vec.get_feature_names_out())\n",
    "\n",
    "    display(bow_matrix)\n",
    "    \n",
    "    return(bow_matrix)\n",
    "\n",
    "# call function\n",
    "train_bow_matrix = do_bow(data_train)\n",
    "test_bow_matrix = do_bow(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Frequency Method: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another raw frequency-based approach is TF-IDF (= term-frequency inverse-docmument-frequency). Instead of using the absolute term frequencies, we use weighted frequencies for this method.\n",
    "It is working very similar to the implementation of BOW. For this method the \"TfidfVectorizer()\" can be used which has again the parameter vocabulary which we set to the created dictionary. The TfidfVectorizer converts a collection of raw documents to a matrix of TF-IDF features. We again get a sparse matrix which we then transform into an array and pass it into a DataFrame which is then given as output of the function.\n",
    "<p>\n",
    "Necessary package: \n",
    "\n",
    "> sklearn.feature_extraction.text.TfidfVectorizer: used to transform a count matrix to a normalized tf or tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def do_tfidf(data):\n",
    "    tfidf_vec = TfidfVectorizer(vocabulary = dct)\n",
    "\n",
    "    tfidf = tfidf_vec.fit_transform(data['CleanText'])\n",
    "    tfidf_matrix = pd.DataFrame(data = tfidf.toarray(), columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "    display(tfidf_matrix)\n",
    "    \n",
    "    return(tfidf_matrix)\n",
    "\n",
    "# call function\n",
    "train_tfidf_matrix = do_tfidf(data_train)\n",
    "test_tfidf_matrix = do_tfidf(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Topic Method: LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA (= latent semantic analysis) is one representative of topic-based approaches. It is a mathematical decomposition technique using raw frequency matrices. As already mentioned, BOW or TF-IDF matrices can be very spare and high dimensional. LSA is facing this problem by reducing the dimensionality.\n",
    "Therefore, the resulting matrix of BOW or TF-IDF is necessary which we already get as output of the corresponding functions. So, we use for example the tf-idf-matrix as input for this function. Then we set the number of topics and apply singular value decomposition to the matrix which can be done for example by the function \"svds\".\n",
    "<p>\n",
    "Necessary Package:\n",
    "\n",
    "> scipy.sparse.linalg.svds: It is used to make a partial singular value decomposition of a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def do_lsa(data):\n",
    "    num_components = 10\n",
    "    q, s, p = svds(data, k = num_components)\n",
    "\n",
    "    lsa_doc_matrix = pd.DataFrame(data=q)\n",
    "\n",
    "    display(lsa_doc_matrix)\n",
    "\n",
    "    return(lsa_doc_matrix)\n",
    "\n",
    "# call function\n",
    "train_lsa_matrix = do_lsa(train_tfidf_matrix)\n",
    "test_lsa_matrix = do_lsa(test_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Word Embedding Method: Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Doc2Vec method it is possible to derive document vectors. As opposite to Word2Vec it is used to create a vectorized representation of a group of words taken collectively as a single unit.\n",
    "First of all we have to train the model. In order to do this we need the tagged document which can be created by using \"models.doc2vec.TaggedDocument()\". In the next step we initialise the model, build the vocabulary and train the Doc2Vec model. Finally, we can use the model.infer_vector() to analyse the output and save the matrix as a DataFrame.\n",
    "\n",
    "<p>\n",
    "Necessary Package:\n",
    "\n",
    "> gensim: Used for topic modelling, document indexing and similarity retrieval with large corpora. The target audience is the natural language processing and information retrieval community.\n",
    "\n",
    "> gensim.models.doc2vec.Doc2Vec: Necessary to build the Doc2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def do_doc2vec(data):\n",
    "    tagged_documents = []\n",
    "    sentences = [text.split() for text in data['CleanText']]\n",
    "    doc2vec_alldocs = []\n",
    "\n",
    "    # train the doc2vec\n",
    "    for i, doc in enumerate(sentences):\n",
    "        tagged_documents.append(gensim.models.doc2vec.TaggedDocument(doc, [i]))\n",
    "\n",
    "    # initialize the model\n",
    "    d2v = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    # build the vocabulary\n",
    "    d2v.build_vocab(tagged_documents)\n",
    "\n",
    "    # train the doc2vec model\n",
    "    d2v.train(tagged_documents, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "\n",
    "    # create Doc2Vec-matrix out of all documents and pass it into a DataFrame\n",
    "    for i in range(len(tagged_documents)):\n",
    "        doc2vec_alldocs.append(d2v.infer_vector(tagged_documents[i].words))\n",
    "    doc2vec_alldocs_matrix = pd.DataFrame(doc2vec_alldocs)\n",
    "\n",
    "    display(doc2vec_alldocs_matrix)\n",
    "    \n",
    "    return(doc2vec_alldocs_matrix)\n",
    "\n",
    "# call function\n",
    "train_doc2vec_matrix = do_doc2vec(data_train) \n",
    "test_doc2vec_matrix = do_doc2vec(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generating a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step we have to create a neural network to generate a target model that tries to predict the correct label for a text. Therefore we first define the neural network. By compilation we define the loww function which is supposed to be minimized and which optimization method should be used. Before we can fit the model, we have to convert the independent variables to an array. In the end we plot the results and calculate the correlation coefficients for the training and test data. <p>\n",
    "As input for the function we use the prepared dataframe with the preprocessed data and the output of BOW/TF-IDF/LSA/Doc2Vec.\n",
    "\n",
    "\n",
    "<p>\n",
    "Necessary Packages:\n",
    "\n",
    "> tensorflow: Provides various tools for machine learning applications. We need it in the following to generate the neural networks.\n",
    "\n",
    "> matplotlib.pyplot: It is used for plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(data_train['LabelNumber'].values.tolist())\n",
    "test_y = np.array(data_test['LabelNumber'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_matrix = np.asarray(train_bow_matrix)\n",
    "test_bow_matrix = np.asarray(test_bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# define function to create a neural network\n",
    "def build_nn(train_X, train_y, test_X, test_y):\n",
    "\n",
    "    # first we define the network, units is the number of neurons, in the first layer we need to tell the model the input shape\n",
    "    neural_network = tf.keras.Sequential([\n",
    "                        # hidden layer\n",
    "                        tf.keras.layers.Dense(units = 100, input_shape = [train_X.shape[1]], activation = 'selu'),\n",
    "                        # output layer - as we want probability predictions, it is important to use the sigmoid activation\n",
    "                        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    # compilation\n",
    "    neural_network.compile(loss = 'binary_crossentropy', optimizer = 'sgd',  metrics = ['accuracy', tf.keras.metrics.Recall()])\n",
    "\n",
    "    # a summary for all parameters which need to be estimated\n",
    "    neural_network.summary()\n",
    "\n",
    "    # convert input to array for the model fit\n",
    "    train_X = np.asarray(train_X)\n",
    "    test_X = np.asarray(test_X)\n",
    "\n",
    "    # we fit the model, epochs is the number of steps which are repeated using gradient descent\n",
    "    history = neural_network.fit(train_X, train_y, epochs = 100, validation_data = (test_X, test_y))\n",
    "    plt.plot(history.history['loss'], label = 'training'), plt.plot(history.history['val_loss'], label = 'test'), plt.legend(loc='lower left'), plt.show()\n",
    "\n",
    "    # correlation coefficient of training and test data\n",
    "    corrcoef_train = np.corrcoef(neural_network.predict(train_X).flatten(), train_y)[0, 1]    \n",
    "    print(\"Correlation coefficient train-data:\", corrcoef_train)\n",
    "    \n",
    "    corrcoef_test = np.corrcoef(neural_network.predict(test_X).flatten(), test_y)[0, 1]\n",
    "    print(\"Correlation coefficient test-data:\", corrcoef_test)\n",
    "\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_bow = build_nn(train_bow_matrix, train_y, test_bow_matrix, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tfidf = build_nn(train_tfidf_matrix, train_y, test_tfidf_matrix, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_lsa = build_nn(train_lsa_matrix, train_y, test_lsa_matrix, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_doc2vec = build_nn(train_doc2vec_matrix, train_y, test_doc2vec_matrix, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the resampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def undersampling(data):\n",
    "    rus = RandomUnderSampler(sampling_strategy=1)\n",
    "    data_res, label_res = rus.fit_resample(data, data['LabelNumber'])\n",
    "\n",
    "    ax = data_res['LabelNumber'].value_counts().plot.pie()\n",
    "    display(data_res)\n",
    "    return data_res\n",
    "\n",
    "df = undersampling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "neural_network = nn_bow\n",
    "\n",
    "# let us take a look at the confusion matrix, remember predictions are 1 if the predicted proability for class 1 is higher than 0.5\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "train_cm = confusion_matrix(train_y, (neural_network.predict(train_bow_matrix) > 0.50).ravel() * 1)\n",
    "sn.heatmap(train_cm, annot = True, cmap='Greens', ax = axs[0])\n",
    "\n",
    "axs[0].set_xlabel('Predictions')\n",
    "axs[0].set_ylabel('True Labels')\n",
    "axs[0].set_title('Training data')\n",
    "\n",
    "test_cm = confusion_matrix(test_y, (neural_network.predict(test_bow_matrix) > 0.50).ravel() * 1)\n",
    "sn.heatmap(test_cm, annot = True, cmap='Greens', ax = axs[1])\n",
    "\n",
    "axs[1].set_xlabel('Predictions')\n",
    "axs[1].set_ylabel('True Labels')\n",
    "axs[1].set_title('Test data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# take a look at the distribution of non-churners and churners\n",
    "df['LabelNumber'].plot(kind = 'hist')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# we use SMOTENC which is capable for mixed features (numerical and categorical)\n",
    "# synthetic observations for categorical features are chosen by majority categories per feature\n",
    "# see some information here: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html#imblearn.over_sampling.SMOTENC\n",
    "# we need to tell SMOTENC which features are categorical\n",
    "\n",
    "smt = SMOTENC(categorical_features = list(range(14, 37)), sampling_strategy = 1)\n",
    "X_train_sm, y_train_sm = smt.fit_resample(train_bow_matrix, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_1 = 0\n",
    "counter_2 = 0\n",
    "\n",
    "for i in range (1293):\n",
    "    if y_train_sm[i] == 1:\n",
    "        counter_1 = counter_1 + 1\n",
    "    elif y_train_sm[i] == 2:\n",
    "        counter_2 = counter_2 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_bow_new = build_nn(X_train_sm, y_train_sm, test_bow_matrix, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "neural_network = nn_bow_new\n",
    "\n",
    "# let us take a look at the confusion matrix, remember predictions are 1 if the predicted proability for class 1 is higher than 0.5\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "train_cm = confusion_matrix(train_y, (neural_network.predict(train_bow_matrix) > 0.50).ravel() * 1)\n",
    "sn.heatmap(train_cm, annot = True, cmap='Greens', ax = axs[0])\n",
    "\n",
    "axs[0].set_xlabel('Predictions')\n",
    "axs[0].set_ylabel('True Labels')\n",
    "axs[0].set_title('Training data')\n",
    "\n",
    "test_cm = confusion_matrix(test_y, (neural_network.predict(test_bow_matrix) > 0.50).ravel() * 1)\n",
    "sn.heatmap(test_cm, annot = True, cmap='Greens', ax = axs[1])\n",
    "\n",
    "axs[1].set_xlabel('Predictions')\n",
    "axs[1].set_ylabel('True Labels')\n",
    "axs[1].set_title('Test data')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d39680e86271bf209db9aab97c2f2e95b8272895522c86f00c9979eddd8c4165"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
